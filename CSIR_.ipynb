{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8GBaI/FpYsyS4NxrYU0zf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vivisteria11/Image-classification/blob/main/CSIR_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMaEoWAWdjbr",
        "outputId": "f078b1d9-6244-408f-c48d-770f382c3374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-23 12:21:44--  https://www.google.com/\n",
            "Resolving www.google.com (www.google.com)... 74.125.196.104, 74.125.196.103, 74.125.196.147, ...\n",
            "Connecting to www.google.com (www.google.com)|74.125.196.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘index.html’\n",
            "\n",
            "\rindex.html              [<=>                 ]       0  --.-KB/s               \rindex.html              [ <=>                ]  19.22K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-23 12:21:44 (95.9 MB/s) - ‘index.html’ saved [19678]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.google.com\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#set up the environments\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "# Load the MNIST dataset from Keras\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the dataset to [-1, 1] (similar to your normalization)\n",
        "x_train = (x_train / 255.0 - 0.5) / 0.5\n",
        "x_test = (x_test / 255.0 - 0.5) / 0.5\n",
        "\n",
        "# Convert NumPy arrays to PyTorch tensors\n",
        "x_train_tensor = torch.tensor(x_train, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "x_test_tensor = torch.tensor(x_test, dtype=torch.float32).unsqueeze(1)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create datasets\n",
        "training_set = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "validation_set = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "\n",
        "\n",
        "\n",
        "#transform the image to tensor and then using mean and variance normalize it\n",
        "\n",
        "#transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5),(0.5))])\n",
        "\n",
        "\n",
        "#spilt into training and validation sets\n",
        "\n",
        "#training_set = datasets.MNIST(\"/path...\",download =True,train =True,transform =transform)\n",
        "#validation_set= datasets.MNIST(\"/path..\",download=True,train=Truerue,transform =transform)\n",
        "\n",
        "#dataloader splits the data inot batches for training and shuffle reduces overfitting\n",
        "training_loader = torch.utils.data.DataLoader(training_set,batch_size=64,shuffle=True)\n",
        "validation_loader=torch.utils.data.DataLoader(validation_set,batch_size=64,shuffle=True)\n",
        "\n",
        "\n",
        "iterator = iter(training_loader)#iterates over the training data\n",
        "images,labels=next(iterator)#retrives the next batch of images and labels\n",
        "\n",
        "print(images.shape)#should be a tensor with 64 images in a batch 1 for greyscale image each a 28*28 pixel images\n",
        "print(labels.shape)#should be a tensor with 64 labels corresponding to the image\n",
        "\n",
        "plt.imshow(images[0].numpy().squeeze(),cmap ='gray_r')# to display the image ,convert tensor to a numpy because matplotlib asks and squeeze any extra single dimension\n",
        "\n",
        "#training the  model\n",
        "\n",
        "input_size=784#flatten the images 28*28 =784\n",
        "hidden_layers=(128,64)\n",
        "output_size=10#0-9 digits present in the MNIST dataset\n",
        "\n",
        "model = nn.Sequential(\n",
        "        nn.Linear(input_size,hidden_layers[0]),\n",
        "        nn.ReLU(),#introduces non linearity\n",
        "        nn.Linear(hidden_layers[0],hidden_layers[1]),#maps it to the layers\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_layers[1],output_size) ,\n",
        "        nn.LogSoftmax(dim=1)\n",
        "\n",
        ")\n",
        "\n",
        "print(model)\n",
        "\n",
        "\n",
        "logloss = nn.NLLLoss() #to calculate the negative log loss likelihood from the output of the softmax\n",
        "images, labels = next(iter(training_loader)) #loads a batch of data continously\n",
        "images = images.view(images.shape[0], -1) #flattens the image [batch_size,num_features]\n",
        "\n",
        "log_probabilities = model(images) #log probabilities ,it is a tensor of log probablities\n",
        "loss = logloss(log_probabilities, labels) #calculate the NLL loss compares predicted to true labels\n",
        "\n",
        "\n",
        "print('Before backward pass: \\n', model[0].weight.grad)#initially none\n",
        "loss.backward()#readjusting the weights\n",
        "print('After backward pass: \\n', model[0].weight.grad)#will give out values\n",
        "\n",
        "#stochastic gradient descent optimizier\n",
        "\n",
        "optimizer = optim.SGD( model.parameters(),lr=0.01 ,momentum =0.9)#dont alter the parameter names as it is the arguments for the\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "for e in range(epochs):\n",
        "        loss_rate = 0 #cumulative loss for epoch\n",
        "\n",
        "        for images,labels in training_loader:\n",
        "                img =images.view(images.shape[0],-1)\n",
        "                optimizer.zero_grad() #clears the gradient from previous steps\n",
        "\n",
        "                output = model(img)\n",
        "\n",
        "                loss = logloss(output,labels)\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                loss_rate += loss.item()#extracts the scalar value from the loss tensor to add it to the loss rate\n",
        "\n",
        "\n",
        "                print(\"Epoch {} - Training loss: {}\".format(e, loss_rate/len(training_loader)))\n",
        "\n",
        "images, labels = next(iter(validation_loader))\n",
        "\n",
        "# Select the first image and reshape it for the model\n",
        "img = images[0].view(1, 784)\n",
        "\n",
        "# Disable gradient calculation for inference\n",
        "with torch.no_grad():\n",
        "    log_probabilities = model(img)  # Get log probabilities from the model\n",
        "\n",
        "# Convert log probabilities to probabilities\n",
        "ps = torch.exp(log_probabilities)\n",
        "\n",
        "# Get the predicted class (digit)\n",
        "probab = list(ps.numpy()[0])  # Convert to a Python list\n",
        "predicted_digit = probab.index(max(probab))  # Find the index of the max probability\n",
        "\n",
        "# Display the results\n",
        "print(\"Predicted Digit =\", predicted_digit)\n",
        "print(\"Probabilities for each digit class:\", probab)\n",
        "\n",
        "\n",
        "correctly_pred =0\n",
        "total_img =0\n",
        "\n",
        "for images,labels in validation_loader:\n",
        "        for i in range(len(labels)):#for images in that batch\n",
        "\n",
        "                img = images[i].view(1, 784)#1st batch flattened image\n",
        "\n",
        "                with(torch.no_grad()):#disabled gradient for faster computation\n",
        "                 output =model(img)\n",
        "\n",
        "                 ps = torch.exp(output)#convert log probabilities to probabilities\n",
        "\n",
        "                 probab = list(ps.numpy()[0])#ps.numpy()[0]: Converts the tensor of probabilities into a NumPy array.\n",
        "                                             #list(ps.numpy()[0]): Converts the NumPy array into a Python list.\n",
        "                                              #max(probab): Finds the highest probability.\n",
        "                                              #probab.index(max(probab)): Returns the index of the highest probability, which corresponds to the predicted label.\n",
        "                 pred_label = probab.index(max(probab))\n",
        "                 true_label = labels.numpy()[i]\n",
        "\n",
        "\n",
        "                if(true_label == pred_label):\n",
        "                    correctly_pred+= 1\n",
        "                total_img += 1\n",
        "\n",
        "\n",
        "print(\"Number Of Images Tested =\", total_img)\n",
        "print(\"\\nModel Accuracy =\", (correctly_pred/total_img))\n",
        "\n",
        "\n",
        "torch.save(model, './Mnist.pt')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rnCUHYFMd3Al",
        "outputId": "0c9dd414-d18c-47ac-e2e3-a3892b6b995a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "Sequential(\n",
            "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (5): LogSoftmax(dim=1)\n",
            ")\n",
            "Before backward pass: \n",
            " None\n",
            "After backward pass: \n",
            " tensor([[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
            "        [-0.0005, -0.0005, -0.0005,  ..., -0.0005, -0.0005, -0.0005],\n",
            "        [ 0.0006,  0.0006,  0.0006,  ...,  0.0006,  0.0006,  0.0006],\n",
            "        ...,\n",
            "        [ 0.0018,  0.0018,  0.0018,  ...,  0.0018,  0.0018,  0.0018],\n",
            "        [ 0.0009,  0.0009,  0.0009,  ...,  0.0009,  0.0009,  0.0009],\n",
            "        [-0.0015, -0.0015, -0.0015,  ..., -0.0015, -0.0015, -0.0015]])\n",
            "Epoch 0 - Training loss: 0.0024395299110331263\n",
            "Epoch 0 - Training loss: 0.004888032799336447\n",
            "Epoch 0 - Training loss: 0.007352895574020678\n",
            "Epoch 0 - Training loss: 0.0097948470349505\n",
            "Epoch 0 - Training loss: 0.01222056747753737\n",
            "Epoch 0 - Training loss: 0.01464922367128482\n",
            "Epoch 0 - Training loss: 0.01708844272312579\n",
            "Epoch 0 - Training loss: 0.019525062300757304\n",
            "Epoch 0 - Training loss: 0.021971568369916254\n",
            "Epoch 0 - Training loss: 0.024379098847476657\n",
            "Epoch 0 - Training loss: 0.02681456852569255\n",
            "Epoch 0 - Training loss: 0.02921380544267992\n",
            "Epoch 0 - Training loss: 0.03157877998311382\n",
            "Epoch 0 - Training loss: 0.033979738953271145\n",
            "Epoch 0 - Training loss: 0.036345519745019456\n",
            "Epoch 0 - Training loss: 0.0387239425675447\n",
            "Epoch 0 - Training loss: 0.041121423371565116\n",
            "Epoch 0 - Training loss: 0.043493608930217684\n",
            "Epoch 0 - Training loss: 0.045839959116124394\n",
            "Epoch 0 - Training loss: 0.04816174380052318\n",
            "Epoch 0 - Training loss: 0.05047798792182255\n",
            "Epoch 0 - Training loss: 0.05277135402663176\n",
            "Epoch 0 - Training loss: 0.055071978680868904\n",
            "Epoch 0 - Training loss: 0.05736608062980018\n",
            "Epoch 0 - Training loss: 0.059642317452664566\n",
            "Epoch 0 - Training loss: 0.061895303380514764\n",
            "Epoch 0 - Training loss: 0.06411747891765668\n",
            "Epoch 0 - Training loss: 0.06629575061391411\n",
            "Epoch 0 - Training loss: 0.06843932376487423\n",
            "Epoch 0 - Training loss: 0.07057803704032004\n",
            "Epoch 0 - Training loss: 0.07268153998389174\n",
            "Epoch 0 - Training loss: 0.07476140873264402\n",
            "Epoch 0 - Training loss: 0.07684593325230613\n",
            "Epoch 0 - Training loss: 0.07886398944265044\n",
            "Epoch 0 - Training loss: 0.08085010478745645\n",
            "Epoch 0 - Training loss: 0.08292130391989182\n",
            "Epoch 0 - Training loss: 0.0847683626451472\n",
            "Epoch 0 - Training loss: 0.08664457299816075\n",
            "Epoch 0 - Training loss: 0.08844907324451373\n",
            "Epoch 0 - Training loss: 0.09023801134085097\n",
            "Epoch 0 - Training loss: 0.09191760719457923\n",
            "Epoch 0 - Training loss: 0.09342573243163542\n",
            "Epoch 0 - Training loss: 0.09510713625055894\n",
            "Epoch 0 - Training loss: 0.09667563489251045\n",
            "Epoch 0 - Training loss: 0.09831641541361046\n",
            "Epoch 0 - Training loss: 0.09973065177006508\n",
            "Epoch 0 - Training loss: 0.10125789904136902\n",
            "Epoch 0 - Training loss: 0.10261061743124207\n",
            "Epoch 0 - Training loss: 0.10399561319778215\n",
            "Epoch 0 - Training loss: 0.10524551726099271\n",
            "Epoch 0 - Training loss: 0.10645966501886657\n",
            "Epoch 0 - Training loss: 0.10747002811828402\n",
            "Epoch 0 - Training loss: 0.10858588591059133\n",
            "Epoch 0 - Training loss: 0.10983443393636105\n",
            "Epoch 0 - Training loss: 0.111060350243725\n",
            "Epoch 0 - Training loss: 0.11192749659898185\n",
            "Epoch 0 - Training loss: 0.11314353581938917\n",
            "Epoch 0 - Training loss: 0.1142046269831627\n",
            "Epoch 0 - Training loss: 0.11520163292315469\n",
            "Epoch 0 - Training loss: 0.11626687080367033\n",
            "Epoch 0 - Training loss: 0.11710962843793288\n",
            "Epoch 0 - Training loss: 0.11828986089875194\n",
            "Epoch 0 - Training loss: 0.11936892686622229\n",
            "Epoch 0 - Training loss: 0.12028663073267255\n",
            "Epoch 0 - Training loss: 0.12125811953026094\n",
            "Epoch 0 - Training loss: 0.12252910025338375\n",
            "Epoch 0 - Training loss: 0.1233862975258817\n",
            "Epoch 0 - Training loss: 0.12416241665893017\n",
            "Epoch 0 - Training loss: 0.12496027693565466\n",
            "Epoch 0 - Training loss: 0.12574944503780114\n",
            "Epoch 0 - Training loss: 0.126690266356031\n",
            "Epoch 0 - Training loss: 0.12787172890929524\n",
            "Epoch 0 - Training loss: 0.12887910720127732\n",
            "Epoch 0 - Training loss: 0.12971186212130956\n",
            "Epoch 0 - Training loss: 0.13037342779926145\n",
            "Epoch 0 - Training loss: 0.131282790828107\n",
            "Epoch 0 - Training loss: 0.1319291549069541\n",
            "Epoch 0 - Training loss: 0.13264236769188187\n",
            "Epoch 0 - Training loss: 0.13345762985601609\n",
            "Epoch 0 - Training loss: 0.1340731582534847\n",
            "Epoch 0 - Training loss: 0.134804023926192\n",
            "Epoch 0 - Training loss: 0.1357899109946131\n",
            "Epoch 0 - Training loss: 0.1364722466672153\n",
            "Epoch 0 - Training loss: 0.1370669640838973\n",
            "Epoch 0 - Training loss: 0.13778803699306333\n",
            "Epoch 0 - Training loss: 0.13857835868020046\n",
            "Epoch 0 - Training loss: 0.13933536226052973\n",
            "Epoch 0 - Training loss: 0.1401562325354578\n",
            "Epoch 0 - Training loss: 0.14072371736518355\n",
            "Epoch 0 - Training loss: 0.14144446530830124\n",
            "Epoch 0 - Training loss: 0.14213326751296199\n",
            "Epoch 0 - Training loss: 0.1426896072272807\n",
            "Epoch 0 - Training loss: 0.14313909017455095\n",
            "Epoch 0 - Training loss: 0.14392946117214048\n",
            "Epoch 0 - Training loss: 0.14450049470228427\n",
            "Epoch 0 - Training loss: 0.14509155901510323\n",
            "Epoch 0 - Training loss: 0.14556512655988177\n",
            "Epoch 0 - Training loss: 0.14625127165556462\n",
            "Epoch 0 - Training loss: 0.14677434730758546\n",
            "Epoch 0 - Training loss: 0.1473676049188256\n",
            "Epoch 0 - Training loss: 0.14782979728570625\n",
            "Epoch 0 - Training loss: 0.14854980963887945\n",
            "Epoch 0 - Training loss: 0.1491063894239316\n",
            "Epoch 0 - Training loss: 0.14977609831641223\n",
            "Epoch 0 - Training loss: 0.15038625956344198\n",
            "Epoch 0 - Training loss: 0.1510555439158035\n",
            "Epoch 0 - Training loss: 0.15143139825573862\n",
            "Epoch 0 - Training loss: 0.15200832610064224\n",
            "Epoch 0 - Training loss: 0.15266190164251878\n",
            "Epoch 0 - Training loss: 0.15341726939942538\n",
            "Epoch 0 - Training loss: 0.15392257718007957\n",
            "Epoch 0 - Training loss: 0.15442610463735137\n",
            "Epoch 0 - Training loss: 0.15503553073924742\n",
            "Epoch 0 - Training loss: 0.15534668506335603\n",
            "Epoch 0 - Training loss: 0.15590539396698797\n",
            "Epoch 0 - Training loss: 0.15644830427190135\n",
            "Epoch 0 - Training loss: 0.15689428233261557\n",
            "Epoch 0 - Training loss: 0.15729572901974864\n",
            "Epoch 0 - Training loss: 0.15784267433035348\n",
            "Epoch 0 - Training loss: 0.15843639016024338\n",
            "Epoch 0 - Training loss: 0.1590948165543298\n",
            "Epoch 0 - Training loss: 0.15967319098744057\n",
            "Epoch 0 - Training loss: 0.1602472048769119\n",
            "Epoch 0 - Training loss: 0.16067461729812216\n",
            "Epoch 0 - Training loss: 0.16106629911770445\n",
            "Epoch 0 - Training loss: 0.16168119277018728\n",
            "Epoch 0 - Training loss: 0.1621805335730632\n",
            "Epoch 0 - Training loss: 0.16267812712741558\n",
            "Epoch 0 - Training loss: 0.16303108333905877\n",
            "Epoch 0 - Training loss: 0.16350968947796934\n",
            "Epoch 0 - Training loss: 0.1639115908252659\n",
            "Epoch 0 - Training loss: 0.16433308154408102\n",
            "Epoch 0 - Training loss: 0.16494222398378702\n",
            "Epoch 0 - Training loss: 0.16543734283335426\n",
            "Epoch 0 - Training loss: 0.16606198063791433\n",
            "Epoch 0 - Training loss: 0.16652593645713984\n",
            "Epoch 0 - Training loss: 0.16712103717362703\n",
            "Epoch 0 - Training loss: 0.16763927067902043\n",
            "Epoch 0 - Training loss: 0.1681882706977157\n",
            "Epoch 0 - Training loss: 0.16863520122540276\n",
            "Epoch 0 - Training loss: 0.16911217757760844\n",
            "Epoch 0 - Training loss: 0.16969061034447602\n",
            "Epoch 0 - Training loss: 0.1700000323212223\n",
            "Epoch 0 - Training loss: 0.17052014712204558\n",
            "Epoch 0 - Training loss: 0.17132130567071788\n",
            "Epoch 0 - Training loss: 0.17166258289869915\n",
            "Epoch 0 - Training loss: 0.1719781854577156\n",
            "Epoch 0 - Training loss: 0.17232622288818808\n",
            "Epoch 0 - Training loss: 0.172803998914863\n",
            "Epoch 0 - Training loss: 0.1735769684063092\n",
            "Epoch 0 - Training loss: 0.17398441934000963\n",
            "Epoch 0 - Training loss: 0.1744109208522829\n",
            "Epoch 0 - Training loss: 0.17499220269575302\n",
            "Epoch 0 - Training loss: 0.17555272541066477\n",
            "Epoch 0 - Training loss: 0.17590622511753903\n",
            "Epoch 0 - Training loss: 0.1764058869149385\n",
            "Epoch 0 - Training loss: 0.17689375852598055\n",
            "Epoch 0 - Training loss: 0.17733456324666802\n",
            "Epoch 0 - Training loss: 0.17771676525886634\n",
            "Epoch 0 - Training loss: 0.17810922810263724\n",
            "Epoch 0 - Training loss: 0.178880684601981\n",
            "Epoch 0 - Training loss: 0.17949732845780184\n",
            "Epoch 0 - Training loss: 0.17987332863212901\n",
            "Epoch 0 - Training loss: 0.1802724841624689\n",
            "Epoch 0 - Training loss: 0.18083865794418716\n",
            "Epoch 0 - Training loss: 0.18134324570327426\n",
            "Epoch 0 - Training loss: 0.18178547111782692\n",
            "Epoch 0 - Training loss: 0.18215763765865806\n",
            "Epoch 0 - Training loss: 0.18323807923524366\n",
            "Epoch 0 - Training loss: 0.183544042395122\n",
            "Epoch 0 - Training loss: 0.18412896342623208\n",
            "Epoch 0 - Training loss: 0.18476552921317535\n",
            "Epoch 0 - Training loss: 0.185177119206518\n",
            "Epoch 0 - Training loss: 0.18559717128017564\n",
            "Epoch 0 - Training loss: 0.18614037554147148\n",
            "Epoch 0 - Training loss: 0.18674445152282715\n",
            "Epoch 0 - Training loss: 0.1872769393074487\n",
            "Epoch 0 - Training loss: 0.1876384184431674\n",
            "Epoch 0 - Training loss: 0.18795247232990225\n",
            "Epoch 0 - Training loss: 0.18833407327564541\n",
            "Epoch 0 - Training loss: 0.18885994338785916\n",
            "Epoch 0 - Training loss: 0.189435420387081\n",
            "Epoch 0 - Training loss: 0.18997368483401056\n",
            "Epoch 0 - Training loss: 0.19022947584769365\n",
            "Epoch 0 - Training loss: 0.19071743054303533\n",
            "Epoch 0 - Training loss: 0.19108083761577158\n",
            "Epoch 0 - Training loss: 0.19163558217508198\n",
            "Epoch 0 - Training loss: 0.1921139703567094\n",
            "Epoch 0 - Training loss: 0.1924121646421042\n",
            "Epoch 0 - Training loss: 0.19303862235820624\n",
            "Epoch 0 - Training loss: 0.19348915632980973\n",
            "Epoch 0 - Training loss: 0.19383781916424156\n",
            "Epoch 0 - Training loss: 0.19428727199146742\n",
            "Epoch 0 - Training loss: 0.19456449618090443\n",
            "Epoch 0 - Training loss: 0.19519949000654443\n",
            "Epoch 0 - Training loss: 0.19550802903388864\n",
            "Epoch 0 - Training loss: 0.19573929022624295\n",
            "Epoch 0 - Training loss: 0.19632281830061726\n",
            "Epoch 0 - Training loss: 0.19679479939596994\n",
            "Epoch 0 - Training loss: 0.19721225596694295\n",
            "Epoch 0 - Training loss: 0.19748451705299205\n",
            "Epoch 0 - Training loss: 0.19779461504681023\n",
            "Epoch 0 - Training loss: 0.19831335013990464\n",
            "Epoch 0 - Training loss: 0.19886452515623462\n",
            "Epoch 0 - Training loss: 0.19920505338640354\n",
            "Epoch 0 - Training loss: 0.19962706856890275\n",
            "Epoch 0 - Training loss: 0.19989450402986775\n",
            "Epoch 0 - Training loss: 0.20011276607193165\n",
            "Epoch 0 - Training loss: 0.20061223968259814\n",
            "Epoch 0 - Training loss: 0.20094488287912504\n",
            "Epoch 0 - Training loss: 0.20135735312123287\n",
            "Epoch 0 - Training loss: 0.20162159160001955\n",
            "Epoch 0 - Training loss: 0.20181792005419985\n",
            "Epoch 0 - Training loss: 0.20215292062078202\n",
            "Epoch 0 - Training loss: 0.20241023946418438\n",
            "Epoch 0 - Training loss: 0.2030070108899684\n",
            "Epoch 0 - Training loss: 0.203644172342093\n",
            "Epoch 0 - Training loss: 0.2039153916812909\n",
            "Epoch 0 - Training loss: 0.20428637496189778\n",
            "Epoch 0 - Training loss: 0.20467908960034345\n",
            "Epoch 0 - Training loss: 0.20497589467812194\n",
            "Epoch 0 - Training loss: 0.20534382144144095\n",
            "Epoch 0 - Training loss: 0.20559930254909783\n",
            "Epoch 0 - Training loss: 0.20610493157845317\n",
            "Epoch 0 - Training loss: 0.20672344173322607\n",
            "Epoch 0 - Training loss: 0.20719178186169565\n",
            "Epoch 0 - Training loss: 0.2074400708238199\n",
            "Epoch 0 - Training loss: 0.20778438150247278\n",
            "Epoch 0 - Training loss: 0.20825819464634732\n",
            "Epoch 0 - Training loss: 0.20859318377493796\n",
            "Epoch 0 - Training loss: 0.2088544529511222\n",
            "Epoch 0 - Training loss: 0.20917692815444108\n",
            "Epoch 0 - Training loss: 0.2099667394529782\n",
            "Epoch 0 - Training loss: 0.21045597583881573\n",
            "Epoch 0 - Training loss: 0.2110635429493654\n",
            "Epoch 0 - Training loss: 0.21141921828931837\n",
            "Epoch 0 - Training loss: 0.21186601769314137\n",
            "Epoch 0 - Training loss: 0.2123555295757139\n",
            "Epoch 0 - Training loss: 0.21270892240091174\n",
            "Epoch 0 - Training loss: 0.2131210404164247\n",
            "Epoch 0 - Training loss: 0.21362788317554288\n",
            "Epoch 0 - Training loss: 0.2141277549871758\n",
            "Epoch 0 - Training loss: 0.21457562581308362\n",
            "Epoch 0 - Training loss: 0.2151572025661021\n",
            "Epoch 0 - Training loss: 0.215650231535755\n",
            "Epoch 0 - Training loss: 0.2159931030926674\n",
            "Epoch 0 - Training loss: 0.21641479267367422\n",
            "Epoch 0 - Training loss: 0.2166886570801867\n",
            "Epoch 0 - Training loss: 0.2172473202953969\n",
            "Epoch 0 - Training loss: 0.21772140598119194\n",
            "Epoch 0 - Training loss: 0.218186931569439\n",
            "Epoch 0 - Training loss: 0.21849550027201678\n",
            "Epoch 0 - Training loss: 0.21875697037558567\n",
            "Epoch 0 - Training loss: 0.21917142313934845\n",
            "Epoch 0 - Training loss: 0.21967992729850924\n",
            "Epoch 0 - Training loss: 0.22021035681655413\n",
            "Epoch 0 - Training loss: 0.22061507461040514\n",
            "Epoch 0 - Training loss: 0.22100859114737398\n",
            "Epoch 0 - Training loss: 0.22124659173142935\n",
            "Epoch 0 - Training loss: 0.22174058953073741\n",
            "Epoch 0 - Training loss: 0.22224709566341025\n",
            "Epoch 0 - Training loss: 0.22271538864193693\n",
            "Epoch 0 - Training loss: 0.22318415553457954\n",
            "Epoch 0 - Training loss: 0.223685897235423\n",
            "Epoch 0 - Training loss: 0.22384406735838602\n",
            "Epoch 0 - Training loss: 0.22429550979246718\n",
            "Epoch 0 - Training loss: 0.22453663029523294\n",
            "Epoch 0 - Training loss: 0.22508763710954297\n",
            "Epoch 0 - Training loss: 0.22569120117723307\n",
            "Epoch 0 - Training loss: 0.22596278894684715\n",
            "Epoch 0 - Training loss: 0.22623470158719305\n",
            "Epoch 0 - Training loss: 0.22687933128525709\n",
            "Epoch 0 - Training loss: 0.22707649808067248\n",
            "Epoch 0 - Training loss: 0.22730048394787794\n",
            "Epoch 0 - Training loss: 0.22764232964403847\n",
            "Epoch 0 - Training loss: 0.22795504824057825\n",
            "Epoch 0 - Training loss: 0.22823928766794552\n",
            "Epoch 0 - Training loss: 0.2288696502508131\n",
            "Epoch 0 - Training loss: 0.22921306212573672\n",
            "Epoch 0 - Training loss: 0.229436187093446\n",
            "Epoch 0 - Training loss: 0.22974473889321406\n",
            "Epoch 0 - Training loss: 0.23026175177427752\n",
            "Epoch 0 - Training loss: 0.2306113348269005\n",
            "Epoch 0 - Training loss: 0.23108292070787345\n",
            "Epoch 0 - Training loss: 0.23135843231225572\n",
            "Epoch 0 - Training loss: 0.2318074112253657\n",
            "Epoch 0 - Training loss: 0.23203746923632712\n",
            "Epoch 0 - Training loss: 0.23249824917011425\n",
            "Epoch 0 - Training loss: 0.23270433888570077\n",
            "Epoch 0 - Training loss: 0.23306376585510494\n",
            "Epoch 0 - Training loss: 0.23340003168595624\n",
            "Epoch 0 - Training loss: 0.23390241864838326\n",
            "Epoch 0 - Training loss: 0.23426537544551942\n",
            "Epoch 0 - Training loss: 0.23473828530578472\n",
            "Epoch 0 - Training loss: 0.23504696467093059\n",
            "Epoch 0 - Training loss: 0.2353335951468838\n",
            "Epoch 0 - Training loss: 0.23560061089710385\n",
            "Epoch 0 - Training loss: 0.23594650208378143\n",
            "Epoch 0 - Training loss: 0.23619909363705466\n",
            "Epoch 0 - Training loss: 0.23666404764344698\n",
            "Epoch 0 - Training loss: 0.2369330211647792\n",
            "Epoch 0 - Training loss: 0.23724025940653612\n",
            "Epoch 0 - Training loss: 0.23772353157877668\n",
            "Epoch 0 - Training loss: 0.23804336215959174\n",
            "Epoch 0 - Training loss: 0.23830937597352558\n",
            "Epoch 0 - Training loss: 0.2387578050687369\n",
            "Epoch 0 - Training loss: 0.2391755550241928\n",
            "Epoch 0 - Training loss: 0.23957697054279892\n",
            "Epoch 0 - Training loss: 0.23990643575692228\n",
            "Epoch 0 - Training loss: 0.24009035521351707\n",
            "Epoch 0 - Training loss: 0.24040604892697162\n",
            "Epoch 0 - Training loss: 0.24062508714796382\n",
            "Epoch 0 - Training loss: 0.24089095838415597\n",
            "Epoch 0 - Training loss: 0.24123458097230144\n",
            "Epoch 0 - Training loss: 0.24151302190986015\n",
            "Epoch 0 - Training loss: 0.24195800235530715\n",
            "Epoch 0 - Training loss: 0.24246925979789133\n",
            "Epoch 0 - Training loss: 0.24270448087057325\n",
            "Epoch 0 - Training loss: 0.24309712904157924\n",
            "Epoch 0 - Training loss: 0.24358284757780369\n",
            "Epoch 0 - Training loss: 0.2438366120812227\n",
            "Epoch 0 - Training loss: 0.24420997726002228\n",
            "Epoch 0 - Training loss: 0.2446081652315949\n",
            "Epoch 0 - Training loss: 0.24492525981306268\n",
            "Epoch 0 - Training loss: 0.24525105346367557\n",
            "Epoch 0 - Training loss: 0.24545253296968525\n",
            "Epoch 0 - Training loss: 0.24597393235227447\n",
            "Epoch 0 - Training loss: 0.24655467559343208\n",
            "Epoch 0 - Training loss: 0.2469091891511671\n",
            "Epoch 0 - Training loss: 0.24731082516883227\n",
            "Epoch 0 - Training loss: 0.2475721509630746\n",
            "Epoch 0 - Training loss: 0.24791980525260288\n",
            "Epoch 0 - Training loss: 0.24854599188830553\n",
            "Epoch 0 - Training loss: 0.24873425741630323\n",
            "Epoch 0 - Training loss: 0.2492266105912896\n",
            "Epoch 0 - Training loss: 0.24974510215048087\n",
            "Epoch 0 - Training loss: 0.25012813869125045\n",
            "Epoch 0 - Training loss: 0.2506941728023832\n",
            "Epoch 0 - Training loss: 0.25096158275027264\n",
            "Epoch 0 - Training loss: 0.2513901033540016\n",
            "Epoch 0 - Training loss: 0.2517846715507477\n",
            "Epoch 0 - Training loss: 0.2520940846630505\n",
            "Epoch 0 - Training loss: 0.2525954615555084\n",
            "Epoch 0 - Training loss: 0.25281860293355835\n",
            "Epoch 0 - Training loss: 0.25318434373783405\n",
            "Epoch 0 - Training loss: 0.25358926601755594\n",
            "Epoch 0 - Training loss: 0.25384725816150716\n",
            "Epoch 0 - Training loss: 0.2541348540674903\n",
            "Epoch 0 - Training loss: 0.25451330152719515\n",
            "Epoch 0 - Training loss: 0.2547616470914914\n",
            "Epoch 0 - Training loss: 0.254999939201356\n",
            "Epoch 0 - Training loss: 0.25550939725723854\n",
            "Epoch 0 - Training loss: 0.2557920914119495\n",
            "Epoch 0 - Training loss: 0.2565758195417776\n",
            "Epoch 0 - Training loss: 0.25675034286307374\n",
            "Epoch 0 - Training loss: 0.25701331482258943\n",
            "Epoch 0 - Training loss: 0.2574329524279149\n",
            "Epoch 0 - Training loss: 0.25780586180275183\n",
            "Epoch 0 - Training loss: 0.25803408396841365\n",
            "Epoch 0 - Training loss: 0.25845811519223744\n",
            "Epoch 0 - Training loss: 0.25870211827538925\n",
            "Epoch 0 - Training loss: 0.2590570851270832\n",
            "Epoch 0 - Training loss: 0.25940481636887674\n",
            "Epoch 0 - Training loss: 0.25966320507752616\n",
            "Epoch 0 - Training loss: 0.2598551766418699\n",
            "Epoch 0 - Training loss: 0.2601730074487261\n",
            "Epoch 0 - Training loss: 0.260606637943401\n",
            "Epoch 0 - Training loss: 0.26093866282116884\n",
            "Epoch 0 - Training loss: 0.26115098830733474\n",
            "Epoch 0 - Training loss: 0.26164771959598637\n",
            "Epoch 0 - Training loss: 0.26229902927173987\n",
            "Epoch 0 - Training loss: 0.2627517852956044\n",
            "Epoch 0 - Training loss: 0.2629325376200015\n",
            "Epoch 0 - Training loss: 0.2632346037926196\n",
            "Epoch 0 - Training loss: 0.26350741374340136\n",
            "Epoch 0 - Training loss: 0.2638772116350467\n",
            "Epoch 0 - Training loss: 0.26410961672187105\n",
            "Epoch 0 - Training loss: 0.26428670636308726\n",
            "Epoch 0 - Training loss: 0.26448648418190635\n",
            "Epoch 0 - Training loss: 0.265139152182699\n",
            "Epoch 0 - Training loss: 0.2654756645340401\n",
            "Epoch 0 - Training loss: 0.26589458230843166\n",
            "Epoch 0 - Training loss: 0.26633837625289014\n",
            "Epoch 0 - Training loss: 0.26673795714942633\n",
            "Epoch 0 - Training loss: 0.26706575610236066\n",
            "Epoch 0 - Training loss: 0.26753868712291023\n",
            "Epoch 0 - Training loss: 0.26801820004037197\n",
            "Epoch 0 - Training loss: 0.2682752683123292\n",
            "Epoch 0 - Training loss: 0.2684995822751446\n",
            "Epoch 0 - Training loss: 0.2688463924091254\n",
            "Epoch 0 - Training loss: 0.2691806958936679\n",
            "Epoch 0 - Training loss: 0.2694192363350376\n",
            "Epoch 0 - Training loss: 0.2700253130752903\n",
            "Epoch 0 - Training loss: 0.2705311023318437\n",
            "Epoch 0 - Training loss: 0.27091529976520967\n",
            "Epoch 0 - Training loss: 0.2711067980032231\n",
            "Epoch 0 - Training loss: 0.2713668229324477\n",
            "Epoch 0 - Training loss: 0.2717367148062568\n",
            "Epoch 0 - Training loss: 0.2719269205988851\n",
            "Epoch 0 - Training loss: 0.2721977752885585\n",
            "Epoch 0 - Training loss: 0.2726096096736536\n",
            "Epoch 0 - Training loss: 0.273199634590764\n",
            "Epoch 0 - Training loss: 0.27357701075547286\n",
            "Epoch 0 - Training loss: 0.27384523512013176\n",
            "Epoch 0 - Training loss: 0.27399492598990643\n",
            "Epoch 0 - Training loss: 0.27419477739313775\n",
            "Epoch 0 - Training loss: 0.27464643746678\n",
            "Epoch 0 - Training loss: 0.2752103440479429\n",
            "Epoch 0 - Training loss: 0.2755081650735473\n",
            "Epoch 0 - Training loss: 0.2757174046213693\n",
            "Epoch 0 - Training loss: 0.2760481303657042\n",
            "Epoch 0 - Training loss: 0.2763919939142046\n",
            "Epoch 0 - Training loss: 0.2767423763553471\n",
            "Epoch 0 - Training loss: 0.27718972808707243\n",
            "Epoch 0 - Training loss: 0.2774948066771666\n",
            "Epoch 0 - Training loss: 0.27779488756394843\n",
            "Epoch 0 - Training loss: 0.2780344614119672\n",
            "Epoch 0 - Training loss: 0.27846289413379455\n",
            "Epoch 0 - Training loss: 0.27870668246865526\n",
            "Epoch 0 - Training loss: 0.278905335853476\n",
            "Epoch 0 - Training loss: 0.2792890398328238\n",
            "Epoch 0 - Training loss: 0.2797604051511933\n",
            "Epoch 0 - Training loss: 0.28024203844988016\n",
            "Epoch 0 - Training loss: 0.2806857650055052\n",
            "Epoch 0 - Training loss: 0.2810413308584614\n",
            "Epoch 0 - Training loss: 0.2814328073660956\n",
            "Epoch 0 - Training loss: 0.2816988320143492\n",
            "Epoch 0 - Training loss: 0.2820132120109316\n",
            "Epoch 0 - Training loss: 0.28240977410377976\n",
            "Epoch 0 - Training loss: 0.28263254976793645\n",
            "Epoch 0 - Training loss: 0.282907130987024\n",
            "Epoch 0 - Training loss: 0.28310173797582006\n",
            "Epoch 0 - Training loss: 0.2833987361332501\n",
            "Epoch 0 - Training loss: 0.283673598027941\n",
            "Epoch 0 - Training loss: 0.2839348291251451\n",
            "Epoch 0 - Training loss: 0.28439757401056126\n",
            "Epoch 0 - Training loss: 0.28477477981274063\n",
            "Epoch 0 - Training loss: 0.2851022974863998\n",
            "Epoch 0 - Training loss: 0.28542405540056065\n",
            "Epoch 0 - Training loss: 0.2857358452321878\n",
            "Epoch 0 - Training loss: 0.2859450722617635\n",
            "Epoch 0 - Training loss: 0.28636643131658723\n",
            "Epoch 0 - Training loss: 0.2865902698402212\n",
            "Epoch 0 - Training loss: 0.2868168322262225\n",
            "Epoch 0 - Training loss: 0.2870599154247912\n",
            "Epoch 0 - Training loss: 0.28740303350219343\n",
            "Epoch 0 - Training loss: 0.2879401271436006\n",
            "Epoch 0 - Training loss: 0.28844245357998904\n",
            "Epoch 0 - Training loss: 0.28887319753863916\n",
            "Epoch 0 - Training loss: 0.28916502397642463\n",
            "Epoch 0 - Training loss: 0.2895168020113953\n",
            "Epoch 0 - Training loss: 0.2897845771807089\n",
            "Epoch 0 - Training loss: 0.2900481345429858\n",
            "Epoch 0 - Training loss: 0.2904112913461128\n",
            "Epoch 0 - Training loss: 0.29071279683474033\n",
            "Epoch 0 - Training loss: 0.29099111147780915\n",
            "Epoch 0 - Training loss: 0.29133181683798587\n",
            "Epoch 0 - Training loss: 0.2916572760862074\n",
            "Epoch 0 - Training loss: 0.2920539708279852\n",
            "Epoch 0 - Training loss: 0.2924679999094782\n",
            "Epoch 0 - Training loss: 0.2927496674091323\n",
            "Epoch 0 - Training loss: 0.2929642737737851\n",
            "Epoch 0 - Training loss: 0.293188686468708\n",
            "Epoch 0 - Training loss: 0.2934696208248769\n",
            "Epoch 0 - Training loss: 0.29375348632523757\n",
            "Epoch 0 - Training loss: 0.2938571116611012\n",
            "Epoch 0 - Training loss: 0.29426759112078243\n",
            "Epoch 0 - Training loss: 0.29462984819878646\n",
            "Epoch 0 - Training loss: 0.2950027249753475\n",
            "Epoch 0 - Training loss: 0.2953003891098347\n",
            "Epoch 0 - Training loss: 0.29575428089448635\n",
            "Epoch 0 - Training loss: 0.295923924482644\n",
            "Epoch 0 - Training loss: 0.29616150438690236\n",
            "Epoch 0 - Training loss: 0.29651019543504664\n",
            "Epoch 0 - Training loss: 0.2968138231159146\n",
            "Epoch 0 - Training loss: 0.29713406257315483\n",
            "Epoch 0 - Training loss: 0.29749775842340515\n",
            "Epoch 0 - Training loss: 0.29772258163102144\n",
            "Epoch 0 - Training loss: 0.2980513977708974\n",
            "Epoch 0 - Training loss: 0.2983556368441851\n",
            "Epoch 0 - Training loss: 0.2987607347606215\n",
            "Epoch 0 - Training loss: 0.2989781680884265\n",
            "Epoch 0 - Training loss: 0.2992469454799761\n",
            "Epoch 0 - Training loss: 0.29968468215006755\n",
            "Epoch 0 - Training loss: 0.3000497880187243\n",
            "Epoch 0 - Training loss: 0.30027070353184937\n",
            "Epoch 0 - Training loss: 0.3004671861923961\n",
            "Epoch 0 - Training loss: 0.30076215801430917\n",
            "Epoch 0 - Training loss: 0.3010116625330977\n",
            "Epoch 0 - Training loss: 0.30139527017913903\n",
            "Epoch 0 - Training loss: 0.30169144299016326\n",
            "Epoch 0 - Training loss: 0.30189582286899025\n",
            "Epoch 0 - Training loss: 0.3023144186639201\n",
            "Epoch 0 - Training loss: 0.30254472851721464\n",
            "Epoch 0 - Training loss: 0.3028565938078137\n",
            "Epoch 0 - Training loss: 0.3031059433259308\n",
            "Epoch 0 - Training loss: 0.3034066572737719\n",
            "Epoch 0 - Training loss: 0.3036942887328454\n",
            "Epoch 0 - Training loss: 0.30409716348499377\n",
            "Epoch 0 - Training loss: 0.30451480551029064\n",
            "Epoch 0 - Training loss: 0.3047256293152568\n",
            "Epoch 0 - Training loss: 0.3050687988000765\n",
            "Epoch 0 - Training loss: 0.30533891103701044\n",
            "Epoch 0 - Training loss: 0.30555056679699977\n",
            "Epoch 0 - Training loss: 0.3058219736239422\n",
            "Epoch 0 - Training loss: 0.30608928787396916\n",
            "Epoch 0 - Training loss: 0.30655228512595967\n",
            "Epoch 0 - Training loss: 0.30685094198279544\n",
            "Epoch 0 - Training loss: 0.30715253017445615\n",
            "Epoch 0 - Training loss: 0.30734477123058934\n",
            "Epoch 0 - Training loss: 0.3076305614272033\n",
            "Epoch 0 - Training loss: 0.308237243364297\n",
            "Epoch 0 - Training loss: 0.3085841511024722\n",
            "Epoch 0 - Training loss: 0.3089282252371057\n",
            "Epoch 0 - Training loss: 0.30923817393336217\n",
            "Epoch 0 - Training loss: 0.30950193497131884\n",
            "Epoch 0 - Training loss: 0.3096563176734488\n",
            "Epoch 0 - Training loss: 0.31004134874576444\n",
            "Epoch 0 - Training loss: 0.31035524202403486\n",
            "Epoch 0 - Training loss: 0.3108428776947293\n",
            "Epoch 0 - Training loss: 0.31111409976633625\n",
            "Epoch 0 - Training loss: 0.31142087985298783\n",
            "Epoch 0 - Training loss: 0.3116925313894047\n",
            "Epoch 0 - Training loss: 0.3119795895826969\n",
            "Epoch 0 - Training loss: 0.31218778913113865\n",
            "Epoch 0 - Training loss: 0.3125327614658296\n",
            "Epoch 0 - Training loss: 0.3132742164946441\n",
            "Epoch 0 - Training loss: 0.3135789531523358\n",
            "Epoch 0 - Training loss: 0.3138299564133956\n",
            "Epoch 0 - Training loss: 0.3143030403853098\n",
            "Epoch 0 - Training loss: 0.3144925448225378\n",
            "Epoch 0 - Training loss: 0.314671580209089\n",
            "Epoch 0 - Training loss: 0.31503745026266905\n",
            "Epoch 0 - Training loss: 0.3154065614300115\n",
            "Epoch 0 - Training loss: 0.3157415585969684\n",
            "Epoch 0 - Training loss: 0.31600528786272636\n",
            "Epoch 0 - Training loss: 0.3164034062214116\n",
            "Epoch 0 - Training loss: 0.31662197248227814\n",
            "Epoch 0 - Training loss: 0.31677065800025517\n",
            "Epoch 0 - Training loss: 0.31715390601677934\n",
            "Epoch 0 - Training loss: 0.3173422596173119\n",
            "Epoch 0 - Training loss: 0.31755265991475534\n",
            "Epoch 0 - Training loss: 0.3178536778589937\n",
            "Epoch 0 - Training loss: 0.31822101562929306\n",
            "Epoch 0 - Training loss: 0.31835767600550324\n",
            "Epoch 0 - Training loss: 0.31855666573081953\n",
            "Epoch 0 - Training loss: 0.3189081717322249\n",
            "Epoch 0 - Training loss: 0.3191652916975494\n",
            "Epoch 0 - Training loss: 0.319551675312364\n",
            "Epoch 0 - Training loss: 0.3197484930584044\n",
            "Epoch 0 - Training loss: 0.3201514492426981\n",
            "Epoch 0 - Training loss: 0.32045383567907915\n",
            "Epoch 0 - Training loss: 0.3208637061292556\n",
            "Epoch 0 - Training loss: 0.32126931904126077\n",
            "Epoch 0 - Training loss: 0.3214713607245544\n",
            "Epoch 0 - Training loss: 0.32165059741975655\n",
            "Epoch 0 - Training loss: 0.32182211183440457\n",
            "Epoch 0 - Training loss: 0.3221738846905069\n",
            "Epoch 0 - Training loss: 0.3224967692786061\n",
            "Epoch 0 - Training loss: 0.3227371570350392\n",
            "Epoch 0 - Training loss: 0.3230913338392401\n",
            "Epoch 0 - Training loss: 0.32361072096139637\n",
            "Epoch 0 - Training loss: 0.3238431395831774\n",
            "Epoch 0 - Training loss: 0.3242483569948531\n",
            "Epoch 0 - Training loss: 0.32472975816585614\n",
            "Epoch 0 - Training loss: 0.32508507883275495\n",
            "Epoch 0 - Training loss: 0.3253060863263953\n",
            "Epoch 0 - Training loss: 0.3255854881187873\n",
            "Epoch 0 - Training loss: 0.3257907097384747\n",
            "Epoch 0 - Training loss: 0.3260091429850313\n",
            "Epoch 0 - Training loss: 0.3261919231223526\n",
            "Epoch 0 - Training loss: 0.32629855585568496\n",
            "Epoch 0 - Training loss: 0.3265776900308473\n",
            "Epoch 0 - Training loss: 0.3270700668125773\n",
            "Epoch 0 - Training loss: 0.32734050423796496\n",
            "Epoch 0 - Training loss: 0.32754033155786966\n",
            "Epoch 0 - Training loss: 0.32776705838088543\n",
            "Epoch 0 - Training loss: 0.32805727168059806\n",
            "Epoch 0 - Training loss: 0.32838134111753153\n",
            "Epoch 0 - Training loss: 0.32880387412332523\n",
            "Epoch 0 - Training loss: 0.329422412555355\n",
            "Epoch 0 - Training loss: 0.3296074632451987\n",
            "Epoch 0 - Training loss: 0.33003333531844337\n",
            "Epoch 0 - Training loss: 0.330557025413015\n",
            "Epoch 0 - Training loss: 0.3308758598083118\n",
            "Epoch 0 - Training loss: 0.33097169798479154\n",
            "Epoch 0 - Training loss: 0.331198570102072\n",
            "Epoch 0 - Training loss: 0.33144383222214197\n",
            "Epoch 0 - Training loss: 0.3316380998997419\n",
            "Epoch 0 - Training loss: 0.33192965544776115\n",
            "Epoch 0 - Training loss: 0.33211036100348174\n",
            "Epoch 0 - Training loss: 0.33250973099616293\n",
            "Epoch 0 - Training loss: 0.3326132357406464\n",
            "Epoch 0 - Training loss: 0.3329183495661089\n",
            "Epoch 0 - Training loss: 0.33319611302507457\n",
            "Epoch 0 - Training loss: 0.3337554419949365\n",
            "Epoch 0 - Training loss: 0.33404327450848337\n",
            "Epoch 0 - Training loss: 0.3342913443854114\n",
            "Epoch 0 - Training loss: 0.33462059380276116\n",
            "Epoch 0 - Training loss: 0.33492378077146084\n",
            "Epoch 0 - Training loss: 0.33514114790188987\n",
            "Epoch 0 - Training loss: 0.33558162003120123\n",
            "Epoch 0 - Training loss: 0.335916158915964\n",
            "Epoch 0 - Training loss: 0.3361924423623695\n",
            "Epoch 0 - Training loss: 0.3365164859685054\n",
            "Epoch 0 - Training loss: 0.3368765714485\n",
            "Epoch 0 - Training loss: 0.3372235112130515\n",
            "Epoch 0 - Training loss: 0.33768965820196084\n",
            "Epoch 0 - Training loss: 0.33796418194514094\n",
            "Epoch 0 - Training loss: 0.3382279401394858\n",
            "Epoch 0 - Training loss: 0.3384876524461612\n",
            "Epoch 0 - Training loss: 0.3388032090943505\n",
            "Epoch 0 - Training loss: 0.338924480653775\n",
            "Epoch 0 - Training loss: 0.3391801895839827\n",
            "Epoch 0 - Training loss: 0.33940651218520046\n",
            "Epoch 0 - Training loss: 0.3396732215243362\n",
            "Epoch 0 - Training loss: 0.3399659602054909\n",
            "Epoch 0 - Training loss: 0.3401535372458287\n",
            "Epoch 0 - Training loss: 0.3403980268566593\n",
            "Epoch 0 - Training loss: 0.3405767577066859\n",
            "Epoch 0 - Training loss: 0.3407629916408677\n",
            "Epoch 0 - Training loss: 0.3409320183559013\n",
            "Epoch 0 - Training loss: 0.3412784026471028\n",
            "Epoch 0 - Training loss: 0.3413792482094724\n",
            "Epoch 0 - Training loss: 0.34169235852545005\n",
            "Epoch 0 - Training loss: 0.3420288345293958\n",
            "Epoch 0 - Training loss: 0.34230544306893845\n",
            "Epoch 0 - Training loss: 0.3427092748314841\n",
            "Epoch 0 - Training loss: 0.3430188881086388\n",
            "Epoch 0 - Training loss: 0.34344086529158835\n",
            "Epoch 0 - Training loss: 0.3438025192697165\n",
            "Epoch 0 - Training loss: 0.3439432403553269\n",
            "Epoch 0 - Training loss: 0.344263464752545\n",
            "Epoch 0 - Training loss: 0.34465450343928106\n",
            "Epoch 0 - Training loss: 0.344820224558875\n",
            "Epoch 0 - Training loss: 0.34506358214215177\n",
            "Epoch 0 - Training loss: 0.34541142480904613\n",
            "Epoch 0 - Training loss: 0.3457856501248091\n",
            "Epoch 0 - Training loss: 0.34606146118216424\n",
            "Epoch 0 - Training loss: 0.34631126272335233\n",
            "Epoch 0 - Training loss: 0.3465649619507891\n",
            "Epoch 0 - Training loss: 0.34694320889614794\n",
            "Epoch 0 - Training loss: 0.34744940582178296\n",
            "Epoch 0 - Training loss: 0.34779163627926984\n",
            "Epoch 0 - Training loss: 0.3482777424045463\n",
            "Epoch 0 - Training loss: 0.3487252223053212\n",
            "Epoch 0 - Training loss: 0.3490819391379478\n",
            "Epoch 0 - Training loss: 0.3492570287192554\n",
            "Epoch 0 - Training loss: 0.3495315798182986\n",
            "Epoch 0 - Training loss: 0.34979878953779175\n",
            "Epoch 0 - Training loss: 0.35000385523541394\n",
            "Epoch 0 - Training loss: 0.35030631697190595\n",
            "Epoch 0 - Training loss: 0.35055399199983456\n",
            "Epoch 0 - Training loss: 0.3510093556633636\n",
            "Epoch 0 - Training loss: 0.35130971868727\n",
            "Epoch 0 - Training loss: 0.35151553757663473\n",
            "Epoch 0 - Training loss: 0.35185523286684234\n",
            "Epoch 0 - Training loss: 0.3520447443892707\n",
            "Epoch 0 - Training loss: 0.35255252776433155\n",
            "Epoch 0 - Training loss: 0.352849024556466\n",
            "Epoch 0 - Training loss: 0.35310737086511623\n",
            "Epoch 0 - Training loss: 0.3534622922825661\n",
            "Epoch 0 - Training loss: 0.3537236159480711\n",
            "Epoch 0 - Training loss: 0.353876294167057\n",
            "Epoch 0 - Training loss: 0.35405325603637616\n",
            "Epoch 0 - Training loss: 0.3543418330996275\n",
            "Epoch 0 - Training loss: 0.35447984824239065\n",
            "Epoch 0 - Training loss: 0.3546168702497665\n",
            "Epoch 0 - Training loss: 0.3548483700831053\n",
            "Epoch 0 - Training loss: 0.35521082402165255\n",
            "Epoch 0 - Training loss: 0.355485072172781\n",
            "Epoch 0 - Training loss: 0.3557765734856571\n",
            "Epoch 0 - Training loss: 0.3560830721024003\n",
            "Epoch 0 - Training loss: 0.35628243017870226\n",
            "Epoch 0 - Training loss: 0.356819779904031\n",
            "Epoch 0 - Training loss: 0.35702807451489127\n",
            "Epoch 0 - Training loss: 0.3572541489275788\n",
            "Epoch 0 - Training loss: 0.35735780562260255\n",
            "Epoch 0 - Training loss: 0.3575232668614972\n",
            "Epoch 0 - Training loss: 0.35768642380579446\n",
            "Epoch 0 - Training loss: 0.35788564136954765\n",
            "Epoch 0 - Training loss: 0.3581931250197674\n",
            "Epoch 0 - Training loss: 0.35841715846584044\n",
            "Epoch 0 - Training loss: 0.3586815161698027\n",
            "Epoch 0 - Training loss: 0.359080596622437\n",
            "Epoch 0 - Training loss: 0.3592015935985773\n",
            "Epoch 0 - Training loss: 0.3595053198050334\n",
            "Epoch 0 - Training loss: 0.35986762663830063\n",
            "Epoch 0 - Training loss: 0.3601832635112917\n",
            "Epoch 0 - Training loss: 0.3603817017983272\n",
            "Epoch 0 - Training loss: 0.3607768437374375\n",
            "Epoch 0 - Training loss: 0.36091799217500664\n",
            "Epoch 0 - Training loss: 0.3610393424897687\n",
            "Epoch 0 - Training loss: 0.3612463943946209\n",
            "Epoch 0 - Training loss: 0.3615327357117937\n",
            "Epoch 0 - Training loss: 0.361878259897804\n",
            "Epoch 0 - Training loss: 0.3623311038353423\n",
            "Epoch 0 - Training loss: 0.36246682141921416\n",
            "Epoch 0 - Training loss: 0.36259220810587217\n",
            "Epoch 0 - Training loss: 0.36297843263728785\n",
            "Epoch 0 - Training loss: 0.3632966549411766\n",
            "Epoch 0 - Training loss: 0.3637359725641035\n",
            "Epoch 0 - Training loss: 0.36420806015986623\n",
            "Epoch 0 - Training loss: 0.3645165893060566\n",
            "Epoch 0 - Training loss: 0.36487014664769934\n",
            "Epoch 0 - Training loss: 0.36512537160789027\n",
            "Epoch 0 - Training loss: 0.36528982595403564\n",
            "Epoch 0 - Training loss: 0.3657765471255347\n",
            "Epoch 0 - Training loss: 0.3661788663407887\n",
            "Epoch 0 - Training loss: 0.3663644668644171\n",
            "Epoch 0 - Training loss: 0.3667608578957474\n",
            "Epoch 0 - Training loss: 0.36704226648375426\n",
            "Epoch 0 - Training loss: 0.3673983912096857\n",
            "Epoch 0 - Training loss: 0.3676221974527658\n",
            "Epoch 0 - Training loss: 0.3678625812694462\n",
            "Epoch 0 - Training loss: 0.36805681663472006\n",
            "Epoch 0 - Training loss: 0.3682580858405465\n",
            "Epoch 0 - Training loss: 0.36838248813711505\n",
            "Epoch 0 - Training loss: 0.3686880291715614\n",
            "Epoch 0 - Training loss: 0.36891977339665266\n",
            "Epoch 0 - Training loss: 0.3693655725798881\n",
            "Epoch 0 - Training loss: 0.36980604709211445\n",
            "Epoch 0 - Training loss: 0.37002907598069484\n",
            "Epoch 0 - Training loss: 0.3703059248451485\n",
            "Epoch 0 - Training loss: 0.3705093230106937\n",
            "Epoch 0 - Training loss: 0.3708106605832511\n",
            "Epoch 0 - Training loss: 0.37112386592987506\n",
            "Epoch 0 - Training loss: 0.37155371581885355\n",
            "Epoch 0 - Training loss: 0.37190341118619896\n",
            "Epoch 0 - Training loss: 0.3721211609174448\n",
            "Epoch 0 - Training loss: 0.37227600323620125\n",
            "Epoch 0 - Training loss: 0.37265410815983185\n",
            "Epoch 0 - Training loss: 0.3728750260399857\n",
            "Epoch 0 - Training loss: 0.37310024695609934\n",
            "Epoch 0 - Training loss: 0.3732812953179579\n",
            "Epoch 0 - Training loss: 0.37353759394017366\n",
            "Epoch 0 - Training loss: 0.3738305903574043\n",
            "Epoch 0 - Training loss: 0.37409509939234903\n",
            "Epoch 0 - Training loss: 0.37430409774152457\n",
            "Epoch 0 - Training loss: 0.37444822728506794\n",
            "Epoch 0 - Training loss: 0.37477174807967406\n",
            "Epoch 0 - Training loss: 0.37492825820057124\n",
            "Epoch 0 - Training loss: 0.3752062125469067\n",
            "Epoch 0 - Training loss: 0.37549900730599217\n",
            "Epoch 0 - Training loss: 0.37614823780906226\n",
            "Epoch 0 - Training loss: 0.3764537375396503\n",
            "Epoch 0 - Training loss: 0.3768888880163113\n",
            "Epoch 0 - Training loss: 0.3771533995771459\n",
            "Epoch 0 - Training loss: 0.3774461053423028\n",
            "Epoch 0 - Training loss: 0.37763051936494263\n",
            "Epoch 0 - Training loss: 0.37778857278861977\n",
            "Epoch 0 - Training loss: 0.37797708850679623\n",
            "Epoch 0 - Training loss: 0.3781716094723643\n",
            "Epoch 0 - Training loss: 0.37836570132261654\n",
            "Epoch 0 - Training loss: 0.37853491433393727\n",
            "Epoch 0 - Training loss: 0.37898170210913557\n",
            "Epoch 0 - Training loss: 0.3792093291974017\n",
            "Epoch 0 - Training loss: 0.37940065405452683\n",
            "Epoch 0 - Training loss: 0.37956269175958024\n",
            "Epoch 0 - Training loss: 0.37993365416585256\n",
            "Epoch 0 - Training loss: 0.3801614088512687\n",
            "Epoch 0 - Training loss: 0.38039031478641894\n",
            "Epoch 0 - Training loss: 0.3806061856686942\n",
            "Epoch 0 - Training loss: 0.3808413214330226\n",
            "Epoch 0 - Training loss: 0.38108914536136046\n",
            "Epoch 0 - Training loss: 0.38132129012267474\n",
            "Epoch 0 - Training loss: 0.38154417501965054\n",
            "Epoch 0 - Training loss: 0.3818133237328865\n",
            "Epoch 0 - Training loss: 0.38227084676212847\n",
            "Epoch 0 - Training loss: 0.3824116965251437\n",
            "Epoch 0 - Training loss: 0.382621996390667\n",
            "Epoch 0 - Training loss: 0.38294580313506155\n",
            "Epoch 0 - Training loss: 0.38337979526090216\n",
            "Epoch 0 - Training loss: 0.383677442714985\n",
            "Epoch 0 - Training loss: 0.3838819375456269\n",
            "Epoch 0 - Training loss: 0.3842155168782165\n",
            "Epoch 0 - Training loss: 0.3844130615721633\n",
            "Epoch 0 - Training loss: 0.384674645166026\n",
            "Epoch 0 - Training loss: 0.384807485642273\n",
            "Epoch 0 - Training loss: 0.38547992732510894\n",
            "Epoch 0 - Training loss: 0.38578249479947824\n",
            "Epoch 0 - Training loss: 0.38598896895866913\n",
            "Epoch 0 - Training loss: 0.38631661327631234\n",
            "Epoch 0 - Training loss: 0.38643688871376297\n",
            "Epoch 0 - Training loss: 0.3865527758704447\n",
            "Epoch 0 - Training loss: 0.38676664610502565\n",
            "Epoch 0 - Training loss: 0.38688646637395757\n",
            "Epoch 0 - Training loss: 0.3871123869933172\n",
            "Epoch 0 - Training loss: 0.38744709430250535\n",
            "Epoch 0 - Training loss: 0.38773325442282885\n",
            "Epoch 0 - Training loss: 0.3881090637415584\n",
            "Epoch 0 - Training loss: 0.3882948092417295\n",
            "Epoch 0 - Training loss: 0.3885764974886293\n",
            "Epoch 0 - Training loss: 0.38891840501348857\n",
            "Epoch 0 - Training loss: 0.38915434566292684\n",
            "Epoch 0 - Training loss: 0.3894757442652925\n",
            "Epoch 0 - Training loss: 0.38970718281006\n",
            "Epoch 0 - Training loss: 0.3899946411424227\n",
            "Epoch 0 - Training loss: 0.3903094140975587\n",
            "Epoch 0 - Training loss: 0.39044205340812965\n",
            "Epoch 0 - Training loss: 0.3907217986579897\n",
            "Epoch 0 - Training loss: 0.3908521726346219\n",
            "Epoch 0 - Training loss: 0.3912953639557875\n",
            "Epoch 0 - Training loss: 0.3916076997608772\n",
            "Epoch 0 - Training loss: 0.3921450722986447\n",
            "Epoch 0 - Training loss: 0.39243742713986685\n",
            "Epoch 0 - Training loss: 0.3926618956267707\n",
            "Epoch 0 - Training loss: 0.3928073583953162\n",
            "Epoch 0 - Training loss: 0.3929795125749574\n",
            "Epoch 0 - Training loss: 0.3932184207000966\n",
            "Epoch 0 - Training loss: 0.3934057455486072\n",
            "Epoch 0 - Training loss: 0.39351073805012426\n",
            "Epoch 0 - Training loss: 0.3937431645196384\n",
            "Epoch 0 - Training loss: 0.39415704843395555\n",
            "Epoch 0 - Training loss: 0.39429736126269865\n",
            "Epoch 0 - Training loss: 0.3945030526820022\n",
            "Epoch 0 - Training loss: 0.3947275219314388\n",
            "Epoch 0 - Training loss: 0.394907325935135\n",
            "Epoch 0 - Training loss: 0.3951143868156333\n",
            "Epoch 0 - Training loss: 0.39539770354657794\n",
            "Epoch 0 - Training loss: 0.3955670376894062\n",
            "Epoch 0 - Training loss: 0.395769805796365\n",
            "Epoch 0 - Training loss: 0.3960088158467177\n",
            "Epoch 0 - Training loss: 0.396141410445862\n",
            "Epoch 0 - Training loss: 0.3964502826047096\n",
            "Epoch 0 - Training loss: 0.3967206310679409\n",
            "Epoch 0 - Training loss: 0.3968670950102399\n",
            "Epoch 0 - Training loss: 0.39702068061144874\n",
            "Epoch 0 - Training loss: 0.3974614992165871\n",
            "Epoch 0 - Training loss: 0.39758765147820213\n",
            "Epoch 0 - Training loss: 0.39775608000216456\n",
            "Epoch 0 - Training loss: 0.39796448392527445\n",
            "Epoch 0 - Training loss: 0.3983473752670959\n",
            "Epoch 0 - Training loss: 0.3988534670585254\n",
            "Epoch 0 - Training loss: 0.39917122196159893\n",
            "Epoch 0 - Training loss: 0.39946444958511956\n",
            "Epoch 0 - Training loss: 0.39972154948630056\n",
            "Epoch 0 - Training loss: 0.4001019652972598\n",
            "Epoch 0 - Training loss: 0.4004349236739978\n",
            "Epoch 0 - Training loss: 0.40062553859722894\n",
            "Epoch 0 - Training loss: 0.40103436469523385\n",
            "Epoch 0 - Training loss: 0.40123554400162403\n",
            "Epoch 0 - Training loss: 0.40142843770637693\n",
            "Epoch 0 - Training loss: 0.4016098908142749\n",
            "Epoch 0 - Training loss: 0.4018614500093816\n",
            "Epoch 0 - Training loss: 0.40210956716334134\n",
            "Epoch 0 - Training loss: 0.4024060022856381\n",
            "Epoch 0 - Training loss: 0.40266734336230803\n",
            "Epoch 0 - Training loss: 0.4031706309077074\n",
            "Epoch 0 - Training loss: 0.40335226999417045\n",
            "Epoch 0 - Training loss: 0.40354056171834596\n",
            "Epoch 0 - Training loss: 0.40394544498180784\n",
            "Epoch 0 - Training loss: 0.4044196223939406\n",
            "Epoch 0 - Training loss: 0.40465510655631387\n",
            "Epoch 0 - Training loss: 0.4048839902826972\n",
            "Epoch 0 - Training loss: 0.4052860244695566\n",
            "Epoch 0 - Training loss: 0.4055492611232597\n",
            "Epoch 0 - Training loss: 0.4058031284415137\n",
            "Epoch 0 - Training loss: 0.4059622141439269\n",
            "Epoch 0 - Training loss: 0.4061777922771633\n",
            "Epoch 0 - Training loss: 0.4064886538204608\n",
            "Epoch 0 - Training loss: 0.40665592377119736\n",
            "Epoch 0 - Training loss: 0.4068188599940302\n",
            "Epoch 0 - Training loss: 0.4069932334140928\n",
            "Epoch 0 - Training loss: 0.40739519747970965\n",
            "Epoch 0 - Training loss: 0.4077369515766213\n",
            "Epoch 0 - Training loss: 0.4078422001144017\n",
            "Epoch 0 - Training loss: 0.4081395038886111\n",
            "Epoch 0 - Training loss: 0.4084404465009663\n",
            "Epoch 0 - Training loss: 0.40864811639096943\n",
            "Epoch 0 - Training loss: 0.40900136948203736\n",
            "Epoch 0 - Training loss: 0.409309444730597\n",
            "Epoch 0 - Training loss: 0.40946428801840556\n",
            "Epoch 0 - Training loss: 0.40965625538881906\n",
            "Epoch 0 - Training loss: 0.40985724749341446\n",
            "Epoch 0 - Training loss: 0.4101026345894281\n",
            "Epoch 0 - Training loss: 0.4103621472078346\n",
            "Epoch 0 - Training loss: 0.4105088245322201\n",
            "Epoch 0 - Training loss: 0.4108053502370554\n",
            "Epoch 0 - Training loss: 0.4111257514465592\n",
            "Epoch 0 - Training loss: 0.41132104311035134\n",
            "Epoch 0 - Training loss: 0.41146743971147515\n",
            "Epoch 0 - Training loss: 0.4118763535642929\n",
            "Epoch 0 - Training loss: 0.4120576804889036\n",
            "Epoch 0 - Training loss: 0.4121913779646095\n",
            "Epoch 0 - Training loss: 0.412539113050839\n",
            "Epoch 0 - Training loss: 0.4127768531664094\n",
            "Epoch 0 - Training loss: 0.41310053094744936\n",
            "Epoch 0 - Training loss: 0.41335058226577764\n",
            "Epoch 0 - Training loss: 0.4135797844766808\n",
            "Epoch 0 - Training loss: 0.41376075618811\n",
            "Epoch 0 - Training loss: 0.4140352634732911\n",
            "Epoch 0 - Training loss: 0.4142939298868433\n",
            "Epoch 0 - Training loss: 0.4146048668891128\n",
            "Epoch 0 - Training loss: 0.41488803851642586\n",
            "Epoch 0 - Training loss: 0.4152037750588043\n",
            "Epoch 0 - Training loss: 0.4154482838600429\n",
            "Epoch 0 - Training loss: 0.415614343973111\n",
            "Epoch 0 - Training loss: 0.4159942984835171\n",
            "Epoch 0 - Training loss: 0.4164743691301549\n",
            "Epoch 0 - Training loss: 0.4166678200239566\n",
            "Epoch 0 - Training loss: 0.41687277826800273\n",
            "Epoch 0 - Training loss: 0.41703865127459266\n",
            "Epoch 0 - Training loss: 0.41745110080122694\n",
            "Epoch 0 - Training loss: 0.4178155480004323\n",
            "Epoch 0 - Training loss: 0.4179459307064761\n",
            "Epoch 0 - Training loss: 0.4184993078538985\n",
            "Epoch 0 - Training loss: 0.41869786293553646\n",
            "Epoch 0 - Training loss: 0.41885034707404656\n",
            "Epoch 0 - Training loss: 0.41926390440193323\n",
            "Epoch 0 - Training loss: 0.41944885810714033\n",
            "Epoch 0 - Training loss: 0.4196804280299495\n",
            "Epoch 0 - Training loss: 0.42017399051836307\n",
            "Epoch 0 - Training loss: 0.4202352382544515\n",
            "Epoch 0 - Training loss: 0.42059940172792243\n",
            "Epoch 0 - Training loss: 0.4208266233711609\n",
            "Epoch 0 - Training loss: 0.42103451685801246\n",
            "Epoch 0 - Training loss: 0.42133776740288176\n",
            "Epoch 0 - Training loss: 0.42154933551926094\n",
            "Epoch 0 - Training loss: 0.42174245425061124\n",
            "Epoch 0 - Training loss: 0.42194815466144703\n",
            "Epoch 0 - Training loss: 0.4222447044813811\n",
            "Epoch 0 - Training loss: 0.42248789684922455\n",
            "Epoch 0 - Training loss: 0.42265637031496206\n",
            "Epoch 0 - Training loss: 0.4228094337083129\n",
            "Epoch 0 - Training loss: 0.42307383293853895\n",
            "Epoch 0 - Training loss: 0.42324846650936454\n",
            "Epoch 0 - Training loss: 0.4233916552939903\n",
            "Epoch 0 - Training loss: 0.4235304778636391\n",
            "Epoch 0 - Training loss: 0.4237542525410398\n",
            "Epoch 0 - Training loss: 0.42412815565493567\n",
            "Epoch 0 - Training loss: 0.4244043778763143\n",
            "Epoch 0 - Training loss: 0.4246132380005393\n",
            "Epoch 0 - Training loss: 0.42469233740716855\n",
            "Epoch 0 - Training loss: 0.42494437722032513\n",
            "Epoch 0 - Training loss: 0.42504840561035856\n",
            "Epoch 0 - Training loss: 0.42525287805748646\n",
            "Epoch 0 - Training loss: 0.4253518781634663\n",
            "Epoch 1 - Training loss: 9.812946830477034e-05\n",
            "Epoch 1 - Training loss: 0.00023987466719613146\n",
            "Epoch 1 - Training loss: 0.000641914001151697\n",
            "Epoch 1 - Training loss: 0.0007653241313850956\n",
            "Epoch 1 - Training loss: 0.0009698761360985893\n",
            "Epoch 1 - Training loss: 0.0011945192112343143\n",
            "Epoch 1 - Training loss: 0.0016256988302731055\n",
            "Epoch 1 - Training loss: 0.0017533625668681252\n",
            "Epoch 1 - Training loss: 0.001970643451663731\n",
            "Epoch 1 - Training loss: 0.0020875594239118003\n",
            "Epoch 1 - Training loss: 0.002370204840069895\n",
            "Epoch 1 - Training loss: 0.0026462366188894203\n",
            "Epoch 1 - Training loss: 0.0029077372237690476\n",
            "Epoch 1 - Training loss: 0.003316870336530051\n",
            "Epoch 1 - Training loss: 0.0035314656762299\n",
            "Epoch 1 - Training loss: 0.0039359042877708675\n",
            "Epoch 1 - Training loss: 0.004088629577269178\n",
            "Epoch 1 - Training loss: 0.0043941103080823735\n",
            "Epoch 1 - Training loss: 0.004561997560867623\n",
            "Epoch 1 - Training loss: 0.004782250099408347\n",
            "Epoch 1 - Training loss: 0.004997603984466239\n",
            "Epoch 1 - Training loss: 0.00518136580329714\n",
            "Epoch 1 - Training loss: 0.005703868634347469\n",
            "Epoch 1 - Training loss: 0.005957196270034258\n",
            "Epoch 1 - Training loss: 0.006097469748909286\n",
            "Epoch 1 - Training loss: 0.006247246562481435\n",
            "Epoch 1 - Training loss: 0.006400020209266178\n",
            "Epoch 1 - Training loss: 0.006597952396948455\n",
            "Epoch 1 - Training loss: 0.006757216325510285\n",
            "Epoch 1 - Training loss: 0.0068912638037570756\n",
            "Epoch 1 - Training loss: 0.006999939187630407\n",
            "Epoch 1 - Training loss: 0.007132428231587542\n",
            "Epoch 1 - Training loss: 0.00738073647943641\n",
            "Epoch 1 - Training loss: 0.007551920955686935\n",
            "Epoch 1 - Training loss: 0.00769760109372993\n",
            "Epoch 1 - Training loss: 0.007774325401416974\n",
            "Epoch 1 - Training loss: 0.007917131823517366\n",
            "Epoch 1 - Training loss: 0.007992332137978153\n",
            "Epoch 1 - Training loss: 0.00819027882966914\n",
            "Epoch 1 - Training loss: 0.008421166666916438\n",
            "Epoch 1 - Training loss: 0.00857038465517162\n",
            "Epoch 1 - Training loss: 0.008734253646214125\n",
            "Epoch 1 - Training loss: 0.008799527003280898\n",
            "Epoch 1 - Training loss: 0.008958309535374012\n",
            "Epoch 1 - Training loss: 0.00914993924674576\n",
            "Epoch 1 - Training loss: 0.009300377366861809\n",
            "Epoch 1 - Training loss: 0.009430187394910021\n",
            "Epoch 1 - Training loss: 0.009607825313073231\n",
            "Epoch 1 - Training loss: 0.009742794185082542\n",
            "Epoch 1 - Training loss: 0.009903068707854764\n",
            "Epoch 1 - Training loss: 0.010049250989612232\n",
            "Epoch 1 - Training loss: 0.010219348411856176\n",
            "Epoch 1 - Training loss: 0.010519248641916175\n",
            "Epoch 1 - Training loss: 0.01091524055088634\n",
            "Epoch 1 - Training loss: 0.011320013564818704\n",
            "Epoch 1 - Training loss: 0.011599355752565968\n",
            "Epoch 1 - Training loss: 0.011791984874334163\n",
            "Epoch 1 - Training loss: 0.012093616435046134\n",
            "Epoch 1 - Training loss: 0.012253187854947058\n",
            "Epoch 1 - Training loss: 0.012518526334911267\n",
            "Epoch 1 - Training loss: 0.012828555891413424\n",
            "Epoch 1 - Training loss: 0.013190909112090749\n",
            "Epoch 1 - Training loss: 0.01340070549946731\n",
            "Epoch 1 - Training loss: 0.01369601089769462\n",
            "Epoch 1 - Training loss: 0.013827521596223052\n",
            "Epoch 1 - Training loss: 0.014082579228129468\n",
            "Epoch 1 - Training loss: 0.014541458548989885\n",
            "Epoch 1 - Training loss: 0.014678376422984514\n",
            "Epoch 1 - Training loss: 0.014986148818946088\n",
            "Epoch 1 - Training loss: 0.015182286163351174\n",
            "Epoch 1 - Training loss: 0.015409790992196689\n",
            "Epoch 1 - Training loss: 0.015591527841715162\n",
            "Epoch 1 - Training loss: 0.015726093889903158\n",
            "Epoch 1 - Training loss: 0.015774970731215435\n",
            "Epoch 1 - Training loss: 0.015972597572182032\n",
            "Epoch 1 - Training loss: 0.016240630890610123\n",
            "Epoch 1 - Training loss: 0.016507316940724215\n",
            "Epoch 1 - Training loss: 0.016735345724898616\n",
            "Epoch 1 - Training loss: 0.01690523936026005\n",
            "Epoch 1 - Training loss: 0.017047344315757374\n",
            "Epoch 1 - Training loss: 0.017268653661171512\n",
            "Epoch 1 - Training loss: 0.01753429327803507\n",
            "Epoch 1 - Training loss: 0.017630799774771562\n",
            "Epoch 1 - Training loss: 0.017800909302223212\n",
            "Epoch 1 - Training loss: 0.017984694950203144\n",
            "Epoch 1 - Training loss: 0.018110316997366167\n",
            "Epoch 1 - Training loss: 0.018398544598203986\n",
            "Epoch 1 - Training loss: 0.01856426254057808\n",
            "Epoch 1 - Training loss: 0.018877538262622187\n",
            "Epoch 1 - Training loss: 0.019238106457631726\n",
            "Epoch 1 - Training loss: 0.019609890937773402\n",
            "Epoch 1 - Training loss: 0.019772992439583928\n",
            "Epoch 1 - Training loss: 0.019947396821654174\n",
            "Epoch 1 - Training loss: 0.020238449225134687\n",
            "Epoch 1 - Training loss: 0.02037357123914177\n",
            "Epoch 1 - Training loss: 0.020833716555031887\n",
            "Epoch 1 - Training loss: 0.02107628526638693\n",
            "Epoch 1 - Training loss: 0.02121410113391973\n",
            "Epoch 1 - Training loss: 0.021362271926391608\n",
            "Epoch 1 - Training loss: 0.021666432045765523\n",
            "Epoch 1 - Training loss: 0.02186461667945263\n",
            "Epoch 1 - Training loss: 0.022065628117430947\n",
            "Epoch 1 - Training loss: 0.022257551864615636\n",
            "Epoch 1 - Training loss: 0.022538962867309543\n",
            "Epoch 1 - Training loss: 0.02271353635690741\n",
            "Epoch 1 - Training loss: 0.023053553817209912\n",
            "Epoch 1 - Training loss: 0.023249760822160667\n",
            "Epoch 1 - Training loss: 0.02336287316975436\n",
            "Epoch 1 - Training loss: 0.023724734016668313\n",
            "Epoch 1 - Training loss: 0.024008150226367053\n",
            "Epoch 1 - Training loss: 0.02434661739400582\n",
            "Epoch 1 - Training loss: 0.024537193126229845\n",
            "Epoch 1 - Training loss: 0.024710956949001946\n",
            "Epoch 1 - Training loss: 0.025011066621395826\n",
            "Epoch 1 - Training loss: 0.02526984077447386\n",
            "Epoch 1 - Training loss: 0.02563274200203449\n",
            "Epoch 1 - Training loss: 0.02572489044924916\n",
            "Epoch 1 - Training loss: 0.026004403920919655\n",
            "Epoch 1 - Training loss: 0.02630105782657671\n",
            "Epoch 1 - Training loss: 0.026483085232852366\n",
            "Epoch 1 - Training loss: 0.026686213159961485\n",
            "Epoch 1 - Training loss: 0.02690806487269366\n",
            "Epoch 1 - Training loss: 0.027044837547740196\n",
            "Epoch 1 - Training loss: 0.027234748907403142\n",
            "Epoch 1 - Training loss: 0.027451766797823948\n",
            "Epoch 1 - Training loss: 0.02766244248079974\n",
            "Epoch 1 - Training loss: 0.027929179302887366\n",
            "Epoch 1 - Training loss: 0.028096213360934624\n",
            "Epoch 1 - Training loss: 0.028193531343455255\n",
            "Epoch 1 - Training loss: 0.028303066419481215\n",
            "Epoch 1 - Training loss: 0.028552470093311022\n",
            "Epoch 1 - Training loss: 0.028727189496731455\n",
            "Epoch 1 - Training loss: 0.029035209493437554\n",
            "Epoch 1 - Training loss: 0.029438471230966195\n",
            "Epoch 1 - Training loss: 0.029480489284626203\n",
            "Epoch 1 - Training loss: 0.029784346360768845\n",
            "Epoch 1 - Training loss: 0.029973758356784706\n",
            "Epoch 1 - Training loss: 0.030122423890048763\n",
            "Epoch 1 - Training loss: 0.030322675019312007\n",
            "Epoch 1 - Training loss: 0.030566974505305544\n",
            "Epoch 1 - Training loss: 0.0309315787544891\n",
            "Epoch 1 - Training loss: 0.031090085217947646\n",
            "Epoch 1 - Training loss: 0.03129263342951915\n",
            "Epoch 1 - Training loss: 0.031534060724635625\n",
            "Epoch 1 - Training loss: 0.0317400660373763\n",
            "Epoch 1 - Training loss: 0.031972425038626455\n",
            "Epoch 1 - Training loss: 0.0321691213672095\n",
            "Epoch 1 - Training loss: 0.032317663799089666\n",
            "Epoch 1 - Training loss: 0.032425965788140734\n",
            "Epoch 1 - Training loss: 0.032606942344830234\n",
            "Epoch 1 - Training loss: 0.03293833270001767\n",
            "Epoch 1 - Training loss: 0.03310659238651617\n",
            "Epoch 1 - Training loss: 0.03352581609540911\n",
            "Epoch 1 - Training loss: 0.03373279522604017\n",
            "Epoch 1 - Training loss: 0.03407021762846884\n",
            "Epoch 1 - Training loss: 0.034432221196099386\n",
            "Epoch 1 - Training loss: 0.03460863556689037\n",
            "Epoch 1 - Training loss: 0.0347299258242538\n",
            "Epoch 1 - Training loss: 0.03493819021975308\n",
            "Epoch 1 - Training loss: 0.035109878031175525\n",
            "Epoch 1 - Training loss: 0.03544656074504608\n",
            "Epoch 1 - Training loss: 0.03551883963760791\n",
            "Epoch 1 - Training loss: 0.03562404432975407\n",
            "Epoch 1 - Training loss: 0.035991727908664166\n",
            "Epoch 1 - Training loss: 0.03621013357695232\n",
            "Epoch 1 - Training loss: 0.03641701173553589\n",
            "Epoch 1 - Training loss: 0.036638024058550406\n",
            "Epoch 1 - Training loss: 0.03683667053291793\n",
            "Epoch 1 - Training loss: 0.037057906595755745\n",
            "Epoch 1 - Training loss: 0.0371843303587518\n",
            "Epoch 1 - Training loss: 0.03739815595339356\n",
            "Epoch 1 - Training loss: 0.037504079658339524\n",
            "Epoch 1 - Training loss: 0.037631600356495966\n",
            "Epoch 1 - Training loss: 0.03812785264747992\n",
            "Epoch 1 - Training loss: 0.038271338645138465\n",
            "Epoch 1 - Training loss: 0.03844620221491053\n",
            "Epoch 1 - Training loss: 0.03861142295414705\n",
            "Epoch 1 - Training loss: 0.03901466629557264\n",
            "Epoch 1 - Training loss: 0.03924024609455676\n",
            "Epoch 1 - Training loss: 0.03942896339144788\n",
            "Epoch 1 - Training loss: 0.03961481912526241\n",
            "Epoch 1 - Training loss: 0.03992354513199599\n",
            "Epoch 1 - Training loss: 0.040297493966085826\n",
            "Epoch 1 - Training loss: 0.040439102846358634\n",
            "Epoch 1 - Training loss: 0.040729644162250736\n",
            "Epoch 1 - Training loss: 0.04093045255999321\n",
            "Epoch 1 - Training loss: 0.041237361578227105\n",
            "Epoch 1 - Training loss: 0.04170961194296381\n",
            "Epoch 1 - Training loss: 0.042231072868301925\n",
            "Epoch 1 - Training loss: 0.04251617033566747\n",
            "Epoch 1 - Training loss: 0.04268820826083358\n",
            "Epoch 1 - Training loss: 0.04279820192088959\n",
            "Epoch 1 - Training loss: 0.04293074147469962\n",
            "Epoch 1 - Training loss: 0.043128037964230155\n",
            "Epoch 1 - Training loss: 0.0433426064087638\n",
            "Epoch 1 - Training loss: 0.04352539348831055\n",
            "Epoch 1 - Training loss: 0.043837138298732134\n",
            "Epoch 1 - Training loss: 0.04414953955455121\n",
            "Epoch 1 - Training loss: 0.0443436216189663\n",
            "Epoch 1 - Training loss: 0.04476230987099442\n",
            "Epoch 1 - Training loss: 0.045026950903538705\n",
            "Epoch 1 - Training loss: 0.04518722664954057\n",
            "Epoch 1 - Training loss: 0.04543080244427805\n",
            "Epoch 1 - Training loss: 0.045518519829458266\n",
            "Epoch 1 - Training loss: 0.04570289623381486\n",
            "Epoch 1 - Training loss: 0.045855216618412846\n",
            "Epoch 1 - Training loss: 0.0460844167800092\n",
            "Epoch 1 - Training loss: 0.046233561088535574\n",
            "Epoch 1 - Training loss: 0.046428319741922144\n",
            "Epoch 1 - Training loss: 0.04659934492825445\n",
            "Epoch 1 - Training loss: 0.04689128787469254\n",
            "Epoch 1 - Training loss: 0.0472147988516893\n",
            "Epoch 1 - Training loss: 0.04741532706629747\n",
            "Epoch 1 - Training loss: 0.047451615321642555\n",
            "Epoch 1 - Training loss: 0.04768299257783874\n",
            "Epoch 1 - Training loss: 0.04782596296037057\n",
            "Epoch 1 - Training loss: 0.047984994094032465\n",
            "Epoch 1 - Training loss: 0.04822151830344439\n",
            "Epoch 1 - Training loss: 0.048466157338925515\n",
            "Epoch 1 - Training loss: 0.048559955525785875\n",
            "Epoch 1 - Training loss: 0.0488571215675139\n",
            "Epoch 1 - Training loss: 0.048988638525165475\n",
            "Epoch 1 - Training loss: 0.04915895929008023\n",
            "Epoch 1 - Training loss: 0.04939020232462298\n",
            "Epoch 1 - Training loss: 0.049541589337339535\n",
            "Epoch 1 - Training loss: 0.04970450187399825\n",
            "Epoch 1 - Training loss: 0.050046812524514664\n",
            "Epoch 1 - Training loss: 0.050262565516046624\n",
            "Epoch 1 - Training loss: 0.05054885612479024\n",
            "Epoch 1 - Training loss: 0.05064212006212933\n",
            "Epoch 1 - Training loss: 0.05079716685087061\n",
            "Epoch 1 - Training loss: 0.05095221422739756\n",
            "Epoch 1 - Training loss: 0.051295916659078365\n",
            "Epoch 1 - Training loss: 0.05135805143921106\n",
            "Epoch 1 - Training loss: 0.0515320403997832\n",
            "Epoch 1 - Training loss: 0.051690438869538335\n",
            "Epoch 1 - Training loss: 0.05182606922283864\n",
            "Epoch 1 - Training loss: 0.05205646932506358\n",
            "Epoch 1 - Training loss: 0.05243999725465835\n",
            "Epoch 1 - Training loss: 0.05249741870060023\n",
            "Epoch 1 - Training loss: 0.05268255612854637\n",
            "Epoch 1 - Training loss: 0.05280064896686372\n",
            "Epoch 1 - Training loss: 0.05292942558826287\n",
            "Epoch 1 - Training loss: 0.05311351037149363\n",
            "Epoch 1 - Training loss: 0.05326173454125934\n",
            "Epoch 1 - Training loss: 0.05347132369446983\n",
            "Epoch 1 - Training loss: 0.05359035073074578\n",
            "Epoch 1 - Training loss: 0.05415692916318679\n",
            "Epoch 1 - Training loss: 0.054374082784440475\n",
            "Epoch 1 - Training loss: 0.05461403681858897\n",
            "Epoch 1 - Training loss: 0.054809310451658294\n",
            "Epoch 1 - Training loss: 0.05509233862749422\n",
            "Epoch 1 - Training loss: 0.055244678822058094\n",
            "Epoch 1 - Training loss: 0.05544859263450225\n",
            "Epoch 1 - Training loss: 0.055653290219430225\n",
            "Epoch 1 - Training loss: 0.05574392259263916\n",
            "Epoch 1 - Training loss: 0.05581801609078577\n",
            "Epoch 1 - Training loss: 0.05605598874707847\n",
            "Epoch 1 - Training loss: 0.056237556568499825\n",
            "Epoch 1 - Training loss: 0.056472455089820475\n",
            "Epoch 1 - Training loss: 0.056646667027683145\n",
            "Epoch 1 - Training loss: 0.056761215446886226\n",
            "Epoch 1 - Training loss: 0.05685035223121455\n",
            "Epoch 1 - Training loss: 0.05702406542498801\n",
            "Epoch 1 - Training loss: 0.0571086797227801\n",
            "Epoch 1 - Training loss: 0.05740000120501147\n",
            "Epoch 1 - Training loss: 0.05770779317264745\n",
            "Epoch 1 - Training loss: 0.057850829180655705\n",
            "Epoch 1 - Training loss: 0.05809494888366285\n",
            "Epoch 1 - Training loss: 0.058418408548558697\n",
            "Epoch 1 - Training loss: 0.058578556633866165\n",
            "Epoch 1 - Training loss: 0.058917245634996306\n",
            "Epoch 1 - Training loss: 0.05898451339254882\n",
            "Epoch 1 - Training loss: 0.05926230462041618\n",
            "Epoch 1 - Training loss: 0.05938090106396914\n",
            "Epoch 1 - Training loss: 0.0595057847728925\n",
            "Epoch 1 - Training loss: 0.059594270505154055\n",
            "Epoch 1 - Training loss: 0.05986693455799937\n",
            "Epoch 1 - Training loss: 0.0599711335409107\n",
            "Epoch 1 - Training loss: 0.06017237964834867\n",
            "Epoch 1 - Training loss: 0.0603631347346344\n",
            "Epoch 1 - Training loss: 0.06042572199456346\n",
            "Epoch 1 - Training loss: 0.06060356219042974\n",
            "Epoch 1 - Training loss: 0.060837870463728905\n",
            "Epoch 1 - Training loss: 0.061023842547335096\n",
            "Epoch 1 - Training loss: 0.06117126898073565\n",
            "Epoch 1 - Training loss: 0.061359205857109925\n",
            "Epoch 1 - Training loss: 0.061473386846721045\n",
            "Epoch 1 - Training loss: 0.06166768559753132\n",
            "Epoch 1 - Training loss: 0.06183157276227149\n",
            "Epoch 1 - Training loss: 0.06206339838773584\n",
            "Epoch 1 - Training loss: 0.06223679825997175\n",
            "Epoch 1 - Training loss: 0.062337816377152515\n",
            "Epoch 1 - Training loss: 0.06242131670988572\n",
            "Epoch 1 - Training loss: 0.06260690091849008\n",
            "Epoch 1 - Training loss: 0.06275993859224609\n",
            "Epoch 1 - Training loss: 0.06299602930177885\n",
            "Epoch 1 - Training loss: 0.06305220838723534\n",
            "Epoch 1 - Training loss: 0.06327416432866537\n",
            "Epoch 1 - Training loss: 0.06342356203636254\n",
            "Epoch 1 - Training loss: 0.0636669879775248\n",
            "Epoch 1 - Training loss: 0.0638783743410413\n",
            "Epoch 1 - Training loss: 0.06402600508556565\n",
            "Epoch 1 - Training loss: 0.06418721390161305\n",
            "Epoch 1 - Training loss: 0.06444336525968779\n",
            "Epoch 1 - Training loss: 0.0645798235647142\n",
            "Epoch 1 - Training loss: 0.06474533042848618\n",
            "Epoch 1 - Training loss: 0.06505413510318377\n",
            "Epoch 1 - Training loss: 0.06514536168005293\n",
            "Epoch 1 - Training loss: 0.0654263066799084\n",
            "Epoch 1 - Training loss: 0.06562342872418193\n",
            "Epoch 1 - Training loss: 0.06577469671824213\n",
            "Epoch 1 - Training loss: 0.06587815036508701\n",
            "Epoch 1 - Training loss: 0.06599474084704543\n",
            "Epoch 1 - Training loss: 0.0662154445666939\n",
            "Epoch 1 - Training loss: 0.06651800899093212\n",
            "Epoch 1 - Training loss: 0.06675913999440955\n",
            "Epoch 1 - Training loss: 0.06724729911207772\n",
            "Epoch 1 - Training loss: 0.06728286828313555\n",
            "Epoch 1 - Training loss: 0.06746439875633732\n",
            "Epoch 1 - Training loss: 0.06753720491647974\n",
            "Epoch 1 - Training loss: 0.06765190111612206\n",
            "Epoch 1 - Training loss: 0.06774057489214168\n",
            "Epoch 1 - Training loss: 0.06793898367869065\n",
            "Epoch 1 - Training loss: 0.06798622918043182\n",
            "Epoch 1 - Training loss: 0.06816445690180574\n",
            "Epoch 1 - Training loss: 0.06827334344371168\n",
            "Epoch 1 - Training loss: 0.06845874556584526\n",
            "Epoch 1 - Training loss: 0.06867591936244512\n",
            "Epoch 1 - Training loss: 0.06884394685771547\n",
            "Epoch 1 - Training loss: 0.06907385269573121\n",
            "Epoch 1 - Training loss: 0.06926074299588005\n",
            "Epoch 1 - Training loss: 0.06962070774151953\n",
            "Epoch 1 - Training loss: 0.06972430868427763\n",
            "Epoch 1 - Training loss: 0.06995168127723213\n",
            "Epoch 1 - Training loss: 0.0701459751192377\n",
            "Epoch 1 - Training loss: 0.07053158018809519\n",
            "Epoch 1 - Training loss: 0.07069994813836078\n",
            "Epoch 1 - Training loss: 0.070852319247274\n",
            "Epoch 1 - Training loss: 0.07101957893559038\n",
            "Epoch 1 - Training loss: 0.07112042657109593\n",
            "Epoch 1 - Training loss: 0.07138561872419899\n",
            "Epoch 1 - Training loss: 0.07151527694468178\n",
            "Epoch 1 - Training loss: 0.07191966786615249\n",
            "Epoch 1 - Training loss: 0.07205880045303023\n",
            "Epoch 1 - Training loss: 0.07238639142515181\n",
            "Epoch 1 - Training loss: 0.07254543431452724\n",
            "Epoch 1 - Training loss: 0.07280502318621063\n",
            "Epoch 1 - Training loss: 0.0729514464918675\n",
            "Epoch 1 - Training loss: 0.07316551622408413\n",
            "Epoch 1 - Training loss: 0.07345940540832624\n",
            "Epoch 1 - Training loss: 0.07362784884933597\n",
            "Epoch 1 - Training loss: 0.07373452410975626\n",
            "Epoch 1 - Training loss: 0.07398231876001302\n",
            "Epoch 1 - Training loss: 0.07414998600620833\n",
            "Epoch 1 - Training loss: 0.07427992322631101\n",
            "Epoch 1 - Training loss: 0.0744378506414481\n",
            "Epoch 1 - Training loss: 0.07473144148474437\n",
            "Epoch 1 - Training loss: 0.07486080717065059\n",
            "Epoch 1 - Training loss: 0.07497273225472299\n",
            "Epoch 1 - Training loss: 0.07536019377315095\n",
            "Epoch 1 - Training loss: 0.075628075506419\n",
            "Epoch 1 - Training loss: 0.0758550645612768\n",
            "Epoch 1 - Training loss: 0.07595037569258131\n",
            "Epoch 1 - Training loss: 0.07624047324617407\n",
            "Epoch 1 - Training loss: 0.07644879730986252\n",
            "Epoch 1 - Training loss: 0.07657867498092179\n",
            "Epoch 1 - Training loss: 0.07667830619397067\n",
            "Epoch 1 - Training loss: 0.07676699740498431\n",
            "Epoch 1 - Training loss: 0.07703109130438075\n",
            "Epoch 1 - Training loss: 0.07726779034031607\n",
            "Epoch 1 - Training loss: 0.07765159173322513\n",
            "Epoch 1 - Training loss: 0.07784051799586714\n",
            "Epoch 1 - Training loss: 0.0780070111481175\n",
            "Epoch 1 - Training loss: 0.07828696472828449\n",
            "Epoch 1 - Training loss: 0.07844759667240607\n",
            "Epoch 1 - Training loss: 0.07861969885684407\n",
            "Epoch 1 - Training loss: 0.0788367946685822\n",
            "Epoch 1 - Training loss: 0.07904881669847823\n",
            "Epoch 1 - Training loss: 0.07912305025641979\n",
            "Epoch 1 - Training loss: 0.07930106281646407\n",
            "Epoch 1 - Training loss: 0.07941414749841573\n",
            "Epoch 1 - Training loss: 0.07955499313104508\n",
            "Epoch 1 - Training loss: 0.07964616046070672\n",
            "Epoch 1 - Training loss: 0.07983955484765298\n",
            "Epoch 1 - Training loss: 0.08002631612090287\n",
            "Epoch 1 - Training loss: 0.08034609948823065\n",
            "Epoch 1 - Training loss: 0.08057737434263042\n",
            "Epoch 1 - Training loss: 0.08075418808995914\n",
            "Epoch 1 - Training loss: 0.08088219017664125\n",
            "Epoch 1 - Training loss: 0.08116369846580761\n",
            "Epoch 1 - Training loss: 0.08139424398938604\n",
            "Epoch 1 - Training loss: 0.0815192612924619\n",
            "Epoch 1 - Training loss: 0.08164546767801745\n",
            "Epoch 1 - Training loss: 0.08170024922757006\n",
            "Epoch 1 - Training loss: 0.08196324395186612\n",
            "Epoch 1 - Training loss: 0.08213666712900977\n",
            "Epoch 1 - Training loss: 0.082307898405709\n",
            "Epoch 1 - Training loss: 0.08244574186739637\n",
            "Epoch 1 - Training loss: 0.08310381006171454\n",
            "Epoch 1 - Training loss: 0.08342392910232167\n",
            "Epoch 1 - Training loss: 0.08358216626462397\n",
            "Epoch 1 - Training loss: 0.08380205399477914\n",
            "Epoch 1 - Training loss: 0.0839505499757048\n",
            "Epoch 1 - Training loss: 0.08419239232693908\n",
            "Epoch 1 - Training loss: 0.08445993524148011\n",
            "Epoch 1 - Training loss: 0.08470265679299704\n",
            "Epoch 1 - Training loss: 0.08494207799148712\n",
            "Epoch 1 - Training loss: 0.08517105028287435\n",
            "Epoch 1 - Training loss: 0.08531843115494195\n",
            "Epoch 1 - Training loss: 0.08548419356250814\n",
            "Epoch 1 - Training loss: 0.08565758140896683\n",
            "Epoch 1 - Training loss: 0.08587751050652472\n",
            "Epoch 1 - Training loss: 0.0859890914023685\n",
            "Epoch 1 - Training loss: 0.08623454337721186\n",
            "Epoch 1 - Training loss: 0.08640049888031569\n",
            "Epoch 1 - Training loss: 0.08650632866664228\n",
            "Epoch 1 - Training loss: 0.08669150642939467\n",
            "Epoch 1 - Training loss: 0.08695118925146965\n",
            "Epoch 1 - Training loss: 0.08712075925521505\n",
            "Epoch 1 - Training loss: 0.0875537073148339\n",
            "Epoch 1 - Training loss: 0.08769152146666798\n",
            "Epoch 1 - Training loss: 0.08784416429142454\n",
            "Epoch 1 - Training loss: 0.08820608220120738\n",
            "Epoch 1 - Training loss: 0.08841647241097778\n",
            "Epoch 1 - Training loss: 0.0886612687188425\n",
            "Epoch 1 - Training loss: 0.08894847694045699\n",
            "Epoch 1 - Training loss: 0.08910904627746102\n",
            "Epoch 1 - Training loss: 0.08935927539301325\n",
            "Epoch 1 - Training loss: 0.08949207031587039\n",
            "Epoch 1 - Training loss: 0.08968119210462326\n",
            "Epoch 1 - Training loss: 0.08979007355503436\n",
            "Epoch 1 - Training loss: 0.08996056487311178\n",
            "Epoch 1 - Training loss: 0.09019650363210421\n",
            "Epoch 1 - Training loss: 0.09030882788619507\n",
            "Epoch 1 - Training loss: 0.09043230158465504\n",
            "Epoch 1 - Training loss: 0.09057731601570461\n",
            "Epoch 1 - Training loss: 0.09083405338021229\n",
            "Epoch 1 - Training loss: 0.09126307140948421\n",
            "Epoch 1 - Training loss: 0.09144704773831469\n",
            "Epoch 1 - Training loss: 0.09168693657591144\n",
            "Epoch 1 - Training loss: 0.09191081854802713\n",
            "Epoch 1 - Training loss: 0.09205141016192782\n",
            "Epoch 1 - Training loss: 0.09211130945015945\n",
            "Epoch 1 - Training loss: 0.09221244874650608\n",
            "Epoch 1 - Training loss: 0.09230665172706408\n",
            "Epoch 1 - Training loss: 0.09238141692126356\n",
            "Epoch 1 - Training loss: 0.09244339030259835\n",
            "Epoch 1 - Training loss: 0.09249480307372267\n",
            "Epoch 1 - Training loss: 0.09274648900813004\n",
            "Epoch 1 - Training loss: 0.09295589300885257\n",
            "Epoch 1 - Training loss: 0.09304453089023068\n",
            "Epoch 1 - Training loss: 0.093205828430143\n",
            "Epoch 1 - Training loss: 0.09347442297268906\n",
            "Epoch 1 - Training loss: 0.09356950933951685\n",
            "Epoch 1 - Training loss: 0.09365601876158831\n",
            "Epoch 1 - Training loss: 0.09382144991221077\n",
            "Epoch 1 - Training loss: 0.09397524295410495\n",
            "Epoch 1 - Training loss: 0.0942527074366808\n",
            "Epoch 1 - Training loss: 0.09462011736164343\n",
            "Epoch 1 - Training loss: 0.0949110059158952\n",
            "Epoch 1 - Training loss: 0.09507652830038625\n",
            "Epoch 1 - Training loss: 0.09524625598955383\n",
            "Epoch 1 - Training loss: 0.0954025913156998\n",
            "Epoch 1 - Training loss: 0.0957182247890656\n",
            "Epoch 1 - Training loss: 0.09579798902895277\n",
            "Epoch 1 - Training loss: 0.09616706705789195\n",
            "Epoch 1 - Training loss: 0.09628187439668534\n",
            "Epoch 1 - Training loss: 0.09653478335422366\n",
            "Epoch 1 - Training loss: 0.09679021513554206\n",
            "Epoch 1 - Training loss: 0.09702078399580044\n",
            "Epoch 1 - Training loss: 0.09732255017531834\n",
            "Epoch 1 - Training loss: 0.09754123095907509\n",
            "Epoch 1 - Training loss: 0.09772532559168745\n",
            "Epoch 1 - Training loss: 0.09791194403301805\n",
            "Epoch 1 - Training loss: 0.09806460547628307\n",
            "Epoch 1 - Training loss: 0.09824079024528008\n",
            "Epoch 1 - Training loss: 0.09833206087430275\n",
            "Epoch 1 - Training loss: 0.09852597765974018\n",
            "Epoch 1 - Training loss: 0.09880817896410474\n",
            "Epoch 1 - Training loss: 0.09903825923149139\n",
            "Epoch 1 - Training loss: 0.09925572317180985\n",
            "Epoch 1 - Training loss: 0.09952747081912784\n",
            "Epoch 1 - Training loss: 0.09964844354513738\n",
            "Epoch 1 - Training loss: 0.09982694517463636\n",
            "Epoch 1 - Training loss: 0.09996511979795086\n",
            "Epoch 1 - Training loss: 0.10029239145947545\n",
            "Epoch 1 - Training loss: 0.10035768612377298\n",
            "Epoch 1 - Training loss: 0.10049651968660259\n",
            "Epoch 1 - Training loss: 0.10079628262899197\n",
            "Epoch 1 - Training loss: 0.10104070794877848\n",
            "Epoch 1 - Training loss: 0.1010846412822064\n",
            "Epoch 1 - Training loss: 0.10129805596700228\n",
            "Epoch 1 - Training loss: 0.10148276153133749\n",
            "Epoch 1 - Training loss: 0.1015455014328522\n",
            "Epoch 1 - Training loss: 0.10175460223148246\n",
            "Epoch 1 - Training loss: 0.10188733766089751\n",
            "Epoch 1 - Training loss: 0.10195470457153916\n",
            "Epoch 1 - Training loss: 0.10201363865214624\n",
            "Epoch 1 - Training loss: 0.10209874239271638\n",
            "Epoch 1 - Training loss: 0.10234078938868255\n",
            "Epoch 1 - Training loss: 0.10250110057657207\n",
            "Epoch 1 - Training loss: 0.10255687945147059\n",
            "Epoch 1 - Training loss: 0.10264951271860838\n",
            "Epoch 1 - Training loss: 0.10276954184208852\n",
            "Epoch 1 - Training loss: 0.10293902636273329\n",
            "Epoch 1 - Training loss: 0.10305391544345091\n",
            "Epoch 1 - Training loss: 0.10319399199823835\n",
            "Epoch 1 - Training loss: 0.10351498216898965\n",
            "Epoch 1 - Training loss: 0.10355362956569011\n",
            "Epoch 1 - Training loss: 0.10366468195483755\n",
            "Epoch 1 - Training loss: 0.10379923543712097\n",
            "Epoch 1 - Training loss: 0.10415749376151226\n",
            "Epoch 1 - Training loss: 0.10438716821849092\n",
            "Epoch 1 - Training loss: 0.10449061586039025\n",
            "Epoch 1 - Training loss: 0.10462456282728644\n",
            "Epoch 1 - Training loss: 0.10477574753052772\n",
            "Epoch 1 - Training loss: 0.10488200195229003\n",
            "Epoch 1 - Training loss: 0.10520141259995477\n",
            "Epoch 1 - Training loss: 0.10524194390137694\n",
            "Epoch 1 - Training loss: 0.10548345607592226\n",
            "Epoch 1 - Training loss: 0.10563606812930437\n",
            "Epoch 1 - Training loss: 0.10579318495622195\n",
            "Epoch 1 - Training loss: 0.10594668733015625\n",
            "Epoch 1 - Training loss: 0.10607822037248342\n",
            "Epoch 1 - Training loss: 0.10630775266475896\n",
            "Epoch 1 - Training loss: 0.10655538028856712\n",
            "Epoch 1 - Training loss: 0.10674754804623787\n",
            "Epoch 1 - Training loss: 0.10701144912016036\n",
            "Epoch 1 - Training loss: 0.10708897446852121\n",
            "Epoch 1 - Training loss: 0.10724232296969717\n",
            "Epoch 1 - Training loss: 0.10736803758516113\n",
            "Epoch 1 - Training loss: 0.10745974172994907\n",
            "Epoch 1 - Training loss: 0.10764036021395915\n",
            "Epoch 1 - Training loss: 0.10787843172944812\n",
            "Epoch 1 - Training loss: 0.1081160459000228\n",
            "Epoch 1 - Training loss: 0.10827037732182408\n",
            "Epoch 1 - Training loss: 0.10856421134952925\n",
            "Epoch 1 - Training loss: 0.108701767860604\n",
            "Epoch 1 - Training loss: 0.10920337284567641\n",
            "Epoch 1 - Training loss: 0.10930836144715611\n",
            "Epoch 1 - Training loss: 0.1093701811066505\n",
            "Epoch 1 - Training loss: 0.10950674295727252\n",
            "Epoch 1 - Training loss: 0.10968758823123695\n",
            "Epoch 1 - Training loss: 0.1097921344445649\n",
            "Epoch 1 - Training loss: 0.10993914779172397\n",
            "Epoch 1 - Training loss: 0.11008344089854628\n",
            "Epoch 1 - Training loss: 0.11040992661119142\n",
            "Epoch 1 - Training loss: 0.11051209790032429\n",
            "Epoch 1 - Training loss: 0.11064719058859196\n",
            "Epoch 1 - Training loss: 0.11083404304805214\n",
            "Epoch 1 - Training loss: 0.11108675665422671\n",
            "Epoch 1 - Training loss: 0.11132227003113675\n",
            "Epoch 1 - Training loss: 0.1115638920104008\n",
            "Epoch 1 - Training loss: 0.11184536064786316\n",
            "Epoch 1 - Training loss: 0.1121435883258388\n",
            "Epoch 1 - Training loss: 0.11224238613822948\n",
            "Epoch 1 - Training loss: 0.11250950381366302\n",
            "Epoch 1 - Training loss: 0.11267585084557152\n",
            "Epoch 1 - Training loss: 0.11286236205573148\n",
            "Epoch 1 - Training loss: 0.11320630582903367\n",
            "Epoch 1 - Training loss: 0.11342401246129195\n",
            "Epoch 1 - Training loss: 0.11355941699368995\n",
            "Epoch 1 - Training loss: 0.11366033143024327\n",
            "Epoch 1 - Training loss: 0.1137575745495207\n",
            "Epoch 1 - Training loss: 0.11394191390749361\n",
            "Epoch 1 - Training loss: 0.11413139068702263\n",
            "Epoch 1 - Training loss: 0.11441078148623392\n",
            "Epoch 1 - Training loss: 0.11447028540519637\n",
            "Epoch 1 - Training loss: 0.1146012711078564\n",
            "Epoch 1 - Training loss: 0.1146463362821765\n",
            "Epoch 1 - Training loss: 0.11476992666403622\n",
            "Epoch 1 - Training loss: 0.11491770956562018\n",
            "Epoch 1 - Training loss: 0.1151299676947248\n",
            "Epoch 1 - Training loss: 0.11520761824143466\n",
            "Epoch 1 - Training loss: 0.11534321922133726\n",
            "Epoch 1 - Training loss: 0.11551412772426982\n",
            "Epoch 1 - Training loss: 0.11569605268109073\n",
            "Epoch 1 - Training loss: 0.11580908723445589\n",
            "Epoch 1 - Training loss: 0.11593914356058849\n",
            "Epoch 1 - Training loss: 0.11612026852521815\n",
            "Epoch 1 - Training loss: 0.11645711921870328\n",
            "Epoch 1 - Training loss: 0.11666912602971612\n",
            "Epoch 1 - Training loss: 0.11675736072983568\n",
            "Epoch 1 - Training loss: 0.11687625297255862\n",
            "Epoch 1 - Training loss: 0.11702813666417146\n",
            "Epoch 1 - Training loss: 0.1171499847142554\n",
            "Epoch 1 - Training loss: 0.11721969905025414\n",
            "Epoch 1 - Training loss: 0.11730229340668426\n",
            "Epoch 1 - Training loss: 0.11752272291637178\n",
            "Epoch 1 - Training loss: 0.11771613347536719\n",
            "Epoch 1 - Training loss: 0.11790204274533654\n",
            "Epoch 1 - Training loss: 0.11807145423758258\n",
            "Epoch 1 - Training loss: 0.11819034046741692\n",
            "Epoch 1 - Training loss: 0.11824871805399212\n",
            "Epoch 1 - Training loss: 0.11855238265415499\n",
            "Epoch 1 - Training loss: 0.11867268120588016\n",
            "Epoch 1 - Training loss: 0.11870655265730073\n",
            "Epoch 1 - Training loss: 0.11877835525283173\n",
            "Epoch 1 - Training loss: 0.11890209043648706\n",
            "Epoch 1 - Training loss: 0.11907500374927195\n",
            "Epoch 1 - Training loss: 0.11931317634801113\n",
            "Epoch 1 - Training loss: 0.11946934846037233\n",
            "Epoch 1 - Training loss: 0.11961945368727642\n",
            "Epoch 1 - Training loss: 0.12000256657663948\n",
            "Epoch 1 - Training loss: 0.12009376073792291\n",
            "Epoch 1 - Training loss: 0.1202323588369879\n",
            "Epoch 1 - Training loss: 0.12033005921380606\n",
            "Epoch 1 - Training loss: 0.12038047139499106\n",
            "Epoch 1 - Training loss: 0.12047465504216615\n",
            "Epoch 1 - Training loss: 0.12055639083435668\n",
            "Epoch 1 - Training loss: 0.12070756522751948\n",
            "Epoch 1 - Training loss: 0.12080034669607814\n",
            "Epoch 1 - Training loss: 0.12111529369931867\n",
            "Epoch 1 - Training loss: 0.12121630725718892\n",
            "Epoch 1 - Training loss: 0.12126759678395445\n",
            "Epoch 1 - Training loss: 0.12140800996518719\n",
            "Epoch 1 - Training loss: 0.121693612261018\n",
            "Epoch 1 - Training loss: 0.12182031306964375\n",
            "Epoch 1 - Training loss: 0.12191797819520746\n",
            "Epoch 1 - Training loss: 0.12200479102589047\n",
            "Epoch 1 - Training loss: 0.12207855447443691\n",
            "Epoch 1 - Training loss: 0.12220295264061962\n",
            "Epoch 1 - Training loss: 0.1223401122692742\n",
            "Epoch 1 - Training loss: 0.12262454093185696\n",
            "Epoch 1 - Training loss: 0.12272343281378496\n",
            "Epoch 1 - Training loss: 0.12289368024846511\n",
            "Epoch 1 - Training loss: 0.12302549562649305\n",
            "Epoch 1 - Training loss: 0.1232633787487298\n",
            "Epoch 1 - Training loss: 0.12345049331294321\n",
            "Epoch 1 - Training loss: 0.1235134174098084\n",
            "Epoch 1 - Training loss: 0.12357545602343865\n",
            "Epoch 1 - Training loss: 0.12377108880944217\n",
            "Epoch 1 - Training loss: 0.12413242261912395\n",
            "Epoch 1 - Training loss: 0.12438758654492114\n",
            "Epoch 1 - Training loss: 0.12443196256039367\n",
            "Epoch 1 - Training loss: 0.12472083100647942\n",
            "Epoch 1 - Training loss: 0.12490383874394619\n",
            "Epoch 1 - Training loss: 0.1250749526223712\n",
            "Epoch 1 - Training loss: 0.1253094636022981\n",
            "Epoch 1 - Training loss: 0.12554519801839456\n",
            "Epoch 1 - Training loss: 0.12567169651754503\n",
            "Epoch 1 - Training loss: 0.12579539054031694\n",
            "Epoch 1 - Training loss: 0.12610635022793623\n",
            "Epoch 1 - Training loss: 0.12649568169911915\n",
            "Epoch 1 - Training loss: 0.12665487186852167\n",
            "Epoch 1 - Training loss: 0.12688610248013474\n",
            "Epoch 1 - Training loss: 0.12699832305359815\n",
            "Epoch 1 - Training loss: 0.12705457357884342\n",
            "Epoch 1 - Training loss: 0.12712700314311456\n",
            "Epoch 1 - Training loss: 0.12738145981976853\n",
            "Epoch 1 - Training loss: 0.12754287328054784\n",
            "Epoch 1 - Training loss: 0.12771616933712446\n",
            "Epoch 1 - Training loss: 0.12810234929214537\n",
            "Epoch 1 - Training loss: 0.12826036177734448\n",
            "Epoch 1 - Training loss: 0.12839012480636777\n",
            "Epoch 1 - Training loss: 0.1285228239995902\n",
            "Epoch 1 - Training loss: 0.12883648248131213\n",
            "Epoch 1 - Training loss: 0.12901591224027975\n",
            "Epoch 1 - Training loss: 0.12918615274862058\n",
            "Epoch 1 - Training loss: 0.12935608938765297\n",
            "Epoch 1 - Training loss: 0.12957057563750857\n",
            "Epoch 1 - Training loss: 0.12987752395795224\n",
            "Epoch 1 - Training loss: 0.12992890894826034\n",
            "Epoch 1 - Training loss: 0.13005701607383136\n",
            "Epoch 1 - Training loss: 0.13020983532960737\n",
            "Epoch 1 - Training loss: 0.13044409940777812\n",
            "Epoch 1 - Training loss: 0.13063085013265802\n",
            "Epoch 1 - Training loss: 0.13091880036220113\n",
            "Epoch 1 - Training loss: 0.1310460416794713\n",
            "Epoch 1 - Training loss: 0.1311642945464105\n",
            "Epoch 1 - Training loss: 0.13126352779678443\n",
            "Epoch 1 - Training loss: 0.13182824628471312\n",
            "Epoch 1 - Training loss: 0.13202803828981893\n",
            "Epoch 1 - Training loss: 0.1322010587225718\n",
            "Epoch 1 - Training loss: 0.13232639781447617\n",
            "Epoch 1 - Training loss: 0.13246806075514506\n",
            "Epoch 1 - Training loss: 0.13272416841056048\n",
            "Epoch 1 - Training loss: 0.13287520109971704\n",
            "Epoch 1 - Training loss: 0.1331083735169124\n",
            "Epoch 1 - Training loss: 0.13322146792909992\n",
            "Epoch 1 - Training loss: 0.13333590061012615\n",
            "Epoch 1 - Training loss: 0.1334803306233527\n",
            "Epoch 1 - Training loss: 0.13387652685933277\n",
            "Epoch 1 - Training loss: 0.1343450367784322\n",
            "Epoch 1 - Training loss: 0.1344666710889924\n",
            "Epoch 1 - Training loss: 0.13451821679499612\n",
            "Epoch 1 - Training loss: 0.1345769457622314\n",
            "Epoch 1 - Training loss: 0.13506996745605077\n",
            "Epoch 1 - Training loss: 0.13556577423726446\n",
            "Epoch 1 - Training loss: 0.13562528128578846\n",
            "Epoch 1 - Training loss: 0.1357904397832877\n",
            "Epoch 1 - Training loss: 0.1360270312544443\n",
            "Epoch 1 - Training loss: 0.13622684680274935\n",
            "Epoch 1 - Training loss: 0.13648765725192866\n",
            "Epoch 1 - Training loss: 0.13659221747679623\n",
            "Epoch 1 - Training loss: 0.13671141545544427\n",
            "Epoch 1 - Training loss: 0.13685993381988393\n",
            "Epoch 1 - Training loss: 0.13704625913090923\n",
            "Epoch 1 - Training loss: 0.13711468712972807\n",
            "Epoch 1 - Training loss: 0.13726732239270134\n",
            "Epoch 1 - Training loss: 0.13738200180867968\n",
            "Epoch 1 - Training loss: 0.13767244628291014\n",
            "Epoch 1 - Training loss: 0.13788099205300117\n",
            "Epoch 1 - Training loss: 0.13793888682924482\n",
            "Epoch 1 - Training loss: 0.138177508893393\n",
            "Epoch 1 - Training loss: 0.13832877269352295\n",
            "Epoch 1 - Training loss: 0.13868572943802202\n",
            "Epoch 1 - Training loss: 0.1387667793717021\n",
            "Epoch 1 - Training loss: 0.1389449482986223\n",
            "Epoch 1 - Training loss: 0.1391774439838713\n",
            "Epoch 1 - Training loss: 0.13927094531513606\n",
            "Epoch 1 - Training loss: 0.13933601264538034\n",
            "Epoch 1 - Training loss: 0.13948827356830842\n",
            "Epoch 1 - Training loss: 0.13965502650594153\n",
            "Epoch 1 - Training loss: 0.13978436682174708\n",
            "Epoch 1 - Training loss: 0.13986493766244287\n",
            "Epoch 1 - Training loss: 0.14005751731489768\n",
            "Epoch 1 - Training loss: 0.14029179125833613\n",
            "Epoch 1 - Training loss: 0.14038320857960024\n",
            "Epoch 1 - Training loss: 0.1405816278350887\n",
            "Epoch 1 - Training loss: 0.14101461348121863\n",
            "Epoch 1 - Training loss: 0.1411204877645095\n",
            "Epoch 1 - Training loss: 0.14125148080654745\n",
            "Epoch 1 - Training loss: 0.14142174684384992\n",
            "Epoch 1 - Training loss: 0.14184315584456997\n",
            "Epoch 1 - Training loss: 0.14196186901123792\n",
            "Epoch 1 - Training loss: 0.1424169854552888\n",
            "Epoch 1 - Training loss: 0.14263416489145395\n",
            "Epoch 1 - Training loss: 0.14291921377912767\n",
            "Epoch 1 - Training loss: 0.142986000704168\n",
            "Epoch 1 - Training loss: 0.1433549059876628\n",
            "Epoch 1 - Training loss: 0.14351072720786148\n",
            "Epoch 1 - Training loss: 0.1437211757831609\n",
            "Epoch 1 - Training loss: 0.14389863561815036\n",
            "Epoch 1 - Training loss: 0.14403244396294357\n",
            "Epoch 1 - Training loss: 0.14418428593765953\n",
            "Epoch 1 - Training loss: 0.1444155706890992\n",
            "Epoch 1 - Training loss: 0.14466629708721948\n",
            "Epoch 1 - Training loss: 0.14476814374391203\n",
            "Epoch 1 - Training loss: 0.14484942228650488\n",
            "Epoch 1 - Training loss: 0.1450640602644954\n",
            "Epoch 1 - Training loss: 0.14516212035026124\n",
            "Epoch 1 - Training loss: 0.14543157893775113\n",
            "Epoch 1 - Training loss: 0.14561402292521014\n",
            "Epoch 1 - Training loss: 0.1459519622613118\n",
            "Epoch 1 - Training loss: 0.14615650495677107\n",
            "Epoch 1 - Training loss: 0.14653909099953516\n",
            "Epoch 1 - Training loss: 0.14696581876163545\n",
            "Epoch 1 - Training loss: 0.1471713842518294\n",
            "Epoch 1 - Training loss: 0.14723113828154008\n",
            "Epoch 1 - Training loss: 0.14736505048965087\n",
            "Epoch 1 - Training loss: 0.14742743955460438\n",
            "Epoch 1 - Training loss: 0.14762582454377654\n",
            "Epoch 1 - Training loss: 0.14792094985718157\n",
            "Epoch 1 - Training loss: 0.14819857577429907\n",
            "Epoch 1 - Training loss: 0.14843522931244582\n",
            "Epoch 1 - Training loss: 0.14859202832205973\n",
            "Epoch 1 - Training loss: 0.1487009868994832\n",
            "Epoch 1 - Training loss: 0.14880266603725806\n",
            "Epoch 1 - Training loss: 0.1489913684012158\n",
            "Epoch 1 - Training loss: 0.14905649278241434\n",
            "Epoch 1 - Training loss: 0.14931051044671267\n",
            "Epoch 1 - Training loss: 0.14946637101677943\n",
            "Epoch 1 - Training loss: 0.14969378688346857\n",
            "Epoch 1 - Training loss: 0.14990884198276982\n",
            "Epoch 1 - Training loss: 0.15010586627987402\n",
            "Epoch 1 - Training loss: 0.15019824733929849\n",
            "Epoch 1 - Training loss: 0.15022145905919165\n",
            "Epoch 1 - Training loss: 0.1503234013581454\n",
            "Epoch 1 - Training loss: 0.15063851036782713\n",
            "Epoch 1 - Training loss: 0.1507178446822075\n",
            "Epoch 1 - Training loss: 0.1509748053099555\n",
            "Epoch 1 - Training loss: 0.1511495234901463\n",
            "Epoch 1 - Training loss: 0.15124956518411636\n",
            "Epoch 1 - Training loss: 0.151594953194483\n",
            "Epoch 1 - Training loss: 0.1517946971600244\n",
            "Epoch 1 - Training loss: 0.1521941907147863\n",
            "Epoch 1 - Training loss: 0.1525399730816833\n",
            "Epoch 1 - Training loss: 0.1525989593362122\n",
            "Epoch 1 - Training loss: 0.15271054854048594\n",
            "Epoch 1 - Training loss: 0.15283927769422023\n",
            "Epoch 1 - Training loss: 0.15291216568366042\n",
            "Epoch 1 - Training loss: 0.1531028779251362\n",
            "Epoch 1 - Training loss: 0.1532060901827014\n",
            "Epoch 1 - Training loss: 0.1533949481192301\n",
            "Epoch 1 - Training loss: 0.15368607855523064\n",
            "Epoch 1 - Training loss: 0.1537720094293928\n",
            "Epoch 1 - Training loss: 0.15391590088796514\n",
            "Epoch 1 - Training loss: 0.15402992583636535\n",
            "Epoch 1 - Training loss: 0.15429478836879293\n",
            "Epoch 1 - Training loss: 0.15446781015186423\n",
            "Epoch 1 - Training loss: 0.1545860776975592\n",
            "Epoch 1 - Training loss: 0.1546860294364917\n",
            "Epoch 1 - Training loss: 0.15478518648124706\n",
            "Epoch 1 - Training loss: 0.1549098198093585\n",
            "Epoch 1 - Training loss: 0.15504245324207266\n",
            "Epoch 1 - Training loss: 0.1553426749750114\n",
            "Epoch 1 - Training loss: 0.15542409336293683\n",
            "Epoch 1 - Training loss: 0.1555278991092878\n",
            "Epoch 1 - Training loss: 0.15558939998242646\n",
            "Epoch 1 - Training loss: 0.15589455120376686\n",
            "Epoch 1 - Training loss: 0.15603986716886828\n",
            "Epoch 1 - Training loss: 0.15614658826862826\n",
            "Epoch 1 - Training loss: 0.15629017825669317\n",
            "Epoch 1 - Training loss: 0.15649002603789383\n",
            "Epoch 1 - Training loss: 0.1566187187806884\n",
            "Epoch 1 - Training loss: 0.15686679088167035\n",
            "Epoch 1 - Training loss: 0.15694182033318954\n",
            "Epoch 1 - Training loss: 0.1571920978536865\n",
            "Epoch 1 - Training loss: 0.15731478862162593\n",
            "Epoch 1 - Training loss: 0.15752046614059254\n",
            "Epoch 1 - Training loss: 0.1575806387769642\n",
            "Epoch 1 - Training loss: 0.15776487412864465\n",
            "Epoch 1 - Training loss: 0.15788568255267163\n",
            "Epoch 1 - Training loss: 0.15796934179405667\n",
            "Epoch 1 - Training loss: 0.15807327845775243\n",
            "Epoch 1 - Training loss: 0.15828968127017845\n",
            "Epoch 1 - Training loss: 0.1585152753984242\n",
            "Epoch 1 - Training loss: 0.15871430923943836\n",
            "Epoch 1 - Training loss: 0.1589069762018952\n",
            "Epoch 1 - Training loss: 0.1591256216390809\n",
            "Epoch 1 - Training loss: 0.15919627389832855\n",
            "Epoch 1 - Training loss: 0.1594725854631299\n",
            "Epoch 1 - Training loss: 0.15955754890561358\n",
            "Epoch 1 - Training loss: 0.15972993008172842\n",
            "Epoch 1 - Training loss: 0.15991665329188426\n",
            "Epoch 1 - Training loss: 0.16022637494400874\n",
            "Epoch 1 - Training loss: 0.1602969258817147\n",
            "Epoch 1 - Training loss: 0.16051090951127284\n",
            "Epoch 1 - Training loss: 0.16059931824361084\n",
            "Epoch 1 - Training loss: 0.16075944483502588\n",
            "Epoch 1 - Training loss: 0.1608450574709027\n",
            "Epoch 1 - Training loss: 0.16101323561405323\n",
            "Epoch 1 - Training loss: 0.16117227419829572\n",
            "Epoch 1 - Training loss: 0.1613562678112023\n",
            "Epoch 1 - Training loss: 0.16155256112533084\n",
            "Epoch 1 - Training loss: 0.16192971847490714\n",
            "Epoch 1 - Training loss: 0.16204152840064534\n",
            "Epoch 1 - Training loss: 0.1623245756398005\n",
            "Epoch 1 - Training loss: 0.16237535564375838\n",
            "Epoch 1 - Training loss: 0.16253261585860873\n",
            "Epoch 1 - Training loss: 0.162876651262932\n",
            "Epoch 1 - Training loss: 0.16312809415590535\n",
            "Epoch 1 - Training loss: 0.16332702909006494\n",
            "Epoch 1 - Training loss: 0.16352027465603244\n",
            "Epoch 1 - Training loss: 0.1636610218742763\n",
            "Epoch 1 - Training loss: 0.16398448957753842\n",
            "Epoch 1 - Training loss: 0.16409497574638965\n",
            "Epoch 1 - Training loss: 0.1642960667896118\n",
            "Epoch 1 - Training loss: 0.1645308616700203\n",
            "Epoch 1 - Training loss: 0.1647052567587224\n",
            "Epoch 1 - Training loss: 0.16481940719142144\n",
            "Epoch 1 - Training loss: 0.1649694955949463\n",
            "Epoch 1 - Training loss: 0.1651288492481973\n",
            "Epoch 1 - Training loss: 0.16523440840687834\n",
            "Epoch 1 - Training loss: 0.16530953593917494\n",
            "Epoch 1 - Training loss: 0.16544933694956906\n",
            "Epoch 1 - Training loss: 0.16562618663125456\n",
            "Epoch 1 - Training loss: 0.1658421807578886\n",
            "Epoch 1 - Training loss: 0.166005037280161\n",
            "Epoch 1 - Training loss: 0.16628156432401398\n",
            "Epoch 1 - Training loss: 0.1663871991561293\n",
            "Epoch 1 - Training loss: 0.16660404032958087\n",
            "Epoch 1 - Training loss: 0.16671581049241238\n",
            "Epoch 1 - Training loss: 0.16699442849643448\n",
            "Epoch 1 - Training loss: 0.1671977214483437\n",
            "Epoch 1 - Training loss: 0.16734139671298995\n",
            "Epoch 1 - Training loss: 0.16751451584607807\n",
            "Epoch 1 - Training loss: 0.16758520504050672\n",
            "Epoch 1 - Training loss: 0.16768505660963973\n",
            "Epoch 1 - Training loss: 0.16783663069706228\n",
            "Epoch 1 - Training loss: 0.16798347717663373\n",
            "Epoch 1 - Training loss: 0.16825528045707167\n",
            "Epoch 1 - Training loss: 0.1683380463357165\n",
            "Epoch 1 - Training loss: 0.16843374077476927\n",
            "Epoch 1 - Training loss: 0.1684932277750359\n",
            "Epoch 1 - Training loss: 0.16862700792200275\n",
            "Epoch 1 - Training loss: 0.16883490131353773\n",
            "Epoch 1 - Training loss: 0.16895368335438943\n",
            "Epoch 1 - Training loss: 0.1690724461015735\n",
            "Epoch 1 - Training loss: 0.16927444706879444\n",
            "Epoch 1 - Training loss: 0.16933676957496321\n",
            "Epoch 1 - Training loss: 0.16944486672070616\n",
            "Epoch 1 - Training loss: 0.1696516428587597\n",
            "Epoch 1 - Training loss: 0.16978534672067744\n",
            "Epoch 1 - Training loss: 0.1699152792146656\n",
            "Epoch 1 - Training loss: 0.17005774970930904\n",
            "Epoch 1 - Training loss: 0.17019023128679947\n",
            "Epoch 1 - Training loss: 0.17039250708353926\n",
            "Epoch 1 - Training loss: 0.17050167909865058\n",
            "Epoch 1 - Training loss: 0.17059755565594636\n",
            "Epoch 1 - Training loss: 0.17070385539280708\n",
            "Epoch 1 - Training loss: 0.1710355164153553\n",
            "Epoch 1 - Training loss: 0.17123719095500675\n",
            "Epoch 1 - Training loss: 0.17142108623295832\n",
            "Epoch 1 - Training loss: 0.17174118630556282\n",
            "Epoch 1 - Training loss: 0.17193451274766217\n",
            "Epoch 1 - Training loss: 0.17208378968923205\n",
            "Epoch 1 - Training loss: 0.17220695944689612\n",
            "Epoch 1 - Training loss: 0.17257012702079852\n",
            "Epoch 1 - Training loss: 0.17267742151342858\n",
            "Epoch 1 - Training loss: 0.17272813906118686\n",
            "Epoch 1 - Training loss: 0.17292838613949477\n",
            "Epoch 1 - Training loss: 0.17307996024280342\n",
            "Epoch 1 - Training loss: 0.17322325389157098\n",
            "Epoch 1 - Training loss: 0.17331220251299553\n",
            "Epoch 1 - Training loss: 0.17350466603806405\n",
            "Epoch 1 - Training loss: 0.1736692170193518\n",
            "Epoch 1 - Training loss: 0.1739151377453288\n",
            "Epoch 1 - Training loss: 0.17395338388298875\n",
            "Epoch 1 - Training loss: 0.17404122273328462\n",
            "Epoch 1 - Training loss: 0.174437199645777\n",
            "Epoch 1 - Training loss: 0.17456505328480368\n",
            "Epoch 1 - Training loss: 0.17478095627288576\n",
            "Epoch 1 - Training loss: 0.17487189497774852\n",
            "Epoch 1 - Training loss: 0.17500461433837408\n",
            "Epoch 1 - Training loss: 0.1752816297526934\n",
            "Epoch 1 - Training loss: 0.1755194964010451\n",
            "Epoch 1 - Training loss: 0.17579035083654085\n",
            "Epoch 1 - Training loss: 0.1759683683530481\n",
            "Epoch 1 - Training loss: 0.17607040854214606\n",
            "Epoch 1 - Training loss: 0.17616169159409842\n",
            "Epoch 1 - Training loss: 0.17638174677962687\n",
            "Epoch 1 - Training loss: 0.17646165292209653\n",
            "Epoch 1 - Training loss: 0.17668216443582893\n",
            "Epoch 1 - Training loss: 0.17683929521074174\n",
            "Epoch 1 - Training loss: 0.17700388666981065\n",
            "Epoch 1 - Training loss: 0.17725464393462198\n",
            "Epoch 1 - Training loss: 0.17739742166642694\n",
            "Epoch 1 - Training loss: 0.17750365531711437\n",
            "Epoch 1 - Training loss: 0.1776087980614161\n",
            "Epoch 1 - Training loss: 0.17774235299115243\n",
            "Epoch 1 - Training loss: 0.17782247420757819\n",
            "Epoch 1 - Training loss: 0.1780033739708634\n",
            "Epoch 1 - Training loss: 0.17819804011949344\n",
            "Epoch 1 - Training loss: 0.17836452021337013\n",
            "Epoch 1 - Training loss: 0.17844202195498735\n",
            "Epoch 2 - Training loss: 8.036781634603228e-05\n",
            "Epoch 2 - Training loss: 0.00017917464409809885\n",
            "Epoch 2 - Training loss: 0.00037493362927487666\n",
            "Epoch 2 - Training loss: 0.00045990739772314713\n",
            "Epoch 2 - Training loss: 0.0006619966598843207\n",
            "Epoch 2 - Training loss: 0.000755269104229616\n",
            "Epoch 2 - Training loss: 0.0008284622815244996\n",
            "Epoch 2 - Training loss: 0.0010301007279582114\n",
            "Epoch 2 - Training loss: 0.0011531084553519293\n",
            "Epoch 2 - Training loss: 0.001234239932379997\n",
            "Epoch 2 - Training loss: 0.001406546570916674\n",
            "Epoch 2 - Training loss: 0.0014662860330742304\n",
            "Epoch 2 - Training loss: 0.0015547638778874615\n",
            "Epoch 2 - Training loss: 0.0016712647940177144\n",
            "Epoch 2 - Training loss: 0.0019023921618710703\n",
            "Epoch 2 - Training loss: 0.002026073348674693\n",
            "Epoch 2 - Training loss: 0.002210133031868477\n",
            "Epoch 2 - Training loss: 0.002291826582920831\n",
            "Epoch 2 - Training loss: 0.002471884351168106\n",
            "Epoch 2 - Training loss: 0.0025332935297412915\n",
            "Epoch 2 - Training loss: 0.002662672575857085\n",
            "Epoch 2 - Training loss: 0.0028016146248591735\n",
            "Epoch 2 - Training loss: 0.0029480855411558008\n",
            "Epoch 2 - Training loss: 0.0030605673853522425\n",
            "Epoch 2 - Training loss: 0.003180569033823542\n",
            "Epoch 2 - Training loss: 0.0032941923816320992\n",
            "Epoch 2 - Training loss: 0.0034400684905967227\n",
            "Epoch 2 - Training loss: 0.0035132407061835087\n",
            "Epoch 2 - Training loss: 0.0036100432403814563\n",
            "Epoch 2 - Training loss: 0.003724123726585018\n",
            "Epoch 2 - Training loss: 0.00397485776591911\n",
            "Epoch 2 - Training loss: 0.004138840215483199\n",
            "Epoch 2 - Training loss: 0.004329282949283433\n",
            "Epoch 2 - Training loss: 0.004558580190832935\n",
            "Epoch 2 - Training loss: 0.004696622753003513\n",
            "Epoch 2 - Training loss: 0.004817130008358945\n",
            "Epoch 2 - Training loss: 0.004995816885662485\n",
            "Epoch 2 - Training loss: 0.0051714893120692485\n",
            "Epoch 2 - Training loss: 0.005389035717129453\n",
            "Epoch 2 - Training loss: 0.005610595649874795\n",
            "Epoch 2 - Training loss: 0.005830337196143705\n",
            "Epoch 2 - Training loss: 0.006036281696896055\n",
            "Epoch 2 - Training loss: 0.006194999906172885\n",
            "Epoch 2 - Training loss: 0.0062317334885981035\n",
            "Epoch 2 - Training loss: 0.00638122200123918\n",
            "Epoch 2 - Training loss: 0.006528444075857652\n",
            "Epoch 2 - Training loss: 0.006689600690182592\n",
            "Epoch 2 - Training loss: 0.0068734680880297985\n",
            "Epoch 2 - Training loss: 0.006940975153782983\n",
            "Epoch 2 - Training loss: 0.00701902736462891\n",
            "Epoch 2 - Training loss: 0.0071097774935493085\n",
            "Epoch 2 - Training loss: 0.0072957246518656135\n",
            "Epoch 2 - Training loss: 0.0074296955591135186\n",
            "Epoch 2 - Training loss: 0.007473296050959304\n",
            "Epoch 2 - Training loss: 0.007702735028287241\n",
            "Epoch 2 - Training loss: 0.007873798992588067\n",
            "Epoch 2 - Training loss: 0.008040151171592761\n",
            "Epoch 2 - Training loss: 0.008157305340014541\n",
            "Epoch 2 - Training loss: 0.008414467054008166\n",
            "Epoch 2 - Training loss: 0.008582396484387201\n",
            "Epoch 2 - Training loss: 0.008711518850852685\n",
            "Epoch 2 - Training loss: 0.008752243339157561\n",
            "Epoch 2 - Training loss: 0.008882088554915842\n",
            "Epoch 2 - Training loss: 0.008957806398778329\n",
            "Epoch 2 - Training loss: 0.008993006924957608\n",
            "Epoch 2 - Training loss: 0.00902904968446633\n",
            "Epoch 2 - Training loss: 0.009123014381477066\n",
            "Epoch 2 - Training loss: 0.009244087563235876\n",
            "Epoch 2 - Training loss: 0.009463109628065054\n",
            "Epoch 2 - Training loss: 0.009557192620914628\n",
            "Epoch 2 - Training loss: 0.009706498395754838\n",
            "Epoch 2 - Training loss: 0.009793567977575605\n",
            "Epoch 2 - Training loss: 0.009917674339942333\n",
            "Epoch 2 - Training loss: 0.010085247206821371\n",
            "Epoch 2 - Training loss: 0.01041390238873867\n",
            "Epoch 2 - Training loss: 0.010513617694854482\n",
            "Epoch 2 - Training loss: 0.010695774890561857\n",
            "Epoch 2 - Training loss: 0.010827885090764652\n",
            "Epoch 2 - Training loss: 0.010944576123788921\n",
            "Epoch 2 - Training loss: 0.011136207090162518\n",
            "Epoch 2 - Training loss: 0.011230581299042397\n",
            "Epoch 2 - Training loss: 0.011345566419173659\n",
            "Epoch 2 - Training loss: 0.01145077267610061\n",
            "Epoch 2 - Training loss: 0.011709809394628764\n",
            "Epoch 2 - Training loss: 0.01179108123328767\n",
            "Epoch 2 - Training loss: 0.011861837236707145\n",
            "Epoch 2 - Training loss: 0.011982614142713007\n",
            "Epoch 2 - Training loss: 0.012195758112489796\n",
            "Epoch 2 - Training loss: 0.01238916282540064\n",
            "Epoch 2 - Training loss: 0.012605374671026334\n",
            "Epoch 2 - Training loss: 0.012726855370948817\n",
            "Epoch 2 - Training loss: 0.012838646964128338\n",
            "Epoch 2 - Training loss: 0.013130715093029334\n",
            "Epoch 2 - Training loss: 0.01325675101279577\n",
            "Epoch 2 - Training loss: 0.013316882150704418\n",
            "Epoch 2 - Training loss: 0.01354068361206858\n",
            "Epoch 2 - Training loss: 0.013664691472676262\n",
            "Epoch 2 - Training loss: 0.01375806071102492\n",
            "Epoch 2 - Training loss: 0.013945270099365381\n",
            "Epoch 2 - Training loss: 0.01404307682567568\n",
            "Epoch 2 - Training loss: 0.014180213530688907\n",
            "Epoch 2 - Training loss: 0.01436114749674604\n",
            "Epoch 2 - Training loss: 0.014709818464860733\n",
            "Epoch 2 - Training loss: 0.014941223394641998\n",
            "Epoch 2 - Training loss: 0.015169669125380038\n",
            "Epoch 2 - Training loss: 0.015387774260440615\n",
            "Epoch 2 - Training loss: 0.015526204364004929\n",
            "Epoch 2 - Training loss: 0.015657683735145436\n",
            "Epoch 2 - Training loss: 0.01587230160133417\n",
            "Epoch 2 - Training loss: 0.015943852735798496\n",
            "Epoch 2 - Training loss: 0.016044976122216628\n",
            "Epoch 2 - Training loss: 0.016302002661390855\n",
            "Epoch 2 - Training loss: 0.01641902047942188\n",
            "Epoch 2 - Training loss: 0.016617393426930728\n",
            "Epoch 2 - Training loss: 0.016696964785742608\n",
            "Epoch 2 - Training loss: 0.01684740255636447\n",
            "Epoch 2 - Training loss: 0.017044969125470117\n",
            "Epoch 2 - Training loss: 0.017209589068315177\n",
            "Epoch 2 - Training loss: 0.017421753405889213\n",
            "Epoch 2 - Training loss: 0.017507116614120095\n",
            "Epoch 2 - Training loss: 0.01771650402975489\n",
            "Epoch 2 - Training loss: 0.017738576182154322\n",
            "Epoch 2 - Training loss: 0.017827813628751205\n",
            "Epoch 2 - Training loss: 0.017907199850961217\n",
            "Epoch 2 - Training loss: 0.018112147085940535\n",
            "Epoch 2 - Training loss: 0.01840390168678468\n",
            "Epoch 2 - Training loss: 0.018546611759692494\n",
            "Epoch 2 - Training loss: 0.01878779097748146\n",
            "Epoch 2 - Training loss: 0.018935625604204913\n",
            "Epoch 2 - Training loss: 0.01919328771444208\n",
            "Epoch 2 - Training loss: 0.01936692014328643\n",
            "Epoch 2 - Training loss: 0.019521574195879483\n",
            "Epoch 2 - Training loss: 0.019657964063589887\n",
            "Epoch 2 - Training loss: 0.019809820192820356\n",
            "Epoch 2 - Training loss: 0.019979877327916336\n",
            "Epoch 2 - Training loss: 0.02003894380724697\n",
            "Epoch 2 - Training loss: 0.020096921818708178\n",
            "Epoch 2 - Training loss: 0.020200152448547292\n",
            "Epoch 2 - Training loss: 0.02047395521699429\n",
            "Epoch 2 - Training loss: 0.02065217628209258\n",
            "Epoch 2 - Training loss: 0.02081341742635218\n",
            "Epoch 2 - Training loss: 0.020890998662010565\n",
            "Epoch 2 - Training loss: 0.02091081921400419\n",
            "Epoch 2 - Training loss: 0.0210849516260535\n",
            "Epoch 2 - Training loss: 0.02119847452605584\n",
            "Epoch 2 - Training loss: 0.021267492677579557\n",
            "Epoch 2 - Training loss: 0.021323773160036693\n",
            "Epoch 2 - Training loss: 0.0214984808871741\n",
            "Epoch 2 - Training loss: 0.021628987234729186\n",
            "Epoch 2 - Training loss: 0.021899331599346865\n",
            "Epoch 2 - Training loss: 0.02207202566807458\n",
            "Epoch 2 - Training loss: 0.022506301487877425\n",
            "Epoch 2 - Training loss: 0.02256869907969478\n",
            "Epoch 2 - Training loss: 0.022730125614733837\n",
            "Epoch 2 - Training loss: 0.02300214351637404\n",
            "Epoch 2 - Training loss: 0.023144003269927844\n",
            "Epoch 2 - Training loss: 0.023223075455725828\n",
            "Epoch 2 - Training loss: 0.023274496038839507\n",
            "Epoch 2 - Training loss: 0.02336447462757259\n",
            "Epoch 2 - Training loss: 0.023501159261856506\n",
            "Epoch 2 - Training loss: 0.02378741253826664\n",
            "Epoch 2 - Training loss: 0.023833584628189044\n",
            "Epoch 2 - Training loss: 0.023945404689258605\n",
            "Epoch 2 - Training loss: 0.024268954460109984\n",
            "Epoch 2 - Training loss: 0.024575144918297907\n",
            "Epoch 2 - Training loss: 0.024683512580483707\n",
            "Epoch 2 - Training loss: 0.02473215545926775\n",
            "Epoch 2 - Training loss: 0.024870815688867304\n",
            "Epoch 2 - Training loss: 0.025090992831980494\n",
            "Epoch 2 - Training loss: 0.025155281457406625\n",
            "Epoch 2 - Training loss: 0.025257064776221062\n",
            "Epoch 2 - Training loss: 0.025513883811165528\n",
            "Epoch 2 - Training loss: 0.02570415502275104\n",
            "Epoch 2 - Training loss: 0.02596702334135453\n",
            "Epoch 2 - Training loss: 0.02604130319138961\n",
            "Epoch 2 - Training loss: 0.026188590986801107\n",
            "Epoch 2 - Training loss: 0.026290976825648786\n",
            "Epoch 2 - Training loss: 0.02634009374563755\n",
            "Epoch 2 - Training loss: 0.026551424571350693\n",
            "Epoch 2 - Training loss: 0.026758809432959253\n",
            "Epoch 2 - Training loss: 0.026838592203981333\n",
            "Epoch 2 - Training loss: 0.02696024270247676\n",
            "Epoch 2 - Training loss: 0.027106307284124116\n",
            "Epoch 2 - Training loss: 0.02725399189841137\n",
            "Epoch 2 - Training loss: 0.027472088470864398\n",
            "Epoch 2 - Training loss: 0.02759665979553006\n",
            "Epoch 2 - Training loss: 0.027682870932257\n",
            "Epoch 2 - Training loss: 0.02815805568194974\n",
            "Epoch 2 - Training loss: 0.02822383936804368\n",
            "Epoch 2 - Training loss: 0.028255441901224383\n",
            "Epoch 2 - Training loss: 0.028361520026999116\n",
            "Epoch 2 - Training loss: 0.028505012394983504\n",
            "Epoch 2 - Training loss: 0.028691491467564473\n",
            "Epoch 2 - Training loss: 0.02895649492557941\n",
            "Epoch 2 - Training loss: 0.029142347148565978\n",
            "Epoch 2 - Training loss: 0.029304591881266155\n",
            "Epoch 2 - Training loss: 0.02946130051486083\n",
            "Epoch 2 - Training loss: 0.02965958877357402\n",
            "Epoch 2 - Training loss: 0.0297661475968291\n",
            "Epoch 2 - Training loss: 0.02991881930251429\n",
            "Epoch 2 - Training loss: 0.030220020921833353\n",
            "Epoch 2 - Training loss: 0.030375252164074225\n",
            "Epoch 2 - Training loss: 0.03044070998655517\n",
            "Epoch 2 - Training loss: 0.03061518922392557\n",
            "Epoch 2 - Training loss: 0.030917993051681057\n",
            "Epoch 2 - Training loss: 0.030965690640037628\n",
            "Epoch 2 - Training loss: 0.0310354929214824\n",
            "Epoch 2 - Training loss: 0.03120227089560807\n",
            "Epoch 2 - Training loss: 0.03142394732111997\n",
            "Epoch 2 - Training loss: 0.03154939687701621\n",
            "Epoch 2 - Training loss: 0.03169627290473246\n",
            "Epoch 2 - Training loss: 0.03175754503392652\n",
            "Epoch 2 - Training loss: 0.03184763411048061\n",
            "Epoch 2 - Training loss: 0.0320280308563159\n",
            "Epoch 2 - Training loss: 0.03225031066765346\n",
            "Epoch 2 - Training loss: 0.032379699849100635\n",
            "Epoch 2 - Training loss: 0.03273296332967752\n",
            "Epoch 2 - Training loss: 0.03293100751436023\n",
            "Epoch 2 - Training loss: 0.03306882708335419\n",
            "Epoch 2 - Training loss: 0.033247367522593885\n",
            "Epoch 2 - Training loss: 0.03333694282680877\n",
            "Epoch 2 - Training loss: 0.03347524339511895\n",
            "Epoch 2 - Training loss: 0.033764662008597526\n",
            "Epoch 2 - Training loss: 0.03383152322498148\n",
            "Epoch 2 - Training loss: 0.033988114841568315\n",
            "Epoch 2 - Training loss: 0.03412979617834028\n",
            "Epoch 2 - Training loss: 0.034231370880898\n",
            "Epoch 2 - Training loss: 0.03430128177560405\n",
            "Epoch 2 - Training loss: 0.03444830260511591\n",
            "Epoch 2 - Training loss: 0.034738517811160476\n",
            "Epoch 2 - Training loss: 0.03483114138777767\n",
            "Epoch 2 - Training loss: 0.03500196253304987\n",
            "Epoch 2 - Training loss: 0.03512171662843494\n",
            "Epoch 2 - Training loss: 0.03524495283765262\n",
            "Epoch 2 - Training loss: 0.035409103473350564\n",
            "Epoch 2 - Training loss: 0.0355933462756116\n",
            "Epoch 2 - Training loss: 0.0357494348588624\n",
            "Epoch 2 - Training loss: 0.035967138774597694\n",
            "Epoch 2 - Training loss: 0.036187317665181816\n",
            "Epoch 2 - Training loss: 0.036434874151037064\n",
            "Epoch 2 - Training loss: 0.03645800263372693\n",
            "Epoch 2 - Training loss: 0.03658915502128444\n",
            "Epoch 2 - Training loss: 0.036634585377313435\n",
            "Epoch 2 - Training loss: 0.03687727078795433\n",
            "Epoch 2 - Training loss: 0.03710696507078498\n",
            "Epoch 2 - Training loss: 0.03722109377130008\n",
            "Epoch 2 - Training loss: 0.03730504564257827\n",
            "Epoch 2 - Training loss: 0.03736368642607604\n",
            "Epoch 2 - Training loss: 0.03748786727041959\n",
            "Epoch 2 - Training loss: 0.03763462337397182\n",
            "Epoch 2 - Training loss: 0.03771946428697119\n",
            "Epoch 2 - Training loss: 0.037917170367801366\n",
            "Epoch 2 - Training loss: 0.03805161091580447\n",
            "Epoch 2 - Training loss: 0.03818188453200402\n",
            "Epoch 2 - Training loss: 0.03838385299626571\n",
            "Epoch 2 - Training loss: 0.03849216582757959\n",
            "Epoch 2 - Training loss: 0.039001242013405886\n",
            "Epoch 2 - Training loss: 0.03912621758369876\n",
            "Epoch 2 - Training loss: 0.0392611463433072\n",
            "Epoch 2 - Training loss: 0.039333040831979915\n",
            "Epoch 2 - Training loss: 0.03957200716378719\n",
            "Epoch 2 - Training loss: 0.03976443613658963\n",
            "Epoch 2 - Training loss: 0.039823328988797375\n",
            "Epoch 2 - Training loss: 0.04009137682712027\n",
            "Epoch 2 - Training loss: 0.04021599511903867\n",
            "Epoch 2 - Training loss: 0.04038587282064246\n",
            "Epoch 2 - Training loss: 0.04050269713009726\n",
            "Epoch 2 - Training loss: 0.04060183313371403\n",
            "Epoch 2 - Training loss: 0.040758170906319294\n",
            "Epoch 2 - Training loss: 0.04084042400550614\n",
            "Epoch 2 - Training loss: 0.041154248655827314\n",
            "Epoch 2 - Training loss: 0.04126761293312761\n",
            "Epoch 2 - Training loss: 0.04141354427805969\n",
            "Epoch 2 - Training loss: 0.041806628554265125\n",
            "Epoch 2 - Training loss: 0.041970637644817836\n",
            "Epoch 2 - Training loss: 0.042305665249541116\n",
            "Epoch 2 - Training loss: 0.0423719192078627\n",
            "Epoch 2 - Training loss: 0.042435186841626414\n",
            "Epoch 2 - Training loss: 0.042560468259047086\n",
            "Epoch 2 - Training loss: 0.04279214515884929\n",
            "Epoch 2 - Training loss: 0.042875623608480636\n",
            "Epoch 2 - Training loss: 0.04294787968860379\n",
            "Epoch 2 - Training loss: 0.04304000719793951\n",
            "Epoch 2 - Training loss: 0.04317641414320672\n",
            "Epoch 2 - Training loss: 0.043275163010525294\n",
            "Epoch 2 - Training loss: 0.0433864055245098\n",
            "Epoch 2 - Training loss: 0.04354861375095367\n",
            "Epoch 2 - Training loss: 0.04359793898536325\n",
            "Epoch 2 - Training loss: 0.0436753453388969\n",
            "Epoch 2 - Training loss: 0.043853350210069086\n",
            "Epoch 2 - Training loss: 0.043908378204652494\n",
            "Epoch 2 - Training loss: 0.04410465474306012\n",
            "Epoch 2 - Training loss: 0.0442182183909073\n",
            "Epoch 2 - Training loss: 0.04432263850832163\n",
            "Epoch 2 - Training loss: 0.04438580135340248\n",
            "Epoch 2 - Training loss: 0.04449862255645332\n",
            "Epoch 2 - Training loss: 0.04489842245875518\n",
            "Epoch 2 - Training loss: 0.04497221485177464\n",
            "Epoch 2 - Training loss: 0.04512820068373482\n",
            "Epoch 2 - Training loss: 0.04536577186652465\n",
            "Epoch 2 - Training loss: 0.04550592516705807\n",
            "Epoch 2 - Training loss: 0.0456843761993306\n",
            "Epoch 2 - Training loss: 0.04586157767868627\n",
            "Epoch 2 - Training loss: 0.04588077956639818\n",
            "Epoch 2 - Training loss: 0.04596285594067276\n",
            "Epoch 2 - Training loss: 0.04612748595530481\n",
            "Epoch 2 - Training loss: 0.04633976327481745\n",
            "Epoch 2 - Training loss: 0.0464478507141537\n",
            "Epoch 2 - Training loss: 0.04660377643311392\n",
            "Epoch 2 - Training loss: 0.04683553111125856\n",
            "Epoch 2 - Training loss: 0.046909572653917236\n",
            "Epoch 2 - Training loss: 0.046952837746518886\n",
            "Epoch 2 - Training loss: 0.04711155451016068\n",
            "Epoch 2 - Training loss: 0.04725980463781273\n",
            "Epoch 2 - Training loss: 0.04745656573402284\n",
            "Epoch 2 - Training loss: 0.047494242736287336\n",
            "Epoch 2 - Training loss: 0.04766552726319159\n",
            "Epoch 2 - Training loss: 0.047821782761688296\n",
            "Epoch 2 - Training loss: 0.047929235916735646\n",
            "Epoch 2 - Training loss: 0.04804491599811229\n",
            "Epoch 2 - Training loss: 0.04821407618815266\n",
            "Epoch 2 - Training loss: 0.04826381959235554\n",
            "Epoch 2 - Training loss: 0.048458445898648395\n",
            "Epoch 2 - Training loss: 0.04850966824309976\n",
            "Epoch 2 - Training loss: 0.04856179740184596\n",
            "Epoch 2 - Training loss: 0.04871835724623409\n",
            "Epoch 2 - Training loss: 0.0489696023096122\n",
            "Epoch 2 - Training loss: 0.04924972110521247\n",
            "Epoch 2 - Training loss: 0.04960682574929649\n",
            "Epoch 2 - Training loss: 0.04974969446079246\n",
            "Epoch 2 - Training loss: 0.04998365481262967\n",
            "Epoch 2 - Training loss: 0.050206845818853964\n",
            "Epoch 2 - Training loss: 0.050235971147174645\n",
            "Epoch 2 - Training loss: 0.05031639978345206\n",
            "Epoch 2 - Training loss: 0.050486015284986\n",
            "Epoch 2 - Training loss: 0.05063992027622233\n",
            "Epoch 2 - Training loss: 0.05092346325699391\n",
            "Epoch 2 - Training loss: 0.05096286295184385\n",
            "Epoch 2 - Training loss: 0.05114257251267939\n",
            "Epoch 2 - Training loss: 0.051361314060766175\n",
            "Epoch 2 - Training loss: 0.051574780661334745\n",
            "Epoch 2 - Training loss: 0.05186657326172855\n",
            "Epoch 2 - Training loss: 0.052077244655457516\n",
            "Epoch 2 - Training loss: 0.05225787122549215\n",
            "Epoch 2 - Training loss: 0.05237231142858643\n",
            "Epoch 2 - Training loss: 0.05246879736076731\n",
            "Epoch 2 - Training loss: 0.05262661387504482\n",
            "Epoch 2 - Training loss: 0.05285596290527821\n",
            "Epoch 2 - Training loss: 0.05294130216521432\n",
            "Epoch 2 - Training loss: 0.052993931509713245\n",
            "Epoch 2 - Training loss: 0.05313113228336517\n",
            "Epoch 2 - Training loss: 0.05323409593141854\n",
            "Epoch 2 - Training loss: 0.05332489027135344\n",
            "Epoch 2 - Training loss: 0.05348597779703229\n",
            "Epoch 2 - Training loss: 0.05365885047317504\n",
            "Epoch 2 - Training loss: 0.0537376222766634\n",
            "Epoch 2 - Training loss: 0.05386879768317887\n",
            "Epoch 2 - Training loss: 0.053952141744352734\n",
            "Epoch 2 - Training loss: 0.05404876984123673\n",
            "Epoch 2 - Training loss: 0.05418158274715834\n",
            "Epoch 2 - Training loss: 0.05433634854257424\n",
            "Epoch 2 - Training loss: 0.054474038369794774\n",
            "Epoch 2 - Training loss: 0.05460559186547486\n",
            "Epoch 2 - Training loss: 0.05466400883369037\n",
            "Epoch 2 - Training loss: 0.05481418329221544\n",
            "Epoch 2 - Training loss: 0.05497533266247907\n",
            "Epoch 2 - Training loss: 0.055208365838013605\n",
            "Epoch 2 - Training loss: 0.05532670750824818\n",
            "Epoch 2 - Training loss: 0.055354415415240124\n",
            "Epoch 2 - Training loss: 0.05546247334614682\n",
            "Epoch 2 - Training loss: 0.05555599678291886\n",
            "Epoch 2 - Training loss: 0.055734262630931224\n",
            "Epoch 2 - Training loss: 0.05585384982695648\n",
            "Epoch 2 - Training loss: 0.05595514569669835\n",
            "Epoch 2 - Training loss: 0.05614027373595978\n",
            "Epoch 2 - Training loss: 0.05623098612745116\n",
            "Epoch 2 - Training loss: 0.056304878817358885\n",
            "Epoch 2 - Training loss: 0.05632093959033235\n",
            "Epoch 2 - Training loss: 0.05661757211329967\n",
            "Epoch 2 - Training loss: 0.056751312354384964\n",
            "Epoch 2 - Training loss: 0.05699804039604501\n",
            "Epoch 2 - Training loss: 0.05704274910218172\n",
            "Epoch 2 - Training loss: 0.05716608335246155\n",
            "Epoch 2 - Training loss: 0.05726074047688482\n",
            "Epoch 2 - Training loss: 0.057410632718854875\n",
            "Epoch 2 - Training loss: 0.057646268625249235\n",
            "Epoch 2 - Training loss: 0.05781193774964001\n",
            "Epoch 2 - Training loss: 0.05807736624024316\n",
            "Epoch 2 - Training loss: 0.05814388869747297\n",
            "Epoch 2 - Training loss: 0.0582891010057761\n",
            "Epoch 2 - Training loss: 0.05850875473368778\n",
            "Epoch 2 - Training loss: 0.0586454687572555\n",
            "Epoch 2 - Training loss: 0.05889633868628346\n",
            "Epoch 2 - Training loss: 0.0590202703015573\n",
            "Epoch 2 - Training loss: 0.05916688532066117\n",
            "Epoch 2 - Training loss: 0.05927225505349351\n",
            "Epoch 2 - Training loss: 0.05931987039554221\n",
            "Epoch 2 - Training loss: 0.059466971036817216\n",
            "Epoch 2 - Training loss: 0.05969933790168656\n",
            "Epoch 2 - Training loss: 0.05983695676570127\n",
            "Epoch 2 - Training loss: 0.05993195610847681\n",
            "Epoch 2 - Training loss: 0.060006689887914834\n",
            "Epoch 2 - Training loss: 0.060107771454176416\n",
            "Epoch 2 - Training loss: 0.060225381688681495\n",
            "Epoch 2 - Training loss: 0.060412362094785864\n",
            "Epoch 2 - Training loss: 0.060477395265944985\n",
            "Epoch 2 - Training loss: 0.060586791537177845\n",
            "Epoch 2 - Training loss: 0.06074558854150747\n",
            "Epoch 2 - Training loss: 0.06083910788760892\n",
            "Epoch 2 - Training loss: 0.060878459098083634\n",
            "Epoch 2 - Training loss: 0.06113265706960962\n",
            "Epoch 2 - Training loss: 0.06129271728890156\n",
            "Epoch 2 - Training loss: 0.061366500047001754\n",
            "Epoch 2 - Training loss: 0.0614820376618394\n",
            "Epoch 2 - Training loss: 0.0618889361405487\n",
            "Epoch 2 - Training loss: 0.06193353180517393\n",
            "Epoch 2 - Training loss: 0.062047027246053536\n",
            "Epoch 2 - Training loss: 0.06211150683033695\n",
            "Epoch 2 - Training loss: 0.06228616291176536\n",
            "Epoch 2 - Training loss: 0.0624247643627973\n",
            "Epoch 2 - Training loss: 0.06258966874625128\n",
            "Epoch 2 - Training loss: 0.06272905981585161\n",
            "Epoch 2 - Training loss: 0.06292051076094733\n",
            "Epoch 2 - Training loss: 0.06306810182199549\n",
            "Epoch 2 - Training loss: 0.06331451519179954\n",
            "Epoch 2 - Training loss: 0.06349579676159664\n",
            "Epoch 2 - Training loss: 0.06363504828770024\n",
            "Epoch 2 - Training loss: 0.06377732166762291\n",
            "Epoch 2 - Training loss: 0.06388718846128948\n",
            "Epoch 2 - Training loss: 0.06391802037567663\n",
            "Epoch 2 - Training loss: 0.06403891932664078\n",
            "Epoch 2 - Training loss: 0.06428320629438802\n",
            "Epoch 2 - Training loss: 0.06442589863840896\n",
            "Epoch 2 - Training loss: 0.0647103102075488\n",
            "Epoch 2 - Training loss: 0.06476725287822836\n",
            "Epoch 2 - Training loss: 0.06482759434412093\n",
            "Epoch 2 - Training loss: 0.06496869447206192\n",
            "Epoch 2 - Training loss: 0.06507565191726504\n",
            "Epoch 2 - Training loss: 0.06510430408605952\n",
            "Epoch 2 - Training loss: 0.06539382055791011\n",
            "Epoch 2 - Training loss: 0.06555400130503786\n",
            "Epoch 2 - Training loss: 0.06565619902864003\n",
            "Epoch 2 - Training loss: 0.06592417703167017\n",
            "Epoch 2 - Training loss: 0.06611660846681801\n",
            "Epoch 2 - Training loss: 0.06634076846155848\n",
            "Epoch 2 - Training loss: 0.06645759501095329\n",
            "Epoch 2 - Training loss: 0.06654858523443627\n",
            "Epoch 2 - Training loss: 0.06669335549613878\n",
            "Epoch 2 - Training loss: 0.0669372765593596\n",
            "Epoch 2 - Training loss: 0.06702436054192944\n",
            "Epoch 2 - Training loss: 0.06709497596131268\n",
            "Epoch 2 - Training loss: 0.06723968518862147\n",
            "Epoch 2 - Training loss: 0.06737515901618485\n",
            "Epoch 2 - Training loss: 0.06745405165728793\n",
            "Epoch 2 - Training loss: 0.06761275183584199\n",
            "Epoch 2 - Training loss: 0.06766212493506894\n",
            "Epoch 2 - Training loss: 0.06770531180054584\n",
            "Epoch 2 - Training loss: 0.06786754090132426\n",
            "Epoch 2 - Training loss: 0.06800134783225463\n",
            "Epoch 2 - Training loss: 0.06818040434731794\n",
            "Epoch 2 - Training loss: 0.06826670918820192\n",
            "Epoch 2 - Training loss: 0.06830851040852032\n",
            "Epoch 2 - Training loss: 0.06863002958638009\n",
            "Epoch 2 - Training loss: 0.06867741783266701\n",
            "Epoch 2 - Training loss: 0.06881196724810898\n",
            "Epoch 2 - Training loss: 0.06892875196940418\n",
            "Epoch 2 - Training loss: 0.0690664076891694\n",
            "Epoch 2 - Training loss: 0.06912388057032945\n",
            "Epoch 2 - Training loss: 0.06941258561199726\n",
            "Epoch 2 - Training loss: 0.06944126744411075\n",
            "Epoch 2 - Training loss: 0.06961173739538455\n",
            "Epoch 2 - Training loss: 0.06971167425698499\n",
            "Epoch 2 - Training loss: 0.06983701075151214\n",
            "Epoch 2 - Training loss: 0.06998669548925243\n",
            "Epoch 2 - Training loss: 0.07008193550643319\n",
            "Epoch 2 - Training loss: 0.07019234237981949\n",
            "Epoch 2 - Training loss: 0.07026139308196078\n",
            "Epoch 2 - Training loss: 0.07037135270604891\n",
            "Epoch 2 - Training loss: 0.07058591209550592\n",
            "Epoch 2 - Training loss: 0.07069471776128006\n",
            "Epoch 2 - Training loss: 0.07090807959302338\n",
            "Epoch 2 - Training loss: 0.07097423195179656\n",
            "Epoch 2 - Training loss: 0.0710990517986005\n",
            "Epoch 2 - Training loss: 0.07130659421854245\n",
            "Epoch 2 - Training loss: 0.0714092805429594\n",
            "Epoch 2 - Training loss: 0.07147228229107824\n",
            "Epoch 2 - Training loss: 0.07163467999102908\n",
            "Epoch 2 - Training loss: 0.07171082174750978\n",
            "Epoch 2 - Training loss: 0.07183307920620322\n",
            "Epoch 2 - Training loss: 0.07194283644932864\n",
            "Epoch 2 - Training loss: 0.07204574827692592\n",
            "Epoch 2 - Training loss: 0.07210846424206042\n",
            "Epoch 2 - Training loss: 0.0722936159039516\n",
            "Epoch 2 - Training loss: 0.07247428638316485\n",
            "Epoch 2 - Training loss: 0.07254863754589992\n",
            "Epoch 2 - Training loss: 0.07260188685734072\n",
            "Epoch 2 - Training loss: 0.07273849623917199\n",
            "Epoch 2 - Training loss: 0.0728931120856961\n",
            "Epoch 2 - Training loss: 0.07292000702353937\n",
            "Epoch 2 - Training loss: 0.07313735710818377\n",
            "Epoch 2 - Training loss: 0.07328270889643922\n",
            "Epoch 2 - Training loss: 0.0733607559999042\n",
            "Epoch 2 - Training loss: 0.07348098833836726\n",
            "Epoch 2 - Training loss: 0.0736188969171759\n",
            "Epoch 2 - Training loss: 0.07372023752217353\n",
            "Epoch 2 - Training loss: 0.07390350034273764\n",
            "Epoch 2 - Training loss: 0.07404395568567806\n",
            "Epoch 2 - Training loss: 0.07409140053413697\n",
            "Epoch 2 - Training loss: 0.07422731105468548\n",
            "Epoch 2 - Training loss: 0.07432313192325996\n",
            "Epoch 2 - Training loss: 0.07447122962378872\n",
            "Epoch 2 - Training loss: 0.07452301653241043\n",
            "Epoch 2 - Training loss: 0.07460921003556709\n",
            "Epoch 2 - Training loss: 0.07464507215026854\n",
            "Epoch 2 - Training loss: 0.07474476710629108\n",
            "Epoch 2 - Training loss: 0.07486241249673402\n",
            "Epoch 2 - Training loss: 0.07491662929148309\n",
            "Epoch 2 - Training loss: 0.07495334639208022\n",
            "Epoch 2 - Training loss: 0.07505015057048944\n",
            "Epoch 2 - Training loss: 0.07509658926093121\n",
            "Epoch 2 - Training loss: 0.07521962749757874\n",
            "Epoch 2 - Training loss: 0.07528683089259972\n",
            "Epoch 2 - Training loss: 0.07536160299725243\n",
            "Epoch 2 - Training loss: 0.07557667518999658\n",
            "Epoch 2 - Training loss: 0.07576003369253709\n",
            "Epoch 2 - Training loss: 0.07593022938619164\n",
            "Epoch 2 - Training loss: 0.07603410563505153\n",
            "Epoch 2 - Training loss: 0.07606383275065913\n",
            "Epoch 2 - Training loss: 0.07609554291414872\n",
            "Epoch 2 - Training loss: 0.07616071214180575\n",
            "Epoch 2 - Training loss: 0.07632124235134707\n",
            "Epoch 2 - Training loss: 0.07638216522861836\n",
            "Epoch 2 - Training loss: 0.07650531821056152\n",
            "Epoch 2 - Training loss: 0.07656659878718891\n",
            "Epoch 2 - Training loss: 0.0766963344762352\n",
            "Epoch 2 - Training loss: 0.07681967616120954\n",
            "Epoch 2 - Training loss: 0.07685117194178834\n",
            "Epoch 2 - Training loss: 0.07700074811392565\n",
            "Epoch 2 - Training loss: 0.07722464298555401\n",
            "Epoch 2 - Training loss: 0.07734116715893372\n",
            "Epoch 2 - Training loss: 0.07744768422557664\n",
            "Epoch 2 - Training loss: 0.0774714941047688\n",
            "Epoch 2 - Training loss: 0.07765531338362107\n",
            "Epoch 2 - Training loss: 0.0777339039108297\n",
            "Epoch 2 - Training loss: 0.0779098041280151\n",
            "Epoch 2 - Training loss: 0.07794100795981726\n",
            "Epoch 2 - Training loss: 0.07818830130832281\n",
            "Epoch 2 - Training loss: 0.07838459889581209\n",
            "Epoch 2 - Training loss: 0.07840007276280221\n",
            "Epoch 2 - Training loss: 0.07849755121081241\n",
            "Epoch 2 - Training loss: 0.07860489989092737\n",
            "Epoch 2 - Training loss: 0.07882650602044963\n",
            "Epoch 2 - Training loss: 0.07897986999468636\n",
            "Epoch 2 - Training loss: 0.07905401317660869\n",
            "Epoch 2 - Training loss: 0.0792711861789036\n",
            "Epoch 2 - Training loss: 0.07940462903220898\n",
            "Epoch 2 - Training loss: 0.0796556005846145\n",
            "Epoch 2 - Training loss: 0.07979309679205611\n",
            "Epoch 2 - Training loss: 0.08000737923914308\n",
            "Epoch 2 - Training loss: 0.08025782129197105\n",
            "Epoch 2 - Training loss: 0.08050879869046115\n",
            "Epoch 2 - Training loss: 0.08087698770150828\n",
            "Epoch 2 - Training loss: 0.08090128107016274\n",
            "Epoch 2 - Training loss: 0.08108332692019975\n",
            "Epoch 2 - Training loss: 0.08120949816570353\n",
            "Epoch 2 - Training loss: 0.08130877306148696\n",
            "Epoch 2 - Training loss: 0.08151147211157182\n",
            "Epoch 2 - Training loss: 0.08156550991366794\n",
            "Epoch 2 - Training loss: 0.08181599606630771\n",
            "Epoch 2 - Training loss: 0.08192388480231325\n",
            "Epoch 2 - Training loss: 0.08218529804953253\n",
            "Epoch 2 - Training loss: 0.08227006242330522\n",
            "Epoch 2 - Training loss: 0.08243797245675694\n",
            "Epoch 2 - Training loss: 0.08255635790908133\n",
            "Epoch 2 - Training loss: 0.08258414522869818\n",
            "Epoch 2 - Training loss: 0.08268589643376277\n",
            "Epoch 2 - Training loss: 0.08275597569729283\n",
            "Epoch 2 - Training loss: 0.08280842114827716\n",
            "Epoch 2 - Training loss: 0.08293351896210456\n",
            "Epoch 2 - Training loss: 0.0830771132235302\n",
            "Epoch 2 - Training loss: 0.08313590135854254\n",
            "Epoch 2 - Training loss: 0.08326581466212266\n",
            "Epoch 2 - Training loss: 0.0833763690141123\n",
            "Epoch 2 - Training loss: 0.08358056972951078\n",
            "Epoch 2 - Training loss: 0.08372375096823932\n",
            "Epoch 2 - Training loss: 0.08377471361865303\n",
            "Epoch 2 - Training loss: 0.08390551754462122\n",
            "Epoch 2 - Training loss: 0.08421637517993828\n",
            "Epoch 2 - Training loss: 0.08438630392357929\n",
            "Epoch 2 - Training loss: 0.0846413935342037\n",
            "Epoch 2 - Training loss: 0.08476091361940225\n",
            "Epoch 2 - Training loss: 0.08495086658475941\n",
            "Epoch 2 - Training loss: 0.08507915968889557\n",
            "Epoch 2 - Training loss: 0.08526228561894154\n",
            "Epoch 2 - Training loss: 0.0853194554211266\n",
            "Epoch 2 - Training loss: 0.08547500656175017\n",
            "Epoch 2 - Training loss: 0.08557996994404714\n",
            "Epoch 2 - Training loss: 0.08576910793662135\n",
            "Epoch 2 - Training loss: 0.08586507780886472\n",
            "Epoch 2 - Training loss: 0.08600451628258551\n",
            "Epoch 2 - Training loss: 0.08611578708971297\n",
            "Epoch 2 - Training loss: 0.08623865015034292\n",
            "Epoch 2 - Training loss: 0.08641481788944143\n",
            "Epoch 2 - Training loss: 0.08653523875618858\n",
            "Epoch 2 - Training loss: 0.08661078265520619\n",
            "Epoch 2 - Training loss: 0.0868576024867483\n",
            "Epoch 2 - Training loss: 0.08701279912906479\n",
            "Epoch 2 - Training loss: 0.08714707381030454\n",
            "Epoch 2 - Training loss: 0.08723931518325737\n",
            "Epoch 2 - Training loss: 0.08727739273365945\n",
            "Epoch 2 - Training loss: 0.08740535094293514\n",
            "Epoch 2 - Training loss: 0.08758963201480952\n",
            "Epoch 2 - Training loss: 0.08782876613559817\n",
            "Epoch 2 - Training loss: 0.08791413549969254\n",
            "Epoch 2 - Training loss: 0.08800971537613983\n",
            "Epoch 2 - Training loss: 0.08815034019198817\n",
            "Epoch 2 - Training loss: 0.08828457021740263\n",
            "Epoch 2 - Training loss: 0.08833992526705649\n",
            "Epoch 2 - Training loss: 0.08839651786370763\n",
            "Epoch 2 - Training loss: 0.08859875095861235\n",
            "Epoch 2 - Training loss: 0.0888586924699292\n",
            "Epoch 2 - Training loss: 0.08909832142682662\n",
            "Epoch 2 - Training loss: 0.08912114150671245\n",
            "Epoch 2 - Training loss: 0.08920174952882376\n",
            "Epoch 2 - Training loss: 0.0892415762539389\n",
            "Epoch 2 - Training loss: 0.08934981149754354\n",
            "Epoch 2 - Training loss: 0.0894611228857118\n",
            "Epoch 2 - Training loss: 0.08959291600731453\n",
            "Epoch 2 - Training loss: 0.08964783039245842\n",
            "Epoch 2 - Training loss: 0.0897320567675904\n",
            "Epoch 2 - Training loss: 0.08976770884422923\n",
            "Epoch 2 - Training loss: 0.08981594343437378\n",
            "Epoch 2 - Training loss: 0.09000198424124578\n",
            "Epoch 2 - Training loss: 0.09012424958738754\n",
            "Epoch 2 - Training loss: 0.090331944148503\n",
            "Epoch 2 - Training loss: 0.09044801937277193\n",
            "Epoch 2 - Training loss: 0.0905560743568072\n",
            "Epoch 2 - Training loss: 0.09068031316952728\n",
            "Epoch 2 - Training loss: 0.09077507655606913\n",
            "Epoch 2 - Training loss: 0.09080683740018718\n",
            "Epoch 2 - Training loss: 0.09088993199399985\n",
            "Epoch 2 - Training loss: 0.09095891506107313\n",
            "Epoch 2 - Training loss: 0.0911611325399422\n",
            "Epoch 2 - Training loss: 0.09130062548709768\n",
            "Epoch 2 - Training loss: 0.09165182228146522\n",
            "Epoch 2 - Training loss: 0.09182825225240576\n",
            "Epoch 2 - Training loss: 0.09187884357319012\n",
            "Epoch 2 - Training loss: 0.0920614262145243\n",
            "Epoch 2 - Training loss: 0.09218962996729466\n",
            "Epoch 2 - Training loss: 0.09224754017688382\n",
            "Epoch 2 - Training loss: 0.09239371346710905\n",
            "Epoch 2 - Training loss: 0.09256818353820012\n",
            "Epoch 2 - Training loss: 0.09263272942709071\n",
            "Epoch 2 - Training loss: 0.09290117985094344\n",
            "Epoch 2 - Training loss: 0.09307703590477263\n",
            "Epoch 2 - Training loss: 0.09314853803062045\n",
            "Epoch 2 - Training loss: 0.09325677716631943\n",
            "Epoch 2 - Training loss: 0.09332106443348406\n",
            "Epoch 2 - Training loss: 0.09342938020372632\n",
            "Epoch 2 - Training loss: 0.09362503529381333\n",
            "Epoch 2 - Training loss: 0.09367244597524405\n",
            "Epoch 2 - Training loss: 0.09370970551464666\n",
            "Epoch 2 - Training loss: 0.09381234242535158\n",
            "Epoch 2 - Training loss: 0.09389002239311746\n",
            "Epoch 2 - Training loss: 0.09412852091107096\n",
            "Epoch 2 - Training loss: 0.09421950457359492\n",
            "Epoch 2 - Training loss: 0.09430852767699627\n",
            "Epoch 2 - Training loss: 0.09439069693729377\n",
            "Epoch 2 - Training loss: 0.09444433250335424\n",
            "Epoch 2 - Training loss: 0.0945687086911185\n",
            "Epoch 2 - Training loss: 0.09463689409331409\n",
            "Epoch 2 - Training loss: 0.09479160918030086\n",
            "Epoch 2 - Training loss: 0.09495159589461108\n",
            "Epoch 2 - Training loss: 0.09505300036172813\n",
            "Epoch 2 - Training loss: 0.09519338064284912\n",
            "Epoch 2 - Training loss: 0.09544546981411639\n",
            "Epoch 2 - Training loss: 0.09552444717578733\n",
            "Epoch 2 - Training loss: 0.09559268437460987\n",
            "Epoch 2 - Training loss: 0.0957000651287594\n",
            "Epoch 2 - Training loss: 0.09578124602744256\n",
            "Epoch 2 - Training loss: 0.09582540314835208\n",
            "Epoch 2 - Training loss: 0.0958920514751186\n",
            "Epoch 2 - Training loss: 0.09593057996833693\n",
            "Epoch 2 - Training loss: 0.09614774586954541\n",
            "Epoch 2 - Training loss: 0.09620741079809633\n",
            "Epoch 2 - Training loss: 0.0963241029510906\n",
            "Epoch 2 - Training loss: 0.09648027306180328\n",
            "Epoch 2 - Training loss: 0.0965502006448567\n",
            "Epoch 2 - Training loss: 0.09672289441174853\n",
            "Epoch 2 - Training loss: 0.09694090087824603\n",
            "Epoch 2 - Training loss: 0.09704193660516792\n",
            "Epoch 2 - Training loss: 0.09717452286411005\n",
            "Epoch 2 - Training loss: 0.09747796223115629\n",
            "Epoch 2 - Training loss: 0.09765635631176264\n",
            "Epoch 2 - Training loss: 0.09777462707439274\n",
            "Epoch 2 - Training loss: 0.09792704968604007\n",
            "Epoch 2 - Training loss: 0.09803912539416348\n",
            "Epoch 2 - Training loss: 0.09825384639290922\n",
            "Epoch 2 - Training loss: 0.098420720001353\n",
            "Epoch 2 - Training loss: 0.09849743720001058\n",
            "Epoch 2 - Training loss: 0.09878452964373298\n",
            "Epoch 2 - Training loss: 0.09891032409478924\n",
            "Epoch 2 - Training loss: 0.09936359511422259\n",
            "Epoch 2 - Training loss: 0.09949135921125028\n",
            "Epoch 2 - Training loss: 0.09981478615078146\n",
            "Epoch 2 - Training loss: 0.09988763752276264\n",
            "Epoch 2 - Training loss: 0.10015966492429026\n",
            "Epoch 2 - Training loss: 0.10018919214510968\n",
            "Epoch 2 - Training loss: 0.10030740582898481\n",
            "Epoch 2 - Training loss: 0.10043918286591197\n",
            "Epoch 2 - Training loss: 0.10067981566542755\n",
            "Epoch 2 - Training loss: 0.10100950271304228\n",
            "Epoch 2 - Training loss: 0.1011507917425907\n",
            "Epoch 2 - Training loss: 0.10123461184661779\n",
            "Epoch 2 - Training loss: 0.10126923084624413\n",
            "Epoch 2 - Training loss: 0.10157157651889426\n",
            "Epoch 2 - Training loss: 0.10170789725030027\n",
            "Epoch 2 - Training loss: 0.10186222427165204\n",
            "Epoch 2 - Training loss: 0.10194523451822017\n",
            "Epoch 2 - Training loss: 0.10210410585360867\n",
            "Epoch 2 - Training loss: 0.10220016796054489\n",
            "Epoch 2 - Training loss: 0.10237856335969749\n",
            "Epoch 2 - Training loss: 0.10245824733506768\n",
            "Epoch 2 - Training loss: 0.10252634845356316\n",
            "Epoch 2 - Training loss: 0.10259502209119324\n",
            "Epoch 2 - Training loss: 0.10266160877989426\n",
            "Epoch 2 - Training loss: 0.1028400598598251\n",
            "Epoch 2 - Training loss: 0.10293370121911263\n",
            "Epoch 2 - Training loss: 0.10313571851724374\n",
            "Epoch 2 - Training loss: 0.10332489517658378\n",
            "Epoch 2 - Training loss: 0.10347397071418604\n",
            "Epoch 2 - Training loss: 0.10357259335055916\n",
            "Epoch 2 - Training loss: 0.10383810440122064\n",
            "Epoch 2 - Training loss: 0.10396509774442293\n",
            "Epoch 2 - Training loss: 0.10418505953557329\n",
            "Epoch 2 - Training loss: 0.10434122244988296\n",
            "Epoch 2 - Training loss: 0.10443385550255842\n",
            "Epoch 2 - Training loss: 0.10457285115125972\n",
            "Epoch 2 - Training loss: 0.10460432200654865\n",
            "Epoch 2 - Training loss: 0.10475483329803832\n",
            "Epoch 2 - Training loss: 0.10512281302561256\n",
            "Epoch 2 - Training loss: 0.10519346738976837\n",
            "Epoch 2 - Training loss: 0.10531838837542387\n",
            "Epoch 2 - Training loss: 0.10563717292014088\n",
            "Epoch 2 - Training loss: 0.10577973167795235\n",
            "Epoch 2 - Training loss: 0.10583660096279594\n",
            "Epoch 2 - Training loss: 0.10591132301829263\n",
            "Epoch 2 - Training loss: 0.10600657787309019\n",
            "Epoch 2 - Training loss: 0.10610437834822038\n",
            "Epoch 2 - Training loss: 0.10619325927897558\n",
            "Epoch 2 - Training loss: 0.10624592663891026\n",
            "Epoch 2 - Training loss: 0.10630652262791515\n",
            "Epoch 2 - Training loss: 0.10643842278211228\n",
            "Epoch 2 - Training loss: 0.10651497584559134\n",
            "Epoch 2 - Training loss: 0.10655939195757863\n",
            "Epoch 2 - Training loss: 0.10664646319155373\n",
            "Epoch 2 - Training loss: 0.10673749247895502\n",
            "Epoch 2 - Training loss: 0.1067766060492695\n",
            "Epoch 2 - Training loss: 0.1068785723600624\n",
            "Epoch 2 - Training loss: 0.10703175904543034\n",
            "Epoch 2 - Training loss: 0.10709532635060073\n",
            "Epoch 2 - Training loss: 0.10723639365786047\n",
            "Epoch 2 - Training loss: 0.10744010770260524\n",
            "Epoch 2 - Training loss: 0.10748721702473123\n",
            "Epoch 2 - Training loss: 0.10767441073349163\n",
            "Epoch 2 - Training loss: 0.10793582566463744\n",
            "Epoch 2 - Training loss: 0.10810340150221706\n",
            "Epoch 2 - Training loss: 0.10823928830481923\n",
            "Epoch 2 - Training loss: 0.10836369670959296\n",
            "Epoch 2 - Training loss: 0.1084578693953595\n",
            "Epoch 2 - Training loss: 0.10862127889686429\n",
            "Epoch 2 - Training loss: 0.1087037993511602\n",
            "Epoch 2 - Training loss: 0.10873826837409407\n",
            "Epoch 2 - Training loss: 0.10886536275288825\n",
            "Epoch 2 - Training loss: 0.10892310265554929\n",
            "Epoch 2 - Training loss: 0.10904681728259205\n",
            "Epoch 2 - Training loss: 0.10909850738132432\n",
            "Epoch 2 - Training loss: 0.10918746143579483\n",
            "Epoch 2 - Training loss: 0.10933429210869743\n",
            "Epoch 2 - Training loss: 0.10939620726747808\n",
            "Epoch 2 - Training loss: 0.10952949216529759\n",
            "Epoch 2 - Training loss: 0.10956412968811577\n",
            "Epoch 2 - Training loss: 0.10972507673301804\n",
            "Epoch 2 - Training loss: 0.11000886820415572\n",
            "Epoch 2 - Training loss: 0.11017671478852661\n",
            "Epoch 2 - Training loss: 0.11024740257703547\n",
            "Epoch 2 - Training loss: 0.11031576591148687\n",
            "Epoch 2 - Training loss: 0.11068174529320268\n",
            "Epoch 2 - Training loss: 0.11084343766622833\n",
            "Epoch 2 - Training loss: 0.11100732885015163\n",
            "Epoch 2 - Training loss: 0.11108850117828419\n",
            "Epoch 2 - Training loss: 0.11120134636934505\n",
            "Epoch 2 - Training loss: 0.11132407656658305\n",
            "Epoch 2 - Training loss: 0.11150203757623492\n",
            "Epoch 2 - Training loss: 0.11152454610786904\n",
            "Epoch 2 - Training loss: 0.11164407875102911\n",
            "Epoch 2 - Training loss: 0.11169076602318204\n",
            "Epoch 2 - Training loss: 0.11191753797860605\n",
            "Epoch 2 - Training loss: 0.11207958828133624\n",
            "Epoch 2 - Training loss: 0.11222482724651409\n",
            "Epoch 2 - Training loss: 0.11234144067594301\n",
            "Epoch 2 - Training loss: 0.11238700322814778\n",
            "Epoch 2 - Training loss: 0.11257427203049983\n",
            "Epoch 2 - Training loss: 0.11271790894610223\n",
            "Epoch 2 - Training loss: 0.11282866596500439\n",
            "Epoch 2 - Training loss: 0.11289447124030735\n",
            "Epoch 2 - Training loss: 0.11306030721639965\n",
            "Epoch 2 - Training loss: 0.11336895080604978\n",
            "Epoch 2 - Training loss: 0.11354718043375561\n",
            "Epoch 2 - Training loss: 0.11375462078550921\n",
            "Epoch 2 - Training loss: 0.11383412824248645\n",
            "Epoch 2 - Training loss: 0.11390934626200497\n",
            "Epoch 2 - Training loss: 0.11406884648040859\n",
            "Epoch 2 - Training loss: 0.11428441263552604\n",
            "Epoch 2 - Training loss: 0.11465038807748922\n",
            "Epoch 2 - Training loss: 0.11475146322576667\n",
            "Epoch 2 - Training loss: 0.11484353282232719\n",
            "Epoch 2 - Training loss: 0.115030335335891\n",
            "Epoch 2 - Training loss: 0.11521262168558612\n",
            "Epoch 2 - Training loss: 0.11536863080656795\n",
            "Epoch 2 - Training loss: 0.11546328647741314\n",
            "Epoch 2 - Training loss: 0.1155053105995631\n",
            "Epoch 2 - Training loss: 0.11560839311535488\n",
            "Epoch 2 - Training loss: 0.11567029560099977\n",
            "Epoch 2 - Training loss: 0.11580403593740166\n",
            "Epoch 2 - Training loss: 0.11584918025229722\n",
            "Epoch 2 - Training loss: 0.11593324704361814\n",
            "Epoch 2 - Training loss: 0.11608232436046353\n",
            "Epoch 2 - Training loss: 0.11621609883609293\n",
            "Epoch 2 - Training loss: 0.11639286935734533\n",
            "Epoch 2 - Training loss: 0.11646799840676442\n",
            "Epoch 2 - Training loss: 0.11661145601596341\n",
            "Epoch 2 - Training loss: 0.11668324547369024\n",
            "Epoch 2 - Training loss: 0.11673974860777288\n",
            "Epoch 2 - Training loss: 0.11675930797442127\n",
            "Epoch 2 - Training loss: 0.11681491117289008\n",
            "Epoch 2 - Training loss: 0.11713028659245797\n",
            "Epoch 2 - Training loss: 0.1172766147244936\n",
            "Epoch 2 - Training loss: 0.11742732111316945\n",
            "Epoch 2 - Training loss: 0.11756026581811435\n",
            "Epoch 2 - Training loss: 0.11770574900029755\n",
            "Epoch 2 - Training loss: 0.11774689187285806\n",
            "Epoch 2 - Training loss: 0.11785222173952409\n",
            "Epoch 2 - Training loss: 0.11796017911142187\n",
            "Epoch 2 - Training loss: 0.11806264790748038\n",
            "Epoch 2 - Training loss: 0.11816191143675972\n",
            "Epoch 2 - Training loss: 0.11817828600288931\n",
            "Epoch 2 - Training loss: 0.11830382434619484\n",
            "Epoch 2 - Training loss: 0.1184370575903623\n",
            "Epoch 2 - Training loss: 0.11859775147438525\n",
            "Epoch 2 - Training loss: 0.11869859720355888\n",
            "Epoch 2 - Training loss: 0.11885444026714417\n",
            "Epoch 2 - Training loss: 0.11891140835558268\n",
            "Epoch 2 - Training loss: 0.11903951573533131\n",
            "Epoch 2 - Training loss: 0.11906779493028516\n",
            "Epoch 2 - Training loss: 0.11937606954939171\n",
            "Epoch 2 - Training loss: 0.11953882718069563\n",
            "Epoch 2 - Training loss: 0.11968686858088827\n",
            "Epoch 2 - Training loss: 0.11982154797639515\n",
            "Epoch 2 - Training loss: 0.11991650194847095\n",
            "Epoch 2 - Training loss: 0.11997265180350462\n",
            "Epoch 2 - Training loss: 0.12003550897620476\n",
            "Epoch 2 - Training loss: 0.12019451054583576\n",
            "Epoch 2 - Training loss: 0.12028221779568458\n",
            "Epoch 2 - Training loss: 0.12050679486407749\n",
            "Epoch 2 - Training loss: 0.1205610608024749\n",
            "Epoch 2 - Training loss: 0.12075930783676027\n",
            "Epoch 2 - Training loss: 0.12090314717304065\n",
            "Epoch 2 - Training loss: 0.12101299745310694\n",
            "Epoch 2 - Training loss: 0.1210680087633145\n",
            "Epoch 2 - Training loss: 0.12118221453524061\n",
            "Epoch 2 - Training loss: 0.1213312359647885\n",
            "Epoch 2 - Training loss: 0.12146453388920947\n",
            "Epoch 2 - Training loss: 0.12152726273399903\n",
            "Epoch 2 - Training loss: 0.12159203656855772\n",
            "Epoch 2 - Training loss: 0.12164838479331402\n",
            "Epoch 2 - Training loss: 0.121846036956723\n",
            "Epoch 2 - Training loss: 0.1220201080007705\n",
            "Epoch 2 - Training loss: 0.12216322936479058\n",
            "Epoch 2 - Training loss: 0.12229523734588708\n",
            "Epoch 2 - Training loss: 0.12235489155926398\n",
            "Epoch 2 - Training loss: 0.12257928870169561\n",
            "Epoch 2 - Training loss: 0.1226974372207912\n",
            "Epoch 2 - Training loss: 0.12286802437970601\n",
            "Epoch 2 - Training loss: 0.12308340415910585\n",
            "Epoch 2 - Training loss: 0.12333890731666865\n",
            "Epoch 2 - Training loss: 0.12346282114745426\n",
            "Epoch 2 - Training loss: 0.12366806059134508\n",
            "Epoch 2 - Training loss: 0.12369095103076494\n",
            "Epoch 2 - Training loss: 0.12396812648164915\n",
            "Epoch 2 - Training loss: 0.12408573573121606\n",
            "Epoch 2 - Training loss: 0.12414106523205064\n",
            "Epoch 2 - Training loss: 0.12424071299841505\n",
            "Epoch 2 - Training loss: 0.12432708409227637\n",
            "Epoch 2 - Training loss: 0.12469371331077236\n",
            "Epoch 2 - Training loss: 0.1247320810962381\n",
            "Epoch 2 - Training loss: 0.12501077166796207\n",
            "Epoch 2 - Training loss: 0.1250647986158808\n",
            "Epoch 2 - Training loss: 0.12511715887765737\n",
            "Epoch 2 - Training loss: 0.12521773888799015\n",
            "Epoch 2 - Training loss: 0.12543359046865468\n",
            "Epoch 2 - Training loss: 0.12551200725753775\n",
            "Epoch 2 - Training loss: 0.12558590246539222\n",
            "Epoch 2 - Training loss: 0.12570885783220245\n",
            "Epoch 2 - Training loss: 0.12582705930761023\n",
            "Epoch 2 - Training loss: 0.12590168834205218\n",
            "Epoch 2 - Training loss: 0.12611764909199147\n",
            "Epoch 2 - Training loss: 0.12622048917002102\n",
            "Epoch 2 - Training loss: 0.12648865262737502\n",
            "Epoch 2 - Training loss: 0.1267857976359075\n",
            "Epoch 2 - Training loss: 0.12687980704434487\n",
            "Epoch 2 - Training loss: 0.12696052964494775\n",
            "Epoch 2 - Training loss: 0.12711157836317857\n",
            "Epoch 2 - Training loss: 0.12724579969889668\n",
            "Epoch 2 - Training loss: 0.12735358284794288\n",
            "Epoch 2 - Training loss: 0.12776478776223718\n",
            "Epoch 2 - Training loss: 0.12784110216884567\n",
            "Epoch 2 - Training loss: 0.12801990080485814\n",
            "Epoch 2 - Training loss: 0.12816037373371056\n",
            "Epoch 2 - Training loss: 0.1282720516936214\n",
            "Epoch 2 - Training loss: 0.12848661630960512\n",
            "Epoch 2 - Training loss: 0.12851492191122763\n",
            "Epoch 2 - Training loss: 0.128742715168093\n",
            "Epoch 2 - Training loss: 0.1289149273319968\n",
            "Epoch 2 - Training loss: 0.12902814585433553\n",
            "Epoch 2 - Training loss: 0.12909750898044978\n",
            "Epoch 2 - Training loss: 0.129214857173746\n",
            "Epoch 2 - Training loss: 0.12951385583589548\n",
            "Epoch 2 - Training loss: 0.12964058894890426\n",
            "Epoch 2 - Training loss: 0.12975720563498355\n",
            "Epoch 2 - Training loss: 0.1299195691162764\n",
            "Epoch 2 - Training loss: 0.1300251222541258\n",
            "Epoch 2 - Training loss: 0.13007368204066716\n",
            "Epoch 2 - Training loss: 0.13023927982058575\n",
            "Epoch 2 - Training loss: 0.1304002391454428\n",
            "Epoch 2 - Training loss: 0.1304611234711225\n",
            "Epoch 2 - Training loss: 0.13080644018467524\n",
            "Epoch 2 - Training loss: 0.13104063835141977\n",
            "Epoch 3 - Training loss: 0.0002466664035945559\n",
            "Epoch 3 - Training loss: 0.00029571929068834796\n",
            "Epoch 3 - Training loss: 0.00037506934819318084\n",
            "Epoch 3 - Training loss: 0.0004830713282579552\n",
            "Epoch 3 - Training loss: 0.0005151518066522918\n",
            "Epoch 3 - Training loss: 0.0005798706713356952\n",
            "Epoch 3 - Training loss: 0.0006436071197774365\n",
            "Epoch 3 - Training loss: 0.000923884585738055\n",
            "Epoch 3 - Training loss: 0.0010615160950878536\n",
            "Epoch 3 - Training loss: 0.0013633099164186255\n",
            "Epoch 3 - Training loss: 0.0014928004432366348\n",
            "Epoch 3 - Training loss: 0.0016376857044123638\n",
            "Epoch 3 - Training loss: 0.0017357372355137045\n",
            "Epoch 3 - Training loss: 0.0017669138703137826\n",
            "Epoch 3 - Training loss: 0.0018797377263432117\n",
            "Epoch 3 - Training loss: 0.0019988794046551434\n",
            "Epoch 3 - Training loss: 0.002034385809734432\n",
            "Epoch 3 - Training loss: 0.002205078849501447\n",
            "Epoch 3 - Training loss: 0.002257974393395727\n",
            "Epoch 3 - Training loss: 0.002330060531971043\n",
            "Epoch 3 - Training loss: 0.0023838801106918594\n",
            "Epoch 3 - Training loss: 0.0024949128169621995\n",
            "Epoch 3 - Training loss: 0.0025743244728172764\n",
            "Epoch 3 - Training loss: 0.002640455246352946\n",
            "Epoch 3 - Training loss: 0.0027882137310021975\n",
            "Epoch 3 - Training loss: 0.0029158476176165313\n",
            "Epoch 3 - Training loss: 0.0029676359202434766\n",
            "Epoch 3 - Training loss: 0.003049980809312385\n",
            "Epoch 3 - Training loss: 0.003126223799961208\n",
            "Epoch 3 - Training loss: 0.0032415485728397044\n",
            "Epoch 3 - Training loss: 0.003281073394551206\n",
            "Epoch 3 - Training loss: 0.0033444521476083725\n",
            "Epoch 3 - Training loss: 0.003431169844385403\n",
            "Epoch 3 - Training loss: 0.0036784573150341953\n",
            "Epoch 3 - Training loss: 0.003818119417375593\n",
            "Epoch 3 - Training loss: 0.00385041639748921\n",
            "Epoch 3 - Training loss: 0.0038827973415951995\n",
            "Epoch 3 - Training loss: 0.003971626563096987\n",
            "Epoch 3 - Training loss: 0.004036693567676204\n",
            "Epoch 3 - Training loss: 0.004050403489852384\n",
            "Epoch 3 - Training loss: 0.004092705519452913\n",
            "Epoch 3 - Training loss: 0.004221883455493938\n",
            "Epoch 3 - Training loss: 0.004305909626058806\n",
            "Epoch 3 - Training loss: 0.004387013810768183\n",
            "Epoch 3 - Training loss: 0.0044612375749294945\n",
            "Epoch 3 - Training loss: 0.0046458731609970525\n",
            "Epoch 3 - Training loss: 0.0047646997623574504\n",
            "Epoch 3 - Training loss: 0.0048152832949816034\n",
            "Epoch 3 - Training loss: 0.004924355918712326\n",
            "Epoch 3 - Training loss: 0.004984654913118272\n",
            "Epoch 3 - Training loss: 0.0050099948877846\n",
            "Epoch 3 - Training loss: 0.005126746232385066\n",
            "Epoch 3 - Training loss: 0.005364176664334625\n",
            "Epoch 3 - Training loss: 0.005503576741353281\n",
            "Epoch 3 - Training loss: 0.005690784223361819\n",
            "Epoch 3 - Training loss: 0.005771805784468458\n",
            "Epoch 3 - Training loss: 0.005840218159308566\n",
            "Epoch 3 - Training loss: 0.005951549802253496\n",
            "Epoch 3 - Training loss: 0.006081942333849763\n",
            "Epoch 3 - Training loss: 0.006198188365458933\n",
            "Epoch 3 - Training loss: 0.00625365919300488\n",
            "Epoch 3 - Training loss: 0.006335714669115762\n",
            "Epoch 3 - Training loss: 0.00638821975254555\n",
            "Epoch 3 - Training loss: 0.006432814253514001\n",
            "Epoch 3 - Training loss: 0.0066049155642165305\n",
            "Epoch 3 - Training loss: 0.006717089853529483\n",
            "Epoch 3 - Training loss: 0.006854151480837163\n",
            "Epoch 3 - Training loss: 0.0071138644865009065\n",
            "Epoch 3 - Training loss: 0.00716346283870211\n",
            "Epoch 3 - Training loss: 0.007270825387381796\n",
            "Epoch 3 - Training loss: 0.007299485395966308\n",
            "Epoch 3 - Training loss: 0.00742421531950487\n",
            "Epoch 3 - Training loss: 0.007492961199171762\n",
            "Epoch 3 - Training loss: 0.007575094946094159\n",
            "Epoch 3 - Training loss: 0.007615063244155221\n",
            "Epoch 3 - Training loss: 0.007635981142362043\n",
            "Epoch 3 - Training loss: 0.007771355793007147\n",
            "Epoch 3 - Training loss: 0.00802099315135845\n",
            "Epoch 3 - Training loss: 0.008112762365609344\n",
            "Epoch 3 - Training loss: 0.008197335081536378\n",
            "Epoch 3 - Training loss: 0.008234628374928605\n",
            "Epoch 3 - Training loss: 0.008358053724045184\n",
            "Epoch 3 - Training loss: 0.008456601314484946\n",
            "Epoch 3 - Training loss: 0.00860653930842114\n",
            "Epoch 3 - Training loss: 0.008760344010712242\n",
            "Epoch 3 - Training loss: 0.008878678150935721\n",
            "Epoch 3 - Training loss: 0.009002892125739471\n",
            "Epoch 3 - Training loss: 0.009209151917905696\n",
            "Epoch 3 - Training loss: 0.009380214357140985\n",
            "Epoch 3 - Training loss: 0.009519441275676685\n",
            "Epoch 3 - Training loss: 0.009598888579938712\n",
            "Epoch 3 - Training loss: 0.009690343590973536\n",
            "Epoch 3 - Training loss: 0.009727533739894184\n",
            "Epoch 3 - Training loss: 0.00993894045350394\n",
            "Epoch 3 - Training loss: 0.009973281783971197\n",
            "Epoch 3 - Training loss: 0.010077334638597614\n",
            "Epoch 3 - Training loss: 0.01015975089596787\n",
            "Epoch 3 - Training loss: 0.010247782484364153\n",
            "Epoch 3 - Training loss: 0.0105563201137316\n",
            "Epoch 3 - Training loss: 0.010628142519228494\n",
            "Epoch 3 - Training loss: 0.01081445888637988\n",
            "Epoch 3 - Training loss: 0.011006565228390542\n",
            "Epoch 3 - Training loss: 0.01114145740231217\n",
            "Epoch 3 - Training loss: 0.011167351827660857\n",
            "Epoch 3 - Training loss: 0.011190009603241105\n",
            "Epoch 3 - Training loss: 0.011380183099429491\n",
            "Epoch 3 - Training loss: 0.011417925917803606\n",
            "Epoch 3 - Training loss: 0.011533592909034381\n",
            "Epoch 3 - Training loss: 0.01165011851216303\n",
            "Epoch 3 - Training loss: 0.011812521645159864\n",
            "Epoch 3 - Training loss: 0.011962373647640254\n",
            "Epoch 3 - Training loss: 0.012114405763079362\n",
            "Epoch 3 - Training loss: 0.012186270377942239\n",
            "Epoch 3 - Training loss: 0.012715158435994628\n",
            "Epoch 3 - Training loss: 0.012792963602109504\n",
            "Epoch 3 - Training loss: 0.012922626667852596\n",
            "Epoch 3 - Training loss: 0.01296596474913773\n",
            "Epoch 3 - Training loss: 0.012993560420837737\n",
            "Epoch 3 - Training loss: 0.01304394291487457\n",
            "Epoch 3 - Training loss: 0.013174132334747549\n",
            "Epoch 3 - Training loss: 0.013182831731543485\n",
            "Epoch 3 - Training loss: 0.013282478663887678\n",
            "Epoch 3 - Training loss: 0.013406734220556486\n",
            "Epoch 3 - Training loss: 0.013491048906514766\n",
            "Epoch 3 - Training loss: 0.013632196470348438\n",
            "Epoch 3 - Training loss: 0.013800672890900421\n",
            "Epoch 3 - Training loss: 0.01390055275317639\n",
            "Epoch 3 - Training loss: 0.013957933197095831\n",
            "Epoch 3 - Training loss: 0.013990654054083932\n",
            "Epoch 3 - Training loss: 0.014064971244395541\n",
            "Epoch 3 - Training loss: 0.014124979173689127\n",
            "Epoch 3 - Training loss: 0.014252116937420643\n",
            "Epoch 3 - Training loss: 0.014413775218877076\n",
            "Epoch 3 - Training loss: 0.01450093630463012\n",
            "Epoch 3 - Training loss: 0.014663346872321452\n",
            "Epoch 3 - Training loss: 0.014705680161793985\n",
            "Epoch 3 - Training loss: 0.014858694228806348\n",
            "Epoch 3 - Training loss: 0.015048280129951836\n",
            "Epoch 3 - Training loss: 0.015133929415059878\n",
            "Epoch 3 - Training loss: 0.015242792016614094\n",
            "Epoch 3 - Training loss: 0.015333355730101625\n",
            "Epoch 3 - Training loss: 0.015539781182511911\n",
            "Epoch 3 - Training loss: 0.015693283985370895\n",
            "Epoch 3 - Training loss: 0.015743879126762148\n",
            "Epoch 3 - Training loss: 0.01582015488249089\n",
            "Epoch 3 - Training loss: 0.015870255465780112\n",
            "Epoch 3 - Training loss: 0.016025038577044314\n",
            "Epoch 3 - Training loss: 0.016176322822584147\n",
            "Epoch 3 - Training loss: 0.016341813276015492\n",
            "Epoch 3 - Training loss: 0.016421309620666225\n",
            "Epoch 3 - Training loss: 0.01649638754861759\n",
            "Epoch 3 - Training loss: 0.01655556667627874\n",
            "Epoch 3 - Training loss: 0.01662075185755105\n",
            "Epoch 3 - Training loss: 0.01669557281990232\n",
            "Epoch 3 - Training loss: 0.01671298514050779\n",
            "Epoch 3 - Training loss: 0.01680952661049201\n",
            "Epoch 3 - Training loss: 0.01686067696311263\n",
            "Epoch 3 - Training loss: 0.016949264644416793\n",
            "Epoch 3 - Training loss: 0.016978213412643495\n",
            "Epoch 3 - Training loss: 0.017082549913573875\n",
            "Epoch 3 - Training loss: 0.01721609383424335\n",
            "Epoch 3 - Training loss: 0.017297784120702285\n",
            "Epoch 3 - Training loss: 0.017385791228619465\n",
            "Epoch 3 - Training loss: 0.01756252834140492\n",
            "Epoch 3 - Training loss: 0.01766622295654786\n",
            "Epoch 3 - Training loss: 0.017690858331078023\n",
            "Epoch 3 - Training loss: 0.017783359695932886\n",
            "Epoch 3 - Training loss: 0.01789807755627167\n",
            "Epoch 3 - Training loss: 0.01798131282943716\n",
            "Epoch 3 - Training loss: 0.01807043225025889\n",
            "Epoch 3 - Training loss: 0.01820463493569637\n",
            "Epoch 3 - Training loss: 0.018347785148872878\n",
            "Epoch 3 - Training loss: 0.018386587089519384\n",
            "Epoch 3 - Training loss: 0.01840354154692657\n",
            "Epoch 3 - Training loss: 0.018444001237784367\n",
            "Epoch 3 - Training loss: 0.01849010201500677\n",
            "Epoch 3 - Training loss: 0.018644564242950128\n",
            "Epoch 3 - Training loss: 0.018774154288237536\n",
            "Epoch 3 - Training loss: 0.01888068326945498\n",
            "Epoch 3 - Training loss: 0.018912568702292975\n",
            "Epoch 3 - Training loss: 0.019053774647541773\n",
            "Epoch 3 - Training loss: 0.019172580930581096\n",
            "Epoch 3 - Training loss: 0.019287992242985823\n",
            "Epoch 3 - Training loss: 0.019521079796638443\n",
            "Epoch 3 - Training loss: 0.019688446779669855\n",
            "Epoch 3 - Training loss: 0.0197463759652047\n",
            "Epoch 3 - Training loss: 0.019908280576517712\n",
            "Epoch 3 - Training loss: 0.019935328134103245\n",
            "Epoch 3 - Training loss: 0.020006229392072156\n",
            "Epoch 3 - Training loss: 0.020200208879943723\n",
            "Epoch 3 - Training loss: 0.02035165205002149\n",
            "Epoch 3 - Training loss: 0.020398876220702744\n",
            "Epoch 3 - Training loss: 0.02042009054756622\n",
            "Epoch 3 - Training loss: 0.02047393550830228\n",
            "Epoch 3 - Training loss: 0.020611403374942636\n",
            "Epoch 3 - Training loss: 0.020723619031658303\n",
            "Epoch 3 - Training loss: 0.020812060870627352\n",
            "Epoch 3 - Training loss: 0.02102602094110014\n",
            "Epoch 3 - Training loss: 0.021161662096948003\n",
            "Epoch 3 - Training loss: 0.02137020958273777\n",
            "Epoch 3 - Training loss: 0.02144269586832666\n",
            "Epoch 3 - Training loss: 0.021648718639890523\n",
            "Epoch 3 - Training loss: 0.021879601250611135\n",
            "Epoch 3 - Training loss: 0.021973440741329814\n",
            "Epoch 3 - Training loss: 0.02200113945027023\n",
            "Epoch 3 - Training loss: 0.02205497378320582\n",
            "Epoch 3 - Training loss: 0.022283721826414565\n",
            "Epoch 3 - Training loss: 0.0223702967865889\n",
            "Epoch 3 - Training loss: 0.022479235919426754\n",
            "Epoch 3 - Training loss: 0.02251350212055864\n",
            "Epoch 3 - Training loss: 0.022643216704921938\n",
            "Epoch 3 - Training loss: 0.022728683704982943\n",
            "Epoch 3 - Training loss: 0.02277706756488855\n",
            "Epoch 3 - Training loss: 0.022905762984490853\n",
            "Epoch 3 - Training loss: 0.02309124380636063\n",
            "Epoch 3 - Training loss: 0.023251215587737465\n",
            "Epoch 3 - Training loss: 0.02327594015477245\n",
            "Epoch 3 - Training loss: 0.023353578175292974\n",
            "Epoch 3 - Training loss: 0.023461875803609773\n",
            "Epoch 3 - Training loss: 0.023661785286420316\n",
            "Epoch 3 - Training loss: 0.02382878693682489\n",
            "Epoch 3 - Training loss: 0.024039727139241025\n",
            "Epoch 3 - Training loss: 0.02405498771350394\n",
            "Epoch 3 - Training loss: 0.024353540865883136\n",
            "Epoch 3 - Training loss: 0.024528524772857805\n",
            "Epoch 3 - Training loss: 0.024664270427467216\n",
            "Epoch 3 - Training loss: 0.024715079264695457\n",
            "Epoch 3 - Training loss: 0.024843345044740735\n",
            "Epoch 3 - Training loss: 0.02495974829313216\n",
            "Epoch 3 - Training loss: 0.025028143550700217\n",
            "Epoch 3 - Training loss: 0.025197742546576935\n",
            "Epoch 3 - Training loss: 0.025275112767177605\n",
            "Epoch 3 - Training loss: 0.025296718616888467\n",
            "Epoch 3 - Training loss: 0.025439866192972484\n",
            "Epoch 3 - Training loss: 0.025519861512060867\n",
            "Epoch 3 - Training loss: 0.025570953221145724\n",
            "Epoch 3 - Training loss: 0.02590353439834072\n",
            "Epoch 3 - Training loss: 0.02595335994162031\n",
            "Epoch 3 - Training loss: 0.026057676815274936\n",
            "Epoch 3 - Training loss: 0.026200104957577516\n",
            "Epoch 3 - Training loss: 0.026310601968691547\n",
            "Epoch 3 - Training loss: 0.026459666799062857\n",
            "Epoch 3 - Training loss: 0.026525706235470296\n",
            "Epoch 3 - Training loss: 0.026614817554381356\n",
            "Epoch 3 - Training loss: 0.026659043124521466\n",
            "Epoch 3 - Training loss: 0.026702851398603747\n",
            "Epoch 3 - Training loss: 0.02682399934034612\n",
            "Epoch 3 - Training loss: 0.02684750233981401\n",
            "Epoch 3 - Training loss: 0.02711440644252783\n",
            "Epoch 3 - Training loss: 0.027249388960696487\n",
            "Epoch 3 - Training loss: 0.027291145838940066\n",
            "Epoch 3 - Training loss: 0.0274000301567921\n",
            "Epoch 3 - Training loss: 0.027447226109789377\n",
            "Epoch 3 - Training loss: 0.027550263183393965\n",
            "Epoch 3 - Training loss: 0.027865175332532507\n",
            "Epoch 3 - Training loss: 0.028048343297197367\n",
            "Epoch 3 - Training loss: 0.028107142913888005\n",
            "Epoch 3 - Training loss: 0.028242041124527387\n",
            "Epoch 3 - Training loss: 0.028364219986744273\n",
            "Epoch 3 - Training loss: 0.028568153863331912\n",
            "Epoch 3 - Training loss: 0.028698458163532368\n",
            "Epoch 3 - Training loss: 0.02899329120448149\n",
            "Epoch 3 - Training loss: 0.029060760223026724\n",
            "Epoch 3 - Training loss: 0.029108573644876734\n",
            "Epoch 3 - Training loss: 0.029427687031850377\n",
            "Epoch 3 - Training loss: 0.029601082027053784\n",
            "Epoch 3 - Training loss: 0.029813935710137077\n",
            "Epoch 3 - Training loss: 0.029870197311965132\n",
            "Epoch 3 - Training loss: 0.02993055082746406\n",
            "Epoch 3 - Training loss: 0.03000075625839518\n",
            "Epoch 3 - Training loss: 0.030035908439202605\n",
            "Epoch 3 - Training loss: 0.030217476784865232\n",
            "Epoch 3 - Training loss: 0.03029752823748568\n",
            "Epoch 3 - Training loss: 0.03033953218428946\n",
            "Epoch 3 - Training loss: 0.030521184138492988\n",
            "Epoch 3 - Training loss: 0.030593558157017746\n",
            "Epoch 3 - Training loss: 0.030722505609586295\n",
            "Epoch 3 - Training loss: 0.030787316109262296\n",
            "Epoch 3 - Training loss: 0.030922630170324462\n",
            "Epoch 3 - Training loss: 0.030944913546286666\n",
            "Epoch 3 - Training loss: 0.031030107130691695\n",
            "Epoch 3 - Training loss: 0.031086199811653795\n",
            "Epoch 3 - Training loss: 0.031146624802685242\n",
            "Epoch 3 - Training loss: 0.03125650825292698\n",
            "Epoch 3 - Training loss: 0.03133127960298997\n",
            "Epoch 3 - Training loss: 0.03142617470118156\n",
            "Epoch 3 - Training loss: 0.031583318645670724\n",
            "Epoch 3 - Training loss: 0.03180342561988307\n",
            "Epoch 3 - Training loss: 0.03196912397469666\n",
            "Epoch 3 - Training loss: 0.03209953141539717\n",
            "Epoch 3 - Training loss: 0.03215372577500242\n",
            "Epoch 3 - Training loss: 0.03220894995894132\n",
            "Epoch 3 - Training loss: 0.03226313902847548\n",
            "Epoch 3 - Training loss: 0.032373087135141594\n",
            "Epoch 3 - Training loss: 0.032649787750516114\n",
            "Epoch 3 - Training loss: 0.03275257866901121\n",
            "Epoch 3 - Training loss: 0.03277709812863168\n",
            "Epoch 3 - Training loss: 0.03287882822107023\n",
            "Epoch 3 - Training loss: 0.033028326678409506\n",
            "Epoch 3 - Training loss: 0.03324613855607601\n",
            "Epoch 3 - Training loss: 0.033484917590771913\n",
            "Epoch 3 - Training loss: 0.03354517558712695\n",
            "Epoch 3 - Training loss: 0.03360271850215601\n",
            "Epoch 3 - Training loss: 0.033669561036487126\n",
            "Epoch 3 - Training loss: 0.03369306896858886\n",
            "Epoch 3 - Training loss: 0.03386293340529969\n",
            "Epoch 3 - Training loss: 0.033911620125746424\n",
            "Epoch 3 - Training loss: 0.034002917117750976\n",
            "Epoch 3 - Training loss: 0.034081696602168365\n",
            "Epoch 3 - Training loss: 0.034259102074925836\n",
            "Epoch 3 - Training loss: 0.03439458055711631\n",
            "Epoch 3 - Training loss: 0.03444286557371174\n",
            "Epoch 3 - Training loss: 0.03464877475966523\n",
            "Epoch 3 - Training loss: 0.03481751970295459\n",
            "Epoch 3 - Training loss: 0.03488590360990465\n",
            "Epoch 3 - Training loss: 0.03494364227742147\n",
            "Epoch 3 - Training loss: 0.03500635457683855\n",
            "Epoch 3 - Training loss: 0.03513714374462043\n",
            "Epoch 3 - Training loss: 0.03529943090909198\n",
            "Epoch 3 - Training loss: 0.03541556996370811\n",
            "Epoch 3 - Training loss: 0.03551017392529989\n",
            "Epoch 3 - Training loss: 0.03555848569408663\n",
            "Epoch 3 - Training loss: 0.03557792195736536\n",
            "Epoch 3 - Training loss: 0.03569922823585999\n",
            "Epoch 3 - Training loss: 0.03616536632855373\n",
            "Epoch 3 - Training loss: 0.036384205639695945\n",
            "Epoch 3 - Training loss: 0.03642368879216884\n",
            "Epoch 3 - Training loss: 0.03662843475026934\n",
            "Epoch 3 - Training loss: 0.0366928769267603\n",
            "Epoch 3 - Training loss: 0.03689237966188299\n",
            "Epoch 3 - Training loss: 0.03696197135719473\n",
            "Epoch 3 - Training loss: 0.037015939729093617\n",
            "Epoch 3 - Training loss: 0.03708352214261604\n",
            "Epoch 3 - Training loss: 0.03717451345429682\n",
            "Epoch 3 - Training loss: 0.037392704564927104\n",
            "Epoch 3 - Training loss: 0.03746189967965457\n",
            "Epoch 3 - Training loss: 0.03751036165945375\n",
            "Epoch 3 - Training loss: 0.03765111628061991\n",
            "Epoch 3 - Training loss: 0.03787632135829247\n",
            "Epoch 3 - Training loss: 0.03797597864043039\n",
            "Epoch 3 - Training loss: 0.03805300787385148\n",
            "Epoch 3 - Training loss: 0.03815755037777523\n",
            "Epoch 3 - Training loss: 0.0382509094649064\n",
            "Epoch 3 - Training loss: 0.03831189158541371\n",
            "Epoch 3 - Training loss: 0.03834654077657187\n",
            "Epoch 3 - Training loss: 0.038598237545298994\n",
            "Epoch 3 - Training loss: 0.0388198818808902\n",
            "Epoch 3 - Training loss: 0.0388918924175187\n",
            "Epoch 3 - Training loss: 0.038952829079952705\n",
            "Epoch 3 - Training loss: 0.03897092361122306\n",
            "Epoch 3 - Training loss: 0.0389987871583019\n",
            "Epoch 3 - Training loss: 0.03907330326640657\n",
            "Epoch 3 - Training loss: 0.03930823303568465\n",
            "Epoch 3 - Training loss: 0.0393513556617473\n",
            "Epoch 3 - Training loss: 0.039529578267797225\n",
            "Epoch 3 - Training loss: 0.039642001071305416\n",
            "Epoch 3 - Training loss: 0.03968773433950537\n",
            "Epoch 3 - Training loss: 0.03980454088035804\n",
            "Epoch 3 - Training loss: 0.03983695011562122\n",
            "Epoch 3 - Training loss: 0.03988621690109976\n",
            "Epoch 3 - Training loss: 0.039920747466782515\n",
            "Epoch 3 - Training loss: 0.04002042022198121\n",
            "Epoch 3 - Training loss: 0.04009379238795751\n",
            "Epoch 3 - Training loss: 0.04028201718399647\n",
            "Epoch 3 - Training loss: 0.04044262356738419\n",
            "Epoch 3 - Training loss: 0.04048610483405433\n",
            "Epoch 3 - Training loss: 0.04051869267673253\n",
            "Epoch 3 - Training loss: 0.040734130360964516\n",
            "Epoch 3 - Training loss: 0.040918266916993074\n",
            "Epoch 3 - Training loss: 0.04094615114578751\n",
            "Epoch 3 - Training loss: 0.04105241621917944\n",
            "Epoch 3 - Training loss: 0.041165666926755454\n",
            "Epoch 3 - Training loss: 0.041237459180435775\n",
            "Epoch 3 - Training loss: 0.041321533954521615\n",
            "Epoch 3 - Training loss: 0.04134353060029082\n",
            "Epoch 3 - Training loss: 0.041491054224847224\n",
            "Epoch 3 - Training loss: 0.04157861687941973\n",
            "Epoch 3 - Training loss: 0.04169145971933789\n",
            "Epoch 3 - Training loss: 0.04181865688119489\n",
            "Epoch 3 - Training loss: 0.04196776231643615\n",
            "Epoch 3 - Training loss: 0.042089903433280965\n",
            "Epoch 3 - Training loss: 0.04212349657215543\n",
            "Epoch 3 - Training loss: 0.04219131591096362\n",
            "Epoch 3 - Training loss: 0.04255484934173413\n",
            "Epoch 3 - Training loss: 0.042838355689160605\n",
            "Epoch 3 - Training loss: 0.0429536263063264\n",
            "Epoch 3 - Training loss: 0.0430604096239945\n",
            "Epoch 3 - Training loss: 0.04322956571542124\n",
            "Epoch 3 - Training loss: 0.043388823368973824\n",
            "Epoch 3 - Training loss: 0.043462508681740585\n",
            "Epoch 3 - Training loss: 0.043525675776353016\n",
            "Epoch 3 - Training loss: 0.04371982773960526\n",
            "Epoch 3 - Training loss: 0.043830123616815375\n",
            "Epoch 3 - Training loss: 0.04391437559239646\n",
            "Epoch 3 - Training loss: 0.04403368770472531\n",
            "Epoch 3 - Training loss: 0.04419997239957994\n",
            "Epoch 3 - Training loss: 0.04432283920932935\n",
            "Epoch 3 - Training loss: 0.04445559649801712\n",
            "Epoch 3 - Training loss: 0.04463269745013607\n",
            "Epoch 3 - Training loss: 0.04475458941734167\n",
            "Epoch 3 - Training loss: 0.04480726929155113\n",
            "Epoch 3 - Training loss: 0.04507759500588816\n",
            "Epoch 3 - Training loss: 0.04510764422407473\n",
            "Epoch 3 - Training loss: 0.04525263860686692\n",
            "Epoch 3 - Training loss: 0.04534168713597029\n",
            "Epoch 3 - Training loss: 0.045491378641188907\n",
            "Epoch 3 - Training loss: 0.04556425972017589\n",
            "Epoch 3 - Training loss: 0.045680852354700756\n",
            "Epoch 3 - Training loss: 0.04584085894252128\n",
            "Epoch 3 - Training loss: 0.046060137679057714\n",
            "Epoch 3 - Training loss: 0.046177251107577706\n",
            "Epoch 3 - Training loss: 0.04629940293165349\n",
            "Epoch 3 - Training loss: 0.04635829057719217\n",
            "Epoch 3 - Training loss: 0.04640863137418972\n",
            "Epoch 3 - Training loss: 0.04648502010391402\n",
            "Epoch 3 - Training loss: 0.04654403814255619\n",
            "Epoch 3 - Training loss: 0.04659226802644381\n",
            "Epoch 3 - Training loss: 0.04674701560050376\n",
            "Epoch 3 - Training loss: 0.04683531527476969\n",
            "Epoch 3 - Training loss: 0.04686446906502313\n",
            "Epoch 3 - Training loss: 0.04696546811888467\n",
            "Epoch 3 - Training loss: 0.04701154630607379\n",
            "Epoch 3 - Training loss: 0.04721615599321404\n",
            "Epoch 3 - Training loss: 0.04723288278295986\n",
            "Epoch 3 - Training loss: 0.04733135463642096\n",
            "Epoch 3 - Training loss: 0.047423731843466314\n",
            "Epoch 3 - Training loss: 0.04745008314906089\n",
            "Epoch 3 - Training loss: 0.04761450548471609\n",
            "Epoch 3 - Training loss: 0.047726247203486685\n",
            "Epoch 3 - Training loss: 0.047953346730279386\n",
            "Epoch 3 - Training loss: 0.047999019325494385\n",
            "Epoch 3 - Training loss: 0.04807047000420945\n",
            "Epoch 3 - Training loss: 0.048150043968341626\n",
            "Epoch 3 - Training loss: 0.04836998777642751\n",
            "Epoch 3 - Training loss: 0.04842289125145689\n",
            "Epoch 3 - Training loss: 0.048552529217163\n",
            "Epoch 3 - Training loss: 0.048729348762918\n",
            "Epoch 3 - Training loss: 0.04883867746262726\n",
            "Epoch 3 - Training loss: 0.04888131435928759\n",
            "Epoch 3 - Training loss: 0.04891168641875675\n",
            "Epoch 3 - Training loss: 0.0490027349839396\n",
            "Epoch 3 - Training loss: 0.04905600973696851\n",
            "Epoch 3 - Training loss: 0.0491779819845772\n",
            "Epoch 3 - Training loss: 0.04924482309710242\n",
            "Epoch 3 - Training loss: 0.049253639979546134\n",
            "Epoch 3 - Training loss: 0.04927765736495381\n",
            "Epoch 3 - Training loss: 0.04938153488470166\n",
            "Epoch 3 - Training loss: 0.04945553180330725\n",
            "Epoch 3 - Training loss: 0.049538294607991855\n",
            "Epoch 3 - Training loss: 0.04963312462083439\n",
            "Epoch 3 - Training loss: 0.04964392098671656\n",
            "Epoch 3 - Training loss: 0.049796304518559466\n",
            "Epoch 3 - Training loss: 0.04994818633561259\n",
            "Epoch 3 - Training loss: 0.050032376839932224\n",
            "Epoch 3 - Training loss: 0.050118640972686604\n",
            "Epoch 3 - Training loss: 0.05018618560112171\n",
            "Epoch 3 - Training loss: 0.05039379801521741\n",
            "Epoch 3 - Training loss: 0.050461417244775084\n",
            "Epoch 3 - Training loss: 0.05050330921046452\n",
            "Epoch 3 - Training loss: 0.05059377774238777\n",
            "Epoch 3 - Training loss: 0.05065929213526851\n",
            "Epoch 3 - Training loss: 0.05074060041663934\n",
            "Epoch 3 - Training loss: 0.05094748983846735\n",
            "Epoch 3 - Training loss: 0.0510521921247784\n",
            "Epoch 3 - Training loss: 0.051189780995837535\n",
            "Epoch 3 - Training loss: 0.05134834715925745\n",
            "Epoch 3 - Training loss: 0.05143601354211569\n",
            "Epoch 3 - Training loss: 0.051579789476973545\n",
            "Epoch 3 - Training loss: 0.051753435059508156\n",
            "Epoch 3 - Training loss: 0.051842149678689206\n",
            "Epoch 3 - Training loss: 0.05200949303909088\n",
            "Epoch 3 - Training loss: 0.05216586542750663\n",
            "Epoch 3 - Training loss: 0.05227131483707029\n",
            "Epoch 3 - Training loss: 0.05234271136801571\n",
            "Epoch 3 - Training loss: 0.05238293955471915\n",
            "Epoch 3 - Training loss: 0.05245217165228591\n",
            "Epoch 3 - Training loss: 0.05252589015905727\n",
            "Epoch 3 - Training loss: 0.05256118355537338\n",
            "Epoch 3 - Training loss: 0.05275925619206004\n",
            "Epoch 3 - Training loss: 0.05295175812971681\n",
            "Epoch 3 - Training loss: 0.05307265400672073\n",
            "Epoch 3 - Training loss: 0.05321389002435561\n",
            "Epoch 3 - Training loss: 0.05340504766439896\n",
            "Epoch 3 - Training loss: 0.053471108157831085\n",
            "Epoch 3 - Training loss: 0.05361740375513525\n",
            "Epoch 3 - Training loss: 0.05372000504126236\n",
            "Epoch 3 - Training loss: 0.05380502044717704\n",
            "Epoch 3 - Training loss: 0.053844499250830236\n",
            "Epoch 3 - Training loss: 0.05389481725921826\n",
            "Epoch 3 - Training loss: 0.05414302334356219\n",
            "Epoch 3 - Training loss: 0.05424631144930877\n",
            "Epoch 3 - Training loss: 0.054296520761033494\n",
            "Epoch 3 - Training loss: 0.05440543368975046\n",
            "Epoch 3 - Training loss: 0.05444578204288094\n",
            "Epoch 3 - Training loss: 0.05448578036566978\n",
            "Epoch 3 - Training loss: 0.05463640630713849\n",
            "Epoch 3 - Training loss: 0.05475408677408881\n",
            "Epoch 3 - Training loss: 0.05483459882033087\n",
            "Epoch 3 - Training loss: 0.054967844884580515\n",
            "Epoch 3 - Training loss: 0.05501207653512515\n",
            "Epoch 3 - Training loss: 0.05506153914855042\n",
            "Epoch 3 - Training loss: 0.05509033418103639\n",
            "Epoch 3 - Training loss: 0.055165154404684044\n",
            "Epoch 3 - Training loss: 0.05530189673728081\n",
            "Epoch 3 - Training loss: 0.055542601786776266\n",
            "Epoch 3 - Training loss: 0.055621068148232346\n",
            "Epoch 3 - Training loss: 0.05579859275084886\n",
            "Epoch 3 - Training loss: 0.05594051631886377\n",
            "Epoch 3 - Training loss: 0.055983435892776004\n",
            "Epoch 3 - Training loss: 0.056197975853533505\n",
            "Epoch 3 - Training loss: 0.05642450011289641\n",
            "Epoch 3 - Training loss: 0.05649867393116135\n",
            "Epoch 3 - Training loss: 0.056694351468306745\n",
            "Epoch 3 - Training loss: 0.05684459301382939\n",
            "Epoch 3 - Training loss: 0.05692281083726108\n",
            "Epoch 3 - Training loss: 0.0570024278844947\n",
            "Epoch 3 - Training loss: 0.05711155260668825\n",
            "Epoch 3 - Training loss: 0.05718781690059631\n",
            "Epoch 3 - Training loss: 0.0574929781301793\n",
            "Epoch 3 - Training loss: 0.057796029812856906\n",
            "Epoch 3 - Training loss: 0.057886990384935444\n",
            "Epoch 3 - Training loss: 0.057922806171799644\n",
            "Epoch 3 - Training loss: 0.057976010199914225\n",
            "Epoch 3 - Training loss: 0.058051953197462855\n",
            "Epoch 3 - Training loss: 0.05813066692931502\n",
            "Epoch 3 - Training loss: 0.058374953579340284\n",
            "Epoch 3 - Training loss: 0.05846006786828038\n",
            "Epoch 3 - Training loss: 0.05858064419639581\n",
            "Epoch 3 - Training loss: 0.05878074556740045\n",
            "Epoch 3 - Training loss: 0.05905045950805137\n",
            "Epoch 3 - Training loss: 0.05909239908835209\n",
            "Epoch 3 - Training loss: 0.059289780632058565\n",
            "Epoch 3 - Training loss: 0.05952793866324463\n",
            "Epoch 3 - Training loss: 0.05968037539763428\n",
            "Epoch 3 - Training loss: 0.0597595106035884\n",
            "Epoch 3 - Training loss: 0.05989752102817999\n",
            "Epoch 3 - Training loss: 0.059978027212451386\n",
            "Epoch 3 - Training loss: 0.0601237412196582\n",
            "Epoch 3 - Training loss: 0.06032146239228277\n",
            "Epoch 3 - Training loss: 0.06054755910905376\n",
            "Epoch 3 - Training loss: 0.060723524935829484\n",
            "Epoch 3 - Training loss: 0.06086387813115107\n",
            "Epoch 3 - Training loss: 0.06099909695504761\n",
            "Epoch 3 - Training loss: 0.06104356166261282\n",
            "Epoch 3 - Training loss: 0.06120432025071845\n",
            "Epoch 3 - Training loss: 0.061370243949851376\n",
            "Epoch 3 - Training loss: 0.061492057757448156\n",
            "Epoch 3 - Training loss: 0.061589215997296734\n",
            "Epoch 3 - Training loss: 0.061653321551313915\n",
            "Epoch 3 - Training loss: 0.061736789301657345\n",
            "Epoch 3 - Training loss: 0.06191969717910358\n",
            "Epoch 3 - Training loss: 0.06217193459945002\n",
            "Epoch 3 - Training loss: 0.06226502710194794\n",
            "Epoch 3 - Training loss: 0.06229885413186319\n",
            "Epoch 3 - Training loss: 0.06250788261696918\n",
            "Epoch 3 - Training loss: 0.06254889897660597\n",
            "Epoch 3 - Training loss: 0.06269644117201252\n",
            "Epoch 3 - Training loss: 0.0627241146041831\n",
            "Epoch 3 - Training loss: 0.06279158213737804\n",
            "Epoch 3 - Training loss: 0.06299757192939964\n",
            "Epoch 3 - Training loss: 0.06304215783201682\n",
            "Epoch 3 - Training loss: 0.06308777195844315\n",
            "Epoch 3 - Training loss: 0.06312286127398391\n",
            "Epoch 3 - Training loss: 0.06321913644528465\n",
            "Epoch 3 - Training loss: 0.06328760749543272\n",
            "Epoch 3 - Training loss: 0.06339566203465721\n",
            "Epoch 3 - Training loss: 0.06346669833241368\n",
            "Epoch 3 - Training loss: 0.06373938020946247\n",
            "Epoch 3 - Training loss: 0.0639474913676474\n",
            "Epoch 3 - Training loss: 0.06399938727079678\n",
            "Epoch 3 - Training loss: 0.06420778558611362\n",
            "Epoch 3 - Training loss: 0.06428161968014387\n",
            "Epoch 3 - Training loss: 0.06437779488816445\n",
            "Epoch 3 - Training loss: 0.06449921783417273\n",
            "Epoch 3 - Training loss: 0.06460576523532237\n",
            "Epoch 3 - Training loss: 0.06463652566821972\n",
            "Epoch 3 - Training loss: 0.06475168666335693\n",
            "Epoch 3 - Training loss: 0.0648062378644689\n",
            "Epoch 3 - Training loss: 0.06488406737603104\n",
            "Epoch 3 - Training loss: 0.0649476086756568\n",
            "Epoch 3 - Training loss: 0.06500134345040774\n",
            "Epoch 3 - Training loss: 0.06519410960566896\n",
            "Epoch 3 - Training loss: 0.06535451161438849\n",
            "Epoch 3 - Training loss: 0.06559049968383332\n",
            "Epoch 3 - Training loss: 0.06567349015220778\n",
            "Epoch 3 - Training loss: 0.06577210757794029\n",
            "Epoch 3 - Training loss: 0.06590698880633947\n",
            "Epoch 3 - Training loss: 0.0659650029269045\n",
            "Epoch 3 - Training loss: 0.066296446055714\n",
            "Epoch 3 - Training loss: 0.06644070461027023\n",
            "Epoch 3 - Training loss: 0.06653122447018049\n",
            "Epoch 3 - Training loss: 0.06659166338537802\n",
            "Epoch 3 - Training loss: 0.06688107805910395\n",
            "Epoch 3 - Training loss: 0.06695155724724218\n",
            "Epoch 3 - Training loss: 0.0670013833227061\n",
            "Epoch 3 - Training loss: 0.06719375321510504\n",
            "Epoch 3 - Training loss: 0.0673018120197472\n",
            "Epoch 3 - Training loss: 0.06740022190947777\n",
            "Epoch 3 - Training loss: 0.06746399941157176\n",
            "Epoch 3 - Training loss: 0.06757567540160629\n",
            "Epoch 3 - Training loss: 0.06768413908731963\n",
            "Epoch 3 - Training loss: 0.06784093533275223\n",
            "Epoch 3 - Training loss: 0.0680608110530163\n",
            "Epoch 3 - Training loss: 0.06810096697385377\n",
            "Epoch 3 - Training loss: 0.06816214171331575\n",
            "Epoch 3 - Training loss: 0.06829974048141478\n",
            "Epoch 3 - Training loss: 0.06847591970616312\n",
            "Epoch 3 - Training loss: 0.06851377363191612\n",
            "Epoch 3 - Training loss: 0.06854367841368736\n",
            "Epoch 3 - Training loss: 0.06871246159481786\n",
            "Epoch 3 - Training loss: 0.06877332467283966\n",
            "Epoch 3 - Training loss: 0.06880338892721927\n",
            "Epoch 3 - Training loss: 0.06888725797631848\n",
            "Epoch 3 - Training loss: 0.0690099424613056\n",
            "Epoch 3 - Training loss: 0.06913803235045883\n",
            "Epoch 3 - Training loss: 0.0693793637094213\n",
            "Epoch 3 - Training loss: 0.06944844048859468\n",
            "Epoch 3 - Training loss: 0.0695588578547496\n",
            "Epoch 3 - Training loss: 0.06967431229791407\n",
            "Epoch 3 - Training loss: 0.06986653217787682\n",
            "Epoch 3 - Training loss: 0.06999445061630277\n",
            "Epoch 3 - Training loss: 0.07011425204432087\n",
            "Epoch 3 - Training loss: 0.07020645472667873\n",
            "Epoch 3 - Training loss: 0.07035186109957156\n",
            "Epoch 3 - Training loss: 0.07054718685493286\n",
            "Epoch 3 - Training loss: 0.07059944813439586\n",
            "Epoch 3 - Training loss: 0.07076888132705363\n",
            "Epoch 3 - Training loss: 0.07087668400805897\n",
            "Epoch 3 - Training loss: 0.07098705311224404\n",
            "Epoch 3 - Training loss: 0.07105248228811634\n",
            "Epoch 3 - Training loss: 0.07113085392870501\n",
            "Epoch 3 - Training loss: 0.07126239438189753\n",
            "Epoch 3 - Training loss: 0.07144660955823179\n",
            "Epoch 3 - Training loss: 0.07159746354465672\n",
            "Epoch 3 - Training loss: 0.07174826115131505\n",
            "Epoch 3 - Training loss: 0.07181604298304266\n",
            "Epoch 3 - Training loss: 0.07186930087694862\n",
            "Epoch 3 - Training loss: 0.07196725502626093\n",
            "Epoch 3 - Training loss: 0.0721048330312281\n",
            "Epoch 3 - Training loss: 0.07229690494806146\n",
            "Epoch 3 - Training loss: 0.07243572737473541\n",
            "Epoch 3 - Training loss: 0.07254326936485035\n",
            "Epoch 3 - Training loss: 0.07270370664071045\n",
            "Epoch 3 - Training loss: 0.07276943810125276\n",
            "Epoch 3 - Training loss: 0.07280127852479977\n",
            "Epoch 3 - Training loss: 0.07290973731759451\n",
            "Epoch 3 - Training loss: 0.07303908032609392\n",
            "Epoch 3 - Training loss: 0.07315268881245653\n",
            "Epoch 3 - Training loss: 0.07329474367908259\n",
            "Epoch 3 - Training loss: 0.07334824764469602\n",
            "Epoch 3 - Training loss: 0.07344744201384183\n",
            "Epoch 3 - Training loss: 0.07349134781268804\n",
            "Epoch 3 - Training loss: 0.07354839304005349\n",
            "Epoch 3 - Training loss: 0.07358679539764296\n",
            "Epoch 3 - Training loss: 0.07377704529008314\n",
            "Epoch 3 - Training loss: 0.07394794769275354\n",
            "Epoch 3 - Training loss: 0.07402853356964235\n",
            "Epoch 3 - Training loss: 0.07410436816839204\n",
            "Epoch 3 - Training loss: 0.07428365716063329\n",
            "Epoch 3 - Training loss: 0.07441017297563204\n",
            "Epoch 3 - Training loss: 0.07452352521921209\n",
            "Epoch 3 - Training loss: 0.07468844486920755\n",
            "Epoch 3 - Training loss: 0.07477653041474029\n",
            "Epoch 3 - Training loss: 0.07488733813412853\n",
            "Epoch 3 - Training loss: 0.0749222013309026\n",
            "Epoch 3 - Training loss: 0.07503478676835293\n",
            "Epoch 3 - Training loss: 0.07514515614260171\n",
            "Epoch 3 - Training loss: 0.07529789493528447\n",
            "Epoch 3 - Training loss: 0.07544580834117462\n",
            "Epoch 3 - Training loss: 0.07551033692613149\n",
            "Epoch 3 - Training loss: 0.07563773364916856\n",
            "Epoch 3 - Training loss: 0.07573205062043248\n",
            "Epoch 3 - Training loss: 0.07581116458667177\n",
            "Epoch 3 - Training loss: 0.07588628515291379\n",
            "Epoch 3 - Training loss: 0.07593931528384179\n",
            "Epoch 3 - Training loss: 0.07601578844818416\n",
            "Epoch 3 - Training loss: 0.07604435426609985\n",
            "Epoch 3 - Training loss: 0.07617667439316254\n",
            "Epoch 3 - Training loss: 0.07643247574869631\n",
            "Epoch 3 - Training loss: 0.07655409395075174\n",
            "Epoch 3 - Training loss: 0.07659488917588553\n",
            "Epoch 3 - Training loss: 0.07670448100500142\n",
            "Epoch 3 - Training loss: 0.07678579812698653\n",
            "Epoch 3 - Training loss: 0.07684965266871935\n",
            "Epoch 3 - Training loss: 0.07687721741018391\n",
            "Epoch 3 - Training loss: 0.07701628928038992\n",
            "Epoch 3 - Training loss: 0.07711598394251963\n",
            "Epoch 3 - Training loss: 0.07722504584313329\n",
            "Epoch 3 - Training loss: 0.0774413627117618\n",
            "Epoch 3 - Training loss: 0.07766230113264214\n",
            "Epoch 3 - Training loss: 0.07769520089292387\n",
            "Epoch 3 - Training loss: 0.0777323979800984\n",
            "Epoch 3 - Training loss: 0.0778370867949972\n",
            "Epoch 3 - Training loss: 0.07793013511825281\n",
            "Epoch 3 - Training loss: 0.07805889341503636\n",
            "Epoch 3 - Training loss: 0.07824871543127654\n",
            "Epoch 3 - Training loss: 0.07837627104707937\n",
            "Epoch 3 - Training loss: 0.07842941908066524\n",
            "Epoch 3 - Training loss: 0.07856473689084686\n",
            "Epoch 3 - Training loss: 0.07868979143093961\n",
            "Epoch 3 - Training loss: 0.07880639184965317\n",
            "Epoch 3 - Training loss: 0.07886217516074494\n",
            "Epoch 3 - Training loss: 0.07901323015398498\n",
            "Epoch 3 - Training loss: 0.07908356906786593\n",
            "Epoch 3 - Training loss: 0.07915735698894842\n",
            "Epoch 3 - Training loss: 0.07952394445877531\n",
            "Epoch 3 - Training loss: 0.07962115915782099\n",
            "Epoch 3 - Training loss: 0.07979359661838584\n",
            "Epoch 3 - Training loss: 0.07986877571696094\n",
            "Epoch 3 - Training loss: 0.07997128656352444\n",
            "Epoch 3 - Training loss: 0.08001670276106738\n",
            "Epoch 3 - Training loss: 0.0800709980887486\n",
            "Epoch 3 - Training loss: 0.08008820926353558\n",
            "Epoch 3 - Training loss: 0.08016279859067217\n",
            "Epoch 3 - Training loss: 0.08023163626022113\n",
            "Epoch 3 - Training loss: 0.08026604739818047\n",
            "Epoch 3 - Training loss: 0.08047932997417412\n",
            "Epoch 3 - Training loss: 0.08058265404604963\n",
            "Epoch 3 - Training loss: 0.08067442713650877\n",
            "Epoch 3 - Training loss: 0.08085200937389374\n",
            "Epoch 3 - Training loss: 0.08103634096356406\n",
            "Epoch 3 - Training loss: 0.08126018347460896\n",
            "Epoch 3 - Training loss: 0.08138350656490399\n",
            "Epoch 3 - Training loss: 0.08151610770912122\n",
            "Epoch 3 - Training loss: 0.08163029020791178\n",
            "Epoch 3 - Training loss: 0.08165457478956754\n",
            "Epoch 3 - Training loss: 0.0817661258274876\n",
            "Epoch 3 - Training loss: 0.08209284230161196\n",
            "Epoch 3 - Training loss: 0.08231555912365664\n",
            "Epoch 3 - Training loss: 0.08237636546844612\n",
            "Epoch 3 - Training loss: 0.08257103508024581\n",
            "Epoch 3 - Training loss: 0.08287710064192062\n",
            "Epoch 3 - Training loss: 0.08293648794897075\n",
            "Epoch 3 - Training loss: 0.08300277549447789\n",
            "Epoch 3 - Training loss: 0.08307330535133002\n",
            "Epoch 3 - Training loss: 0.08314191775599014\n",
            "Epoch 3 - Training loss: 0.08325200026699983\n",
            "Epoch 3 - Training loss: 0.08351196756145593\n",
            "Epoch 3 - Training loss: 0.0836797806897016\n",
            "Epoch 3 - Training loss: 0.08379437949166878\n",
            "Epoch 3 - Training loss: 0.08384120172815028\n",
            "Epoch 3 - Training loss: 0.08397643048085891\n",
            "Epoch 3 - Training loss: 0.08413268257973036\n",
            "Epoch 3 - Training loss: 0.0842432016725225\n",
            "Epoch 3 - Training loss: 0.08443178590744543\n",
            "Epoch 3 - Training loss: 0.08451856680706873\n",
            "Epoch 3 - Training loss: 0.0846919088062447\n",
            "Epoch 3 - Training loss: 0.08489329979490878\n",
            "Epoch 3 - Training loss: 0.08496480976848968\n",
            "Epoch 3 - Training loss: 0.08505027994577057\n",
            "Epoch 3 - Training loss: 0.08509831849350603\n",
            "Epoch 3 - Training loss: 0.08523510355971006\n",
            "Epoch 3 - Training loss: 0.08527700223330496\n",
            "Epoch 3 - Training loss: 0.08538512065053494\n",
            "Epoch 3 - Training loss: 0.08559983975883485\n",
            "Epoch 3 - Training loss: 0.08575394048270132\n",
            "Epoch 3 - Training loss: 0.0860608056235288\n",
            "Epoch 3 - Training loss: 0.0862021371166208\n",
            "Epoch 3 - Training loss: 0.08626630094482192\n",
            "Epoch 3 - Training loss: 0.08637637926190138\n",
            "Epoch 3 - Training loss: 0.0864261200012111\n",
            "Epoch 3 - Training loss: 0.0864507364990837\n",
            "Epoch 3 - Training loss: 0.08652805114216579\n",
            "Epoch 3 - Training loss: 0.08665276546555478\n",
            "Epoch 3 - Training loss: 0.08680446379816037\n",
            "Epoch 3 - Training loss: 0.08691723860585804\n",
            "Epoch 3 - Training loss: 0.08697064657971612\n",
            "Epoch 3 - Training loss: 0.08699815582507836\n",
            "Epoch 3 - Training loss: 0.0870873255015754\n",
            "Epoch 3 - Training loss: 0.0871726169840677\n",
            "Epoch 3 - Training loss: 0.08723384941787099\n",
            "Epoch 3 - Training loss: 0.08730884731165382\n",
            "Epoch 3 - Training loss: 0.0873280909817928\n",
            "Epoch 3 - Training loss: 0.08747759542819153\n",
            "Epoch 3 - Training loss: 0.08770730475515826\n",
            "Epoch 3 - Training loss: 0.08782571748391524\n",
            "Epoch 3 - Training loss: 0.08788754844636933\n",
            "Epoch 3 - Training loss: 0.08809631612938223\n",
            "Epoch 3 - Training loss: 0.0882048995327403\n",
            "Epoch 3 - Training loss: 0.08831100112228378\n",
            "Epoch 3 - Training loss: 0.0883849039316368\n",
            "Epoch 3 - Training loss: 0.08844163560314473\n",
            "Epoch 3 - Training loss: 0.08852728391920071\n",
            "Epoch 3 - Training loss: 0.08856094514986854\n",
            "Epoch 3 - Training loss: 0.08862702416649251\n",
            "Epoch 3 - Training loss: 0.08868419602195592\n",
            "Epoch 3 - Training loss: 0.08879140996411919\n",
            "Epoch 3 - Training loss: 0.08896022936555623\n",
            "Epoch 3 - Training loss: 0.08904921896517404\n",
            "Epoch 3 - Training loss: 0.08929214951421406\n",
            "Epoch 3 - Training loss: 0.08951158172635636\n",
            "Epoch 3 - Training loss: 0.08962904819960534\n",
            "Epoch 3 - Training loss: 0.08973765576572053\n",
            "Epoch 3 - Training loss: 0.0898121739549042\n",
            "Epoch 3 - Training loss: 0.0898361433503915\n",
            "Epoch 3 - Training loss: 0.0898960846216122\n",
            "Epoch 3 - Training loss: 0.08997824861566776\n",
            "Epoch 3 - Training loss: 0.09016721449824157\n",
            "Epoch 3 - Training loss: 0.09035367160034713\n",
            "Epoch 3 - Training loss: 0.09069255561923295\n",
            "Epoch 3 - Training loss: 0.09071441551547314\n",
            "Epoch 3 - Training loss: 0.0907737187850577\n",
            "Epoch 3 - Training loss: 0.09093402426983756\n",
            "Epoch 3 - Training loss: 0.09099727546148845\n",
            "Epoch 3 - Training loss: 0.09101735363835528\n",
            "Epoch 3 - Training loss: 0.09107281294053615\n",
            "Epoch 3 - Training loss: 0.09124680717529328\n",
            "Epoch 3 - Training loss: 0.09140438400010374\n",
            "Epoch 3 - Training loss: 0.09154764263590834\n",
            "Epoch 3 - Training loss: 0.09159110614986245\n",
            "Epoch 3 - Training loss: 0.09164528572348071\n",
            "Epoch 3 - Training loss: 0.09175086090329296\n",
            "Epoch 3 - Training loss: 0.09193529459292382\n",
            "Epoch 3 - Training loss: 0.09205976176037908\n",
            "Epoch 3 - Training loss: 0.09216979597764674\n",
            "Epoch 3 - Training loss: 0.09228101482928626\n",
            "Epoch 3 - Training loss: 0.09244651160538514\n",
            "Epoch 3 - Training loss: 0.09258992235059105\n",
            "Epoch 3 - Training loss: 0.09281959240712019\n",
            "Epoch 3 - Training loss: 0.09298118501544189\n",
            "Epoch 3 - Training loss: 0.09307238108305725\n",
            "Epoch 3 - Training loss: 0.09316624502248283\n",
            "Epoch 3 - Training loss: 0.09329743347942893\n",
            "Epoch 3 - Training loss: 0.09336470969632935\n",
            "Epoch 3 - Training loss: 0.09347604891694368\n",
            "Epoch 3 - Training loss: 0.09365973729433726\n",
            "Epoch 3 - Training loss: 0.09373204571915779\n",
            "Epoch 3 - Training loss: 0.09382735655656946\n",
            "Epoch 3 - Training loss: 0.09394613607947443\n",
            "Epoch 3 - Training loss: 0.09400424391213956\n",
            "Epoch 3 - Training loss: 0.09402959994566656\n",
            "Epoch 3 - Training loss: 0.09416437278320985\n",
            "Epoch 3 - Training loss: 0.0942540926112136\n",
            "Epoch 3 - Training loss: 0.09440325502989325\n",
            "Epoch 3 - Training loss: 0.09457421393941905\n",
            "Epoch 3 - Training loss: 0.09471337002183773\n",
            "Epoch 3 - Training loss: 0.09479707951889808\n",
            "Epoch 3 - Training loss: 0.0949503405194388\n",
            "Epoch 3 - Training loss: 0.09518747432614123\n",
            "Epoch 3 - Training loss: 0.09534287218179212\n",
            "Epoch 3 - Training loss: 0.0955197587986586\n",
            "Epoch 3 - Training loss: 0.09559860385295108\n",
            "Epoch 3 - Training loss: 0.09565905983379083\n",
            "Epoch 3 - Training loss: 0.09577786527486688\n",
            "Epoch 3 - Training loss: 0.09589547264987408\n",
            "Epoch 3 - Training loss: 0.09600360167107538\n",
            "Epoch 3 - Training loss: 0.09618533931489882\n",
            "Epoch 3 - Training loss: 0.0962231779979395\n",
            "Epoch 3 - Training loss: 0.09645623125350361\n",
            "Epoch 3 - Training loss: 0.09656885662066467\n",
            "Epoch 3 - Training loss: 0.09669562466720592\n",
            "Epoch 3 - Training loss: 0.09680449998558266\n",
            "Epoch 3 - Training loss: 0.09686732239155436\n",
            "Epoch 3 - Training loss: 0.0969324575951581\n",
            "Epoch 3 - Training loss: 0.096991504129491\n",
            "Epoch 3 - Training loss: 0.09711410459289864\n",
            "Epoch 3 - Training loss: 0.09721608856109096\n",
            "Epoch 3 - Training loss: 0.09730781136410259\n",
            "Epoch 3 - Training loss: 0.09741390375559454\n",
            "Epoch 3 - Training loss: 0.09757714202083441\n",
            "Epoch 3 - Training loss: 0.09772030195630373\n",
            "Epoch 3 - Training loss: 0.09777936154503876\n",
            "Epoch 3 - Training loss: 0.09787240930433784\n",
            "Epoch 3 - Training loss: 0.09795569168773097\n",
            "Epoch 3 - Training loss: 0.09801745677450255\n",
            "Epoch 3 - Training loss: 0.09804102765328722\n",
            "Epoch 3 - Training loss: 0.09810285367119287\n",
            "Epoch 3 - Training loss: 0.09817013483661324\n",
            "Epoch 3 - Training loss: 0.098232977477504\n",
            "Epoch 3 - Training loss: 0.09826191028020084\n",
            "Epoch 3 - Training loss: 0.0985099954236704\n",
            "Epoch 3 - Training loss: 0.09856172934420772\n",
            "Epoch 3 - Training loss: 0.09868878002034258\n",
            "Epoch 3 - Training loss: 0.09882374434892748\n",
            "Epoch 3 - Training loss: 0.09889127378230855\n",
            "Epoch 3 - Training loss: 0.0989239916249093\n",
            "Epoch 3 - Training loss: 0.09896498184396958\n",
            "Epoch 3 - Training loss: 0.09917918014238829\n",
            "Epoch 3 - Training loss: 0.09933087129447697\n",
            "Epoch 3 - Training loss: 0.09937557011945987\n",
            "Epoch 3 - Training loss: 0.09944863627944737\n",
            "Epoch 3 - Training loss: 0.09959499324475334\n",
            "Epoch 3 - Training loss: 0.0997299755246305\n",
            "Epoch 3 - Training loss: 0.0998052344429118\n",
            "Epoch 3 - Training loss: 0.09995604760206139\n",
            "Epoch 3 - Training loss: 0.10003226750226481\n",
            "Epoch 3 - Training loss: 0.10010334952021523\n",
            "Epoch 3 - Training loss: 0.10024148577859979\n",
            "Epoch 3 - Training loss: 0.10027240730250186\n",
            "Epoch 3 - Training loss: 0.10037646690872051\n",
            "Epoch 3 - Training loss: 0.1004260845903331\n",
            "Epoch 3 - Training loss: 0.10051109320494031\n",
            "Epoch 3 - Training loss: 0.10070031811036408\n",
            "Epoch 3 - Training loss: 0.10084758627055677\n",
            "Epoch 3 - Training loss: 0.1010533936583856\n",
            "Epoch 3 - Training loss: 0.10123722982416147\n",
            "Epoch 3 - Training loss: 0.10128906258006594\n",
            "Epoch 3 - Training loss: 0.10138384383846956\n",
            "Epoch 3 - Training loss: 0.10168157491697939\n",
            "Epoch 3 - Training loss: 0.10192077857122492\n",
            "Epoch 3 - Training loss: 0.10208611453234005\n",
            "Epoch 3 - Training loss: 0.10231599120347738\n",
            "Epoch 3 - Training loss: 0.10241774417189901\n",
            "Epoch 3 - Training loss: 0.10263166441592073\n",
            "Epoch 3 - Training loss: 0.10267549915624453\n",
            "Epoch 3 - Training loss: 0.10270679236919895\n",
            "Epoch 3 - Training loss: 0.10291758065284697\n",
            "Epoch 3 - Training loss: 0.10298662249848786\n",
            "Epoch 3 - Training loss: 0.10304368413619396\n",
            "Epoch 3 - Training loss: 0.10309609051928846\n",
            "Epoch 3 - Training loss: 0.10323870551389164\n",
            "Epoch 3 - Training loss: 0.1033489273857079\n",
            "Epoch 3 - Training loss: 0.10353962061152275\n",
            "Epoch 3 - Training loss: 0.10357514721180584\n",
            "Epoch 3 - Training loss: 0.10369286142877424\n",
            "Epoch 3 - Training loss: 0.10383085704752124\n",
            "Epoch 3 - Training loss: 0.10392611428523368\n",
            "Epoch 3 - Training loss: 0.10419935084132752\n",
            "Epoch 3 - Training loss: 0.10438300358620026\n",
            "Epoch 3 - Training loss: 0.10441448110967938\n",
            "Epoch 3 - Training loss: 0.10457502402810019\n",
            "Epoch 3 - Training loss: 0.10463839788823875\n",
            "Epoch 3 - Training loss: 0.1047366792991408\n",
            "Epoch 3 - Training loss: 0.1048122390763147\n",
            "Epoch 3 - Training loss: 0.10498914161105273\n",
            "Epoch 3 - Training loss: 0.10509159426683429\n",
            "Epoch 3 - Training loss: 0.10522239544450729\n",
            "Epoch 3 - Training loss: 0.10525955943696534\n",
            "Epoch 3 - Training loss: 0.10553472920426174\n",
            "Epoch 3 - Training loss: 0.10558950679420408\n",
            "Epoch 3 - Training loss: 0.10573195517539724\n",
            "Epoch 3 - Training loss: 0.10580558812758054\n",
            "Epoch 3 - Training loss: 0.10593461650393919\n",
            "Epoch 3 - Training loss: 0.10598036334681105\n",
            "Epoch 3 - Training loss: 0.1060716469071186\n",
            "Epoch 3 - Training loss: 0.10612048835817303\n",
            "Epoch 3 - Training loss: 0.10622599733266622\n",
            "Epoch 3 - Training loss: 0.10630631112038835\n",
            "Epoch 4 - Training loss: 6.196167327956095e-05\n",
            "Epoch 4 - Training loss: 0.00017670012994615763\n",
            "Epoch 4 - Training loss: 0.00021923083200383543\n",
            "Epoch 4 - Training loss: 0.0003428616837016555\n",
            "Epoch 4 - Training loss: 0.00042302074081607973\n",
            "Epoch 4 - Training loss: 0.0005977026053837367\n",
            "Epoch 4 - Training loss: 0.0006892844748649516\n",
            "Epoch 4 - Training loss: 0.0008539285169227291\n",
            "Epoch 4 - Training loss: 0.0010206357661340791\n",
            "Epoch 4 - Training loss: 0.0011803084599183822\n",
            "Epoch 4 - Training loss: 0.001193827649614196\n",
            "Epoch 4 - Training loss: 0.0012679378837664752\n",
            "Epoch 4 - Training loss: 0.0012853330136266853\n",
            "Epoch 4 - Training loss: 0.0013243478438906323\n",
            "Epoch 4 - Training loss: 0.001353024161541894\n",
            "Epoch 4 - Training loss: 0.0014195748364556828\n",
            "Epoch 4 - Training loss: 0.0014608463824494307\n",
            "Epoch 4 - Training loss: 0.001626461589419003\n",
            "Epoch 4 - Training loss: 0.0017367185774578976\n",
            "Epoch 4 - Training loss: 0.0018302301870289641\n",
            "Epoch 4 - Training loss: 0.002011891196309122\n",
            "Epoch 4 - Training loss: 0.0021528040112526433\n",
            "Epoch 4 - Training loss: 0.0021784181521931436\n",
            "Epoch 4 - Training loss: 0.0022158675821128686\n",
            "Epoch 4 - Training loss: 0.00228497213018792\n",
            "Epoch 4 - Training loss: 0.002349544413832586\n",
            "Epoch 4 - Training loss: 0.0023770396198545185\n",
            "Epoch 4 - Training loss: 0.002660589113926837\n",
            "Epoch 4 - Training loss: 0.002697850857525746\n",
            "Epoch 4 - Training loss: 0.0028122955961013907\n",
            "Epoch 4 - Training loss: 0.002874973132761557\n",
            "Epoch 4 - Training loss: 0.00289079497880074\n",
            "Epoch 4 - Training loss: 0.0030136744894865733\n",
            "Epoch 4 - Training loss: 0.003068626687518442\n",
            "Epoch 4 - Training loss: 0.00325551114158272\n",
            "Epoch 4 - Training loss: 0.003293222477679441\n",
            "Epoch 4 - Training loss: 0.0033730215319533593\n",
            "Epoch 4 - Training loss: 0.0033990817108769406\n",
            "Epoch 4 - Training loss: 0.003434758633375168\n",
            "Epoch 4 - Training loss: 0.0035949253213049762\n",
            "Epoch 4 - Training loss: 0.003736965016706158\n",
            "Epoch 4 - Training loss: 0.0038739287856418185\n",
            "Epoch 4 - Training loss: 0.003954962626703258\n",
            "Epoch 4 - Training loss: 0.003975926630341931\n",
            "Epoch 4 - Training loss: 0.004001423527897675\n",
            "Epoch 4 - Training loss: 0.004079383091012171\n",
            "Epoch 4 - Training loss: 0.004268074811283332\n",
            "Epoch 4 - Training loss: 0.004315691757828061\n",
            "Epoch 4 - Training loss: 0.00437202870742535\n",
            "Epoch 4 - Training loss: 0.00440762876106033\n",
            "Epoch 4 - Training loss: 0.0044306511821936186\n",
            "Epoch 4 - Training loss: 0.00481570042820691\n",
            "Epoch 4 - Training loss: 0.00493953312868312\n",
            "Epoch 4 - Training loss: 0.005066533986010404\n",
            "Epoch 4 - Training loss: 0.0051592679392459045\n",
            "Epoch 4 - Training loss: 0.005219263687872811\n",
            "Epoch 4 - Training loss: 0.005356519729264382\n",
            "Epoch 4 - Training loss: 0.0056769485170367175\n",
            "Epoch 4 - Training loss: 0.005736895407965062\n",
            "Epoch 4 - Training loss: 0.005792617029361506\n",
            "Epoch 4 - Training loss: 0.005824092376445021\n",
            "Epoch 4 - Training loss: 0.0059734431724908004\n",
            "Epoch 4 - Training loss: 0.00598768773141192\n",
            "Epoch 4 - Training loss: 0.0060177531990923605\n",
            "Epoch 4 - Training loss: 0.006106886321674786\n",
            "Epoch 4 - Training loss: 0.00621818639099725\n",
            "Epoch 4 - Training loss: 0.00626707154868254\n",
            "Epoch 4 - Training loss: 0.006522336144691337\n",
            "Epoch 4 - Training loss: 0.006569288686902793\n",
            "Epoch 4 - Training loss: 0.006757058250878666\n",
            "Epoch 4 - Training loss: 0.006871591395597214\n",
            "Epoch 4 - Training loss: 0.006931345798631213\n",
            "Epoch 4 - Training loss: 0.007076062015823718\n",
            "Epoch 4 - Training loss: 0.007194236143311458\n",
            "Epoch 4 - Training loss: 0.0072159332884495445\n",
            "Epoch 4 - Training loss: 0.00739079023371818\n",
            "Epoch 4 - Training loss: 0.007663991204949457\n",
            "Epoch 4 - Training loss: 0.0077030700800229494\n",
            "Epoch 4 - Training loss: 0.007863080035696532\n",
            "Epoch 4 - Training loss: 0.00788350819921824\n",
            "Epoch 4 - Training loss: 0.007987254984311458\n",
            "Epoch 4 - Training loss: 0.008050991873592456\n",
            "Epoch 4 - Training loss: 0.00807794098502029\n",
            "Epoch 4 - Training loss: 0.008249651243501126\n",
            "Epoch 4 - Training loss: 0.008501282402637925\n",
            "Epoch 4 - Training loss: 0.008618076695307994\n",
            "Epoch 4 - Training loss: 0.008819604288540416\n",
            "Epoch 4 - Training loss: 0.00892142775947097\n",
            "Epoch 4 - Training loss: 0.009087860592202082\n",
            "Epoch 4 - Training loss: 0.009107763773755733\n",
            "Epoch 4 - Training loss: 0.00914713121585246\n",
            "Epoch 4 - Training loss: 0.00916792203916479\n",
            "Epoch 4 - Training loss: 0.009339036362400568\n",
            "Epoch 4 - Training loss: 0.009385463610879267\n",
            "Epoch 4 - Training loss: 0.009598935120252531\n",
            "Epoch 4 - Training loss: 0.009651897711429134\n",
            "Epoch 4 - Training loss: 0.009748966202759413\n",
            "Epoch 4 - Training loss: 0.009956661923560125\n",
            "Epoch 4 - Training loss: 0.010092452154564323\n",
            "Epoch 4 - Training loss: 0.010169085061578735\n",
            "Epoch 4 - Training loss: 0.010279363027211826\n",
            "Epoch 4 - Training loss: 0.010340120780394911\n",
            "Epoch 4 - Training loss: 0.010716891271679768\n",
            "Epoch 4 - Training loss: 0.010768391003311952\n",
            "Epoch 4 - Training loss: 0.010849166306049457\n",
            "Epoch 4 - Training loss: 0.010877029173997547\n",
            "Epoch 4 - Training loss: 0.010980171048199571\n",
            "Epoch 4 - Training loss: 0.01107993904056389\n",
            "Epoch 4 - Training loss: 0.011128730871387\n",
            "Epoch 4 - Training loss: 0.011308722537574865\n",
            "Epoch 4 - Training loss: 0.011415755870674591\n",
            "Epoch 4 - Training loss: 0.011544062049070528\n",
            "Epoch 4 - Training loss: 0.011695512304348605\n",
            "Epoch 4 - Training loss: 0.011711718896820919\n",
            "Epoch 4 - Training loss: 0.011765724219786904\n",
            "Epoch 4 - Training loss: 0.012006514847699577\n",
            "Epoch 4 - Training loss: 0.012041997145623096\n",
            "Epoch 4 - Training loss: 0.01215669288456853\n",
            "Epoch 4 - Training loss: 0.012192948998363097\n",
            "Epoch 4 - Training loss: 0.012250854684385474\n",
            "Epoch 4 - Training loss: 0.01234726145613327\n",
            "Epoch 4 - Training loss: 0.012516230917664796\n",
            "Epoch 4 - Training loss: 0.012584155573368644\n",
            "Epoch 4 - Training loss: 0.012807538674306323\n",
            "Epoch 4 - Training loss: 0.012850180563173379\n",
            "Epoch 4 - Training loss: 0.013029775045105198\n",
            "Epoch 4 - Training loss: 0.013148895769652082\n",
            "Epoch 4 - Training loss: 0.013247634946450051\n",
            "Epoch 4 - Training loss: 0.013292585347673849\n",
            "Epoch 4 - Training loss: 0.01340612874074436\n",
            "Epoch 4 - Training loss: 0.013452239724785597\n",
            "Epoch 4 - Training loss: 0.013491732095167644\n",
            "Epoch 4 - Training loss: 0.013535523339351421\n",
            "Epoch 4 - Training loss: 0.01367006937105423\n",
            "Epoch 4 - Training loss: 0.013810261258922978\n",
            "Epoch 4 - Training loss: 0.01394391118753344\n",
            "Epoch 4 - Training loss: 0.014104593633564868\n",
            "Epoch 4 - Training loss: 0.014236050057234858\n",
            "Epoch 4 - Training loss: 0.014268332781560068\n",
            "Epoch 4 - Training loss: 0.01450325440126918\n",
            "Epoch 4 - Training loss: 0.014580708500474437\n",
            "Epoch 4 - Training loss: 0.014627025805231033\n",
            "Epoch 4 - Training loss: 0.014666391575315805\n",
            "Epoch 4 - Training loss: 0.014733822840545922\n",
            "Epoch 4 - Training loss: 0.014840723373798102\n",
            "Epoch 4 - Training loss: 0.014865914646631428\n",
            "Epoch 4 - Training loss: 0.014879363060775978\n",
            "Epoch 4 - Training loss: 0.014945401734650643\n",
            "Epoch 4 - Training loss: 0.01511263686305742\n",
            "Epoch 4 - Training loss: 0.015178463346303017\n",
            "Epoch 4 - Training loss: 0.01520749131467805\n",
            "Epoch 4 - Training loss: 0.015280515368559212\n",
            "Epoch 4 - Training loss: 0.015298484018378293\n",
            "Epoch 4 - Training loss: 0.015437030201670584\n",
            "Epoch 4 - Training loss: 0.015595958266319878\n",
            "Epoch 4 - Training loss: 0.01570393027725822\n",
            "Epoch 4 - Training loss: 0.015742734644506404\n",
            "Epoch 4 - Training loss: 0.01576830665551142\n",
            "Epoch 4 - Training loss: 0.015825985400661477\n",
            "Epoch 4 - Training loss: 0.01585593914140516\n",
            "Epoch 4 - Training loss: 0.01593139676301718\n",
            "Epoch 4 - Training loss: 0.0159543336931068\n",
            "Epoch 4 - Training loss: 0.015986001329508417\n",
            "Epoch 4 - Training loss: 0.016063625012824274\n",
            "Epoch 4 - Training loss: 0.016211198694479744\n",
            "Epoch 4 - Training loss: 0.01628084940665058\n",
            "Epoch 4 - Training loss: 0.016410010122159906\n",
            "Epoch 4 - Training loss: 0.016489130673187375\n",
            "Epoch 4 - Training loss: 0.01659037861456749\n",
            "Epoch 4 - Training loss: 0.016716197530216755\n",
            "Epoch 4 - Training loss: 0.016778686927802273\n",
            "Epoch 4 - Training loss: 0.016824418182439134\n",
            "Epoch 4 - Training loss: 0.016970340218116987\n",
            "Epoch 4 - Training loss: 0.017111872320871618\n",
            "Epoch 4 - Training loss: 0.0171613027967179\n",
            "Epoch 4 - Training loss: 0.01721578449217368\n",
            "Epoch 4 - Training loss: 0.017261254805714082\n",
            "Epoch 4 - Training loss: 0.017342016112798057\n",
            "Epoch 4 - Training loss: 0.017419909303789453\n",
            "Epoch 4 - Training loss: 0.01747469846675518\n",
            "Epoch 4 - Training loss: 0.017841975901649196\n",
            "Epoch 4 - Training loss: 0.01791027037780295\n",
            "Epoch 4 - Training loss: 0.018108441190583618\n",
            "Epoch 4 - Training loss: 0.018239906597842793\n",
            "Epoch 4 - Training loss: 0.018250275109368348\n",
            "Epoch 4 - Training loss: 0.018301244769523394\n",
            "Epoch 4 - Training loss: 0.018414408040008566\n",
            "Epoch 4 - Training loss: 0.018567417515938215\n",
            "Epoch 4 - Training loss: 0.018644696534442496\n",
            "Epoch 4 - Training loss: 0.018672544457542618\n",
            "Epoch 4 - Training loss: 0.01871422113481361\n",
            "Epoch 4 - Training loss: 0.01889117415557538\n",
            "Epoch 4 - Training loss: 0.018944579337451504\n",
            "Epoch 4 - Training loss: 0.019002499622799186\n",
            "Epoch 4 - Training loss: 0.019055761035476158\n",
            "Epoch 4 - Training loss: 0.019108144201036455\n",
            "Epoch 4 - Training loss: 0.01924434043308184\n",
            "Epoch 4 - Training loss: 0.019367815442144998\n",
            "Epoch 4 - Training loss: 0.01945071060980942\n",
            "Epoch 4 - Training loss: 0.019684106878030783\n",
            "Epoch 4 - Training loss: 0.019786016443676786\n",
            "Epoch 4 - Training loss: 0.0198617790466242\n",
            "Epoch 4 - Training loss: 0.019907540016209903\n",
            "Epoch 4 - Training loss: 0.019980020725777918\n",
            "Epoch 4 - Training loss: 0.02005398778645977\n",
            "Epoch 4 - Training loss: 0.020098328924001152\n",
            "Epoch 4 - Training loss: 0.02014914834931461\n",
            "Epoch 4 - Training loss: 0.020324879538402882\n",
            "Epoch 4 - Training loss: 0.020408273505757867\n",
            "Epoch 4 - Training loss: 0.020503320427400978\n",
            "Epoch 4 - Training loss: 0.020691308202837575\n",
            "Epoch 4 - Training loss: 0.020852659326563005\n",
            "Epoch 4 - Training loss: 0.020960713953161035\n",
            "Epoch 4 - Training loss: 0.021049273897335727\n",
            "Epoch 4 - Training loss: 0.02106013421668236\n",
            "Epoch 4 - Training loss: 0.02112245572996991\n",
            "Epoch 4 - Training loss: 0.021285763115628062\n",
            "Epoch 4 - Training loss: 0.021405359215812006\n",
            "Epoch 4 - Training loss: 0.0214172560594547\n",
            "Epoch 4 - Training loss: 0.021643495322195196\n",
            "Epoch 4 - Training loss: 0.0217217952370453\n",
            "Epoch 4 - Training loss: 0.021808046994528284\n",
            "Epoch 4 - Training loss: 0.02186454826993729\n",
            "Epoch 4 - Training loss: 0.022021504393073797\n",
            "Epoch 4 - Training loss: 0.022104629567627713\n",
            "Epoch 4 - Training loss: 0.02235048126850301\n",
            "Epoch 4 - Training loss: 0.022519601981586486\n",
            "Epoch 4 - Training loss: 0.022599444555829584\n",
            "Epoch 4 - Training loss: 0.022635947809671795\n",
            "Epoch 4 - Training loss: 0.022646745577502226\n",
            "Epoch 4 - Training loss: 0.02276385784633696\n",
            "Epoch 4 - Training loss: 0.022954260499508522\n",
            "Epoch 4 - Training loss: 0.023038506049162417\n",
            "Epoch 4 - Training loss: 0.02308128725514928\n",
            "Epoch 4 - Training loss: 0.023154146697268937\n",
            "Epoch 4 - Training loss: 0.023205938653698736\n",
            "Epoch 4 - Training loss: 0.02331683602628868\n",
            "Epoch 4 - Training loss: 0.023326470557330198\n",
            "Epoch 4 - Training loss: 0.023349710447249063\n",
            "Epoch 4 - Training loss: 0.023479051358783358\n",
            "Epoch 4 - Training loss: 0.023547334015679194\n",
            "Epoch 4 - Training loss: 0.02359070431397382\n",
            "Epoch 4 - Training loss: 0.02364967780005036\n",
            "Epoch 4 - Training loss: 0.023736610626384838\n",
            "Epoch 4 - Training loss: 0.02378936346544862\n",
            "Epoch 4 - Training loss: 0.023871826102285942\n",
            "Epoch 4 - Training loss: 0.02405582592665736\n",
            "Epoch 4 - Training loss: 0.024117070044686734\n",
            "Epoch 4 - Training loss: 0.024253503821575755\n",
            "Epoch 4 - Training loss: 0.024394406898340375\n",
            "Epoch 4 - Training loss: 0.02453287624057009\n",
            "Epoch 4 - Training loss: 0.02455629562951132\n",
            "Epoch 4 - Training loss: 0.024577333195718813\n",
            "Epoch 4 - Training loss: 0.024679342804075494\n",
            "Epoch 4 - Training loss: 0.024796989131798303\n",
            "Epoch 4 - Training loss: 0.024836958736491038\n",
            "Epoch 4 - Training loss: 0.024870045045251723\n",
            "Epoch 4 - Training loss: 0.024914087623413374\n",
            "Epoch 4 - Training loss: 0.025219265152134303\n",
            "Epoch 4 - Training loss: 0.02536877155252325\n",
            "Epoch 4 - Training loss: 0.025430533239669574\n",
            "Epoch 4 - Training loss: 0.025476123147737433\n",
            "Epoch 4 - Training loss: 0.025522373973139757\n",
            "Epoch 4 - Training loss: 0.025560539683450196\n",
            "Epoch 4 - Training loss: 0.0256952924091917\n",
            "Epoch 4 - Training loss: 0.025820507454489276\n",
            "Epoch 4 - Training loss: 0.02590403276811372\n",
            "Epoch 4 - Training loss: 0.02597075201042775\n",
            "Epoch 4 - Training loss: 0.026044773631917834\n",
            "Epoch 4 - Training loss: 0.02632041080002941\n",
            "Epoch 4 - Training loss: 0.026428433241390152\n",
            "Epoch 4 - Training loss: 0.02649457124707255\n",
            "Epoch 4 - Training loss: 0.02674157521439228\n",
            "Epoch 4 - Training loss: 0.026882118375689934\n",
            "Epoch 4 - Training loss: 0.027010757550557474\n",
            "Epoch 4 - Training loss: 0.02717271275909693\n",
            "Epoch 4 - Training loss: 0.027261185123205883\n",
            "Epoch 4 - Training loss: 0.02737277375558006\n",
            "Epoch 4 - Training loss: 0.027502089328785884\n",
            "Epoch 4 - Training loss: 0.02754769842329024\n",
            "Epoch 4 - Training loss: 0.02766603129262577\n",
            "Epoch 4 - Training loss: 0.02773486306048882\n",
            "Epoch 4 - Training loss: 0.02777992012098368\n",
            "Epoch 4 - Training loss: 0.027826880097889632\n",
            "Epoch 4 - Training loss: 0.02793001460888461\n",
            "Epoch 4 - Training loss: 0.027969879559926324\n",
            "Epoch 4 - Training loss: 0.027989518923013768\n",
            "Epoch 4 - Training loss: 0.028046181279498695\n",
            "Epoch 4 - Training loss: 0.02809863965779638\n",
            "Epoch 4 - Training loss: 0.028194109681866635\n",
            "Epoch 4 - Training loss: 0.02832589734659425\n",
            "Epoch 4 - Training loss: 0.028450264368079174\n",
            "Epoch 4 - Training loss: 0.028529990616359753\n",
            "Epoch 4 - Training loss: 0.028728008779611732\n",
            "Epoch 4 - Training loss: 0.028776438053903865\n",
            "Epoch 4 - Training loss: 0.028867066487360166\n",
            "Epoch 4 - Training loss: 0.02900073478030148\n",
            "Epoch 4 - Training loss: 0.029054843263625146\n",
            "Epoch 4 - Training loss: 0.0291557042672833\n",
            "Epoch 4 - Training loss: 0.029369256414496885\n",
            "Epoch 4 - Training loss: 0.02948572041133224\n",
            "Epoch 4 - Training loss: 0.02954809669989036\n",
            "Epoch 4 - Training loss: 0.029652928203733553\n",
            "Epoch 4 - Training loss: 0.02966771719913715\n",
            "Epoch 4 - Training loss: 0.029682184154513295\n",
            "Epoch 4 - Training loss: 0.02972634756234663\n",
            "Epoch 4 - Training loss: 0.029794337915014357\n",
            "Epoch 4 - Training loss: 0.02983419638849906\n",
            "Epoch 4 - Training loss: 0.029890317442034606\n",
            "Epoch 4 - Training loss: 0.029966928378573614\n",
            "Epoch 4 - Training loss: 0.030056316997291947\n",
            "Epoch 4 - Training loss: 0.030123486213370174\n",
            "Epoch 4 - Training loss: 0.030316516554463647\n",
            "Epoch 4 - Training loss: 0.030413030978363714\n",
            "Epoch 4 - Training loss: 0.030470799297284978\n",
            "Epoch 4 - Training loss: 0.030489672493658214\n",
            "Epoch 4 - Training loss: 0.03049823150499416\n",
            "Epoch 4 - Training loss: 0.030547460402125744\n",
            "Epoch 4 - Training loss: 0.030689785983770895\n",
            "Epoch 4 - Training loss: 0.030713837597765394\n",
            "Epoch 4 - Training loss: 0.030792189142438395\n",
            "Epoch 4 - Training loss: 0.030968770158411596\n",
            "Epoch 4 - Training loss: 0.03103972603279009\n",
            "Epoch 4 - Training loss: 0.031129918686711965\n",
            "Epoch 4 - Training loss: 0.031293508320697334\n",
            "Epoch 4 - Training loss: 0.03131720610360093\n",
            "Epoch 4 - Training loss: 0.031383769729657215\n",
            "Epoch 4 - Training loss: 0.031461334578804116\n",
            "Epoch 4 - Training loss: 0.031530455846998734\n",
            "Epoch 4 - Training loss: 0.03158661695336228\n",
            "Epoch 4 - Training loss: 0.031642932115174306\n",
            "Epoch 4 - Training loss: 0.03167146396463805\n",
            "Epoch 4 - Training loss: 0.0319063279495787\n",
            "Epoch 4 - Training loss: 0.03199872347329662\n",
            "Epoch 4 - Training loss: 0.03217138140114831\n",
            "Epoch 4 - Training loss: 0.032355169225523846\n",
            "Epoch 4 - Training loss: 0.03239043289100501\n",
            "Epoch 4 - Training loss: 0.032481807780497744\n",
            "Epoch 4 - Training loss: 0.032558088977215516\n",
            "Epoch 4 - Training loss: 0.032722859361421455\n",
            "Epoch 4 - Training loss: 0.0327555318571516\n",
            "Epoch 4 - Training loss: 0.03289117592015564\n",
            "Epoch 4 - Training loss: 0.03299590658698318\n",
            "Epoch 4 - Training loss: 0.033069741856584796\n",
            "Epoch 4 - Training loss: 0.03312712019559608\n",
            "Epoch 4 - Training loss: 0.03323147784092469\n",
            "Epoch 4 - Training loss: 0.03328799059404048\n",
            "Epoch 4 - Training loss: 0.03329617918324051\n",
            "Epoch 4 - Training loss: 0.033369274470391175\n",
            "Epoch 4 - Training loss: 0.03342692991460501\n",
            "Epoch 4 - Training loss: 0.03356505379771818\n",
            "Epoch 4 - Training loss: 0.03358630186665668\n",
            "Epoch 4 - Training loss: 0.033705916680665665\n",
            "Epoch 4 - Training loss: 0.03403460824175049\n",
            "Epoch 4 - Training loss: 0.034113440237669296\n",
            "Epoch 4 - Training loss: 0.03414790709612212\n",
            "Epoch 4 - Training loss: 0.034174638893653846\n",
            "Epoch 4 - Training loss: 0.034295299608729034\n",
            "Epoch 4 - Training loss: 0.03435683551071676\n",
            "Epoch 4 - Training loss: 0.03454955230786729\n",
            "Epoch 4 - Training loss: 0.034672520137322485\n",
            "Epoch 4 - Training loss: 0.03471665713848717\n",
            "Epoch 4 - Training loss: 0.034789966935637406\n",
            "Epoch 4 - Training loss: 0.03485172102922824\n",
            "Epoch 4 - Training loss: 0.03497915888931959\n",
            "Epoch 4 - Training loss: 0.03509192217165219\n",
            "Epoch 4 - Training loss: 0.03520475494772641\n",
            "Epoch 4 - Training loss: 0.035225143382099394\n",
            "Epoch 4 - Training loss: 0.0352523812273545\n",
            "Epoch 4 - Training loss: 0.03536639865368668\n",
            "Epoch 4 - Training loss: 0.035483795045408356\n",
            "Epoch 4 - Training loss: 0.03556216431896824\n",
            "Epoch 4 - Training loss: 0.03562244594192454\n",
            "Epoch 4 - Training loss: 0.03577623634672623\n",
            "Epoch 4 - Training loss: 0.0357928710387968\n",
            "Epoch 4 - Training loss: 0.035936682749150406\n",
            "Epoch 4 - Training loss: 0.035999668738258675\n",
            "Epoch 4 - Training loss: 0.036124885840806115\n",
            "Epoch 4 - Training loss: 0.036272631346512194\n",
            "Epoch 4 - Training loss: 0.03644540660909371\n",
            "Epoch 4 - Training loss: 0.036522638350566315\n",
            "Epoch 4 - Training loss: 0.03670657099262357\n",
            "Epoch 4 - Training loss: 0.03680204486907291\n",
            "Epoch 4 - Training loss: 0.03681993587160987\n",
            "Epoch 4 - Training loss: 0.03701446897415782\n",
            "Epoch 4 - Training loss: 0.03705767707140652\n",
            "Epoch 4 - Training loss: 0.03708014640011894\n",
            "Epoch 4 - Training loss: 0.03713695927342372\n",
            "Epoch 4 - Training loss: 0.03726875364208526\n",
            "Epoch 4 - Training loss: 0.037498464891270025\n",
            "Epoch 4 - Training loss: 0.037647494883425454\n",
            "Epoch 4 - Training loss: 0.037689909771847316\n",
            "Epoch 4 - Training loss: 0.037716338546005396\n",
            "Epoch 4 - Training loss: 0.03789913601506112\n",
            "Epoch 4 - Training loss: 0.038067673438234625\n",
            "Epoch 4 - Training loss: 0.03808442158485526\n",
            "Epoch 4 - Training loss: 0.03818087501407686\n",
            "Epoch 4 - Training loss: 0.03824139658067781\n",
            "Epoch 4 - Training loss: 0.03854090788725342\n",
            "Epoch 4 - Training loss: 0.038603201226543774\n",
            "Epoch 4 - Training loss: 0.038757302093385126\n",
            "Epoch 4 - Training loss: 0.038848466627093264\n",
            "Epoch 4 - Training loss: 0.038987577592989785\n",
            "Epoch 4 - Training loss: 0.03904338540441827\n",
            "Epoch 4 - Training loss: 0.03912036229314199\n",
            "Epoch 4 - Training loss: 0.03915522558126114\n",
            "Epoch 4 - Training loss: 0.03927880355607726\n",
            "Epoch 4 - Training loss: 0.03936406870735988\n",
            "Epoch 4 - Training loss: 0.039429163671473956\n",
            "Epoch 4 - Training loss: 0.039466456647144196\n",
            "Epoch 4 - Training loss: 0.039552808383793464\n",
            "Epoch 4 - Training loss: 0.03978791488592685\n",
            "Epoch 4 - Training loss: 0.0398872889347994\n",
            "Epoch 4 - Training loss: 0.04018629160024591\n",
            "Epoch 4 - Training loss: 0.04020874941749359\n",
            "Epoch 4 - Training loss: 0.040224833746573756\n",
            "Epoch 4 - Training loss: 0.040377072111240773\n",
            "Epoch 4 - Training loss: 0.04039514974367295\n",
            "Epoch 4 - Training loss: 0.040407236562227646\n",
            "Epoch 4 - Training loss: 0.04049379433364248\n",
            "Epoch 4 - Training loss: 0.04052078529104177\n",
            "Epoch 4 - Training loss: 0.040567914701140385\n",
            "Epoch 4 - Training loss: 0.04067507416192593\n",
            "Epoch 4 - Training loss: 0.040788336299550434\n",
            "Epoch 4 - Training loss: 0.04103193592343694\n",
            "Epoch 4 - Training loss: 0.04134184013861519\n",
            "Epoch 4 - Training loss: 0.04138244532628545\n",
            "Epoch 4 - Training loss: 0.041475243022494605\n",
            "Epoch 4 - Training loss: 0.04163755223687206\n",
            "Epoch 4 - Training loss: 0.04171494623896346\n",
            "Epoch 4 - Training loss: 0.04184773689640293\n",
            "Epoch 4 - Training loss: 0.042002014384896896\n",
            "Epoch 4 - Training loss: 0.04210439297968327\n",
            "Epoch 4 - Training loss: 0.042244215928780623\n",
            "Epoch 4 - Training loss: 0.0423502929901867\n",
            "Epoch 4 - Training loss: 0.04245103798560433\n",
            "Epoch 4 - Training loss: 0.042577531194683715\n",
            "Epoch 4 - Training loss: 0.042672954346817824\n",
            "Epoch 4 - Training loss: 0.04274907136069877\n",
            "Epoch 4 - Training loss: 0.04279967033063997\n",
            "Epoch 4 - Training loss: 0.042951948807707856\n",
            "Epoch 4 - Training loss: 0.04306963052966002\n",
            "Epoch 4 - Training loss: 0.04310593202606892\n",
            "Epoch 4 - Training loss: 0.0432246804678205\n",
            "Epoch 4 - Training loss: 0.04336766873810019\n",
            "Epoch 4 - Training loss: 0.04338493765091528\n",
            "Epoch 4 - Training loss: 0.043416922240813914\n",
            "Epoch 4 - Training loss: 0.04357167537500863\n",
            "Epoch 4 - Training loss: 0.0436947260266428\n",
            "Epoch 4 - Training loss: 0.04383011357680058\n",
            "Epoch 4 - Training loss: 0.043957373131312795\n",
            "Epoch 4 - Training loss: 0.04405351178541875\n",
            "Epoch 4 - Training loss: 0.04410899540921773\n",
            "Epoch 4 - Training loss: 0.044314863704351476\n",
            "Epoch 4 - Training loss: 0.044389790206003796\n",
            "Epoch 4 - Training loss: 0.044485363322915805\n",
            "Epoch 4 - Training loss: 0.04455993588227453\n",
            "Epoch 4 - Training loss: 0.04470955210326831\n",
            "Epoch 4 - Training loss: 0.04473981717184408\n",
            "Epoch 4 - Training loss: 0.044838540573745395\n",
            "Epoch 4 - Training loss: 0.044943191150803044\n",
            "Epoch 4 - Training loss: 0.044991398393027564\n",
            "Epoch 4 - Training loss: 0.04505110020910118\n",
            "Epoch 4 - Training loss: 0.04521410747457034\n",
            "Epoch 4 - Training loss: 0.04537231872664459\n",
            "Epoch 4 - Training loss: 0.045391131641228065\n",
            "Epoch 4 - Training loss: 0.04545693768303532\n",
            "Epoch 4 - Training loss: 0.045491914942002755\n",
            "Epoch 4 - Training loss: 0.04551838003774124\n",
            "Epoch 4 - Training loss: 0.045586295686026755\n",
            "Epoch 4 - Training loss: 0.045690094569582805\n",
            "Epoch 4 - Training loss: 0.04578072396414811\n",
            "Epoch 4 - Training loss: 0.045789744974032584\n",
            "Epoch 4 - Training loss: 0.04587433238460152\n",
            "Epoch 4 - Training loss: 0.04598833599797031\n",
            "Epoch 4 - Training loss: 0.046047274244706955\n",
            "Epoch 4 - Training loss: 0.04617748967806763\n",
            "Epoch 4 - Training loss: 0.0461935066729228\n",
            "Epoch 4 - Training loss: 0.046236496296590136\n",
            "Epoch 4 - Training loss: 0.04632076239491354\n",
            "Epoch 4 - Training loss: 0.04640988157744379\n",
            "Epoch 4 - Training loss: 0.046540845865641896\n",
            "Epoch 4 - Training loss: 0.04663027862388728\n",
            "Epoch 4 - Training loss: 0.046702996600292195\n",
            "Epoch 4 - Training loss: 0.04679802236959402\n",
            "Epoch 4 - Training loss: 0.04691210361833035\n",
            "Epoch 4 - Training loss: 0.04695673655571619\n",
            "Epoch 4 - Training loss: 0.047036071171932446\n",
            "Epoch 4 - Training loss: 0.04711300365900847\n",
            "Epoch 4 - Training loss: 0.047183098411485394\n",
            "Epoch 4 - Training loss: 0.04721036116813006\n",
            "Epoch 4 - Training loss: 0.04731893745451562\n",
            "Epoch 4 - Training loss: 0.04740251852834879\n",
            "Epoch 4 - Training loss: 0.04742455851536856\n",
            "Epoch 4 - Training loss: 0.04752980870530327\n",
            "Epoch 4 - Training loss: 0.04756211463723387\n",
            "Epoch 4 - Training loss: 0.04775663145652998\n",
            "Epoch 4 - Training loss: 0.04786549863328081\n",
            "Epoch 4 - Training loss: 0.04790909002636319\n",
            "Epoch 4 - Training loss: 0.04803223886203442\n",
            "Epoch 4 - Training loss: 0.048065516542690964\n",
            "Epoch 4 - Training loss: 0.04815597219098765\n",
            "Epoch 4 - Training loss: 0.04832590679659931\n",
            "Epoch 4 - Training loss: 0.04835500564477813\n",
            "Epoch 4 - Training loss: 0.04856091427471814\n",
            "Epoch 4 - Training loss: 0.048595078638010124\n",
            "Epoch 4 - Training loss: 0.048644096218248105\n",
            "Epoch 4 - Training loss: 0.04869748578806009\n",
            "Epoch 4 - Training loss: 0.048885304884893746\n",
            "Epoch 4 - Training loss: 0.04896601843836306\n",
            "Epoch 4 - Training loss: 0.04902032857188887\n",
            "Epoch 4 - Training loss: 0.04909670000165097\n",
            "Epoch 4 - Training loss: 0.049254678998952674\n",
            "Epoch 4 - Training loss: 0.04939277384227622\n",
            "Epoch 4 - Training loss: 0.049481876971903066\n",
            "Epoch 4 - Training loss: 0.049599011798998886\n",
            "Epoch 4 - Training loss: 0.04960955149218091\n",
            "Epoch 4 - Training loss: 0.04983658955565521\n",
            "Epoch 4 - Training loss: 0.049940036863946456\n",
            "Epoch 4 - Training loss: 0.049961172089949726\n",
            "Epoch 4 - Training loss: 0.0500684342067093\n",
            "Epoch 4 - Training loss: 0.05012901526675232\n",
            "Epoch 4 - Training loss: 0.050221991181167075\n",
            "Epoch 4 - Training loss: 0.050246108551698324\n",
            "Epoch 4 - Training loss: 0.050401541960439576\n",
            "Epoch 4 - Training loss: 0.05052096695144738\n",
            "Epoch 4 - Training loss: 0.05068160192981394\n",
            "Epoch 4 - Training loss: 0.05077764278114922\n",
            "Epoch 4 - Training loss: 0.0508411657442448\n",
            "Epoch 4 - Training loss: 0.05093478038311322\n",
            "Epoch 4 - Training loss: 0.05101989348853893\n",
            "Epoch 4 - Training loss: 0.05107669522965958\n",
            "Epoch 4 - Training loss: 0.05113267422969471\n",
            "Epoch 4 - Training loss: 0.05118222852179936\n",
            "Epoch 4 - Training loss: 0.05130985709451345\n",
            "Epoch 4 - Training loss: 0.05139491574239057\n",
            "Epoch 4 - Training loss: 0.05159331809506932\n",
            "Epoch 4 - Training loss: 0.05163910439504044\n",
            "Epoch 4 - Training loss: 0.0516867027667635\n",
            "Epoch 4 - Training loss: 0.051825826870464185\n",
            "Epoch 4 - Training loss: 0.051948748312509264\n",
            "Epoch 4 - Training loss: 0.05221170988251596\n",
            "Epoch 4 - Training loss: 0.052231620252529566\n",
            "Epoch 4 - Training loss: 0.05230108264293561\n",
            "Epoch 4 - Training loss: 0.05251992082616477\n",
            "Epoch 4 - Training loss: 0.052712366161649545\n",
            "Epoch 4 - Training loss: 0.05279397483327305\n",
            "Epoch 4 - Training loss: 0.05290949763432304\n",
            "Epoch 4 - Training loss: 0.052994727232515304\n",
            "Epoch 4 - Training loss: 0.05312840191166856\n",
            "Epoch 4 - Training loss: 0.05319427669659924\n",
            "Epoch 4 - Training loss: 0.053298714391982506\n",
            "Epoch 4 - Training loss: 0.05339847387734062\n",
            "Epoch 4 - Training loss: 0.053477362165652485\n",
            "Epoch 4 - Training loss: 0.05352844279418304\n",
            "Epoch 4 - Training loss: 0.05358285961080907\n",
            "Epoch 4 - Training loss: 0.05367918480743668\n",
            "Epoch 4 - Training loss: 0.05389545428783083\n",
            "Epoch 4 - Training loss: 0.05395226986574402\n",
            "Epoch 4 - Training loss: 0.054053630677860044\n",
            "Epoch 4 - Training loss: 0.054145338921261624\n",
            "Epoch 4 - Training loss: 0.05425939040739081\n",
            "Epoch 4 - Training loss: 0.05440923864486566\n",
            "Epoch 4 - Training loss: 0.05447840174910293\n",
            "Epoch 4 - Training loss: 0.05450849424896718\n",
            "Epoch 4 - Training loss: 0.054540418588848255\n",
            "Epoch 4 - Training loss: 0.05458747308407384\n",
            "Epoch 4 - Training loss: 0.05467755039697136\n",
            "Epoch 4 - Training loss: 0.0547543940489798\n",
            "Epoch 4 - Training loss: 0.05489866960166233\n",
            "Epoch 4 - Training loss: 0.05496532560141483\n",
            "Epoch 4 - Training loss: 0.054984929991254546\n",
            "Epoch 4 - Training loss: 0.055088061658637735\n",
            "Epoch 4 - Training loss: 0.05524960007947455\n",
            "Epoch 4 - Training loss: 0.05526909931525111\n",
            "Epoch 4 - Training loss: 0.05539687472318154\n",
            "Epoch 4 - Training loss: 0.055444773473242705\n",
            "Epoch 4 - Training loss: 0.0556443103952512\n",
            "Epoch 4 - Training loss: 0.05570490105844129\n",
            "Epoch 4 - Training loss: 0.05580296767339396\n",
            "Epoch 4 - Training loss: 0.05585007304385273\n",
            "Epoch 4 - Training loss: 0.0559178433581583\n",
            "Epoch 4 - Training loss: 0.0560949899510407\n",
            "Epoch 4 - Training loss: 0.056265129646195025\n",
            "Epoch 4 - Training loss: 0.05634186632121042\n",
            "Epoch 4 - Training loss: 0.05646720747830771\n",
            "Epoch 4 - Training loss: 0.056714164073279165\n",
            "Epoch 4 - Training loss: 0.05679666371678493\n",
            "Epoch 4 - Training loss: 0.05688794072407649\n",
            "Epoch 4 - Training loss: 0.05692167758846334\n",
            "Epoch 4 - Training loss: 0.05714781479890158\n",
            "Epoch 4 - Training loss: 0.05726222407970347\n",
            "Epoch 4 - Training loss: 0.05735686067928637\n",
            "Epoch 4 - Training loss: 0.05741224563849379\n",
            "Epoch 4 - Training loss: 0.05745758865672007\n",
            "Epoch 4 - Training loss: 0.05752633795984137\n",
            "Epoch 4 - Training loss: 0.05761951148144599\n",
            "Epoch 4 - Training loss: 0.05770598206994757\n",
            "Epoch 4 - Training loss: 0.057733786101740944\n",
            "Epoch 4 - Training loss: 0.057794683871032204\n",
            "Epoch 4 - Training loss: 0.057891818210244306\n",
            "Epoch 4 - Training loss: 0.0580135751773776\n",
            "Epoch 4 - Training loss: 0.05804633522338704\n",
            "Epoch 4 - Training loss: 0.05807077139417436\n",
            "Epoch 4 - Training loss: 0.05826532894542921\n",
            "Epoch 4 - Training loss: 0.05841004046033631\n",
            "Epoch 4 - Training loss: 0.05848289070440444\n",
            "Epoch 4 - Training loss: 0.05851405580192487\n",
            "Epoch 4 - Training loss: 0.05874135963984135\n",
            "Epoch 4 - Training loss: 0.05890473889421298\n",
            "Epoch 4 - Training loss: 0.05905517725262052\n",
            "Epoch 4 - Training loss: 0.059112668188332496\n",
            "Epoch 4 - Training loss: 0.059261075302418365\n",
            "Epoch 4 - Training loss: 0.05931635025038775\n",
            "Epoch 4 - Training loss: 0.05935658343724096\n",
            "Epoch 4 - Training loss: 0.059384599196980754\n",
            "Epoch 4 - Training loss: 0.05953883662073216\n",
            "Epoch 4 - Training loss: 0.05970672919536069\n",
            "Epoch 4 - Training loss: 0.05975946955192191\n",
            "Epoch 4 - Training loss: 0.059819724212196085\n",
            "Epoch 4 - Training loss: 0.060097918692809435\n",
            "Epoch 4 - Training loss: 0.06021579912603537\n",
            "Epoch 4 - Training loss: 0.06023861244440015\n",
            "Epoch 4 - Training loss: 0.06034175051388138\n",
            "Epoch 4 - Training loss: 0.0605274367906741\n",
            "Epoch 4 - Training loss: 0.06063021156094921\n",
            "Epoch 4 - Training loss: 0.06066682814622421\n",
            "Epoch 4 - Training loss: 0.06078527590383027\n",
            "Epoch 4 - Training loss: 0.060821244634116\n",
            "Epoch 4 - Training loss: 0.060861831163562564\n",
            "Epoch 4 - Training loss: 0.06091589217922136\n",
            "Epoch 4 - Training loss: 0.060980870189872\n",
            "Epoch 4 - Training loss: 0.06111416930575043\n",
            "Epoch 4 - Training loss: 0.061141197160601235\n",
            "Epoch 4 - Training loss: 0.06115328324442384\n",
            "Epoch 4 - Training loss: 0.0612230698224594\n",
            "Epoch 4 - Training loss: 0.06138281217678936\n",
            "Epoch 4 - Training loss: 0.06145203649811049\n",
            "Epoch 4 - Training loss: 0.061608352411443044\n",
            "Epoch 4 - Training loss: 0.061734967220669935\n",
            "Epoch 4 - Training loss: 0.061762026567726946\n",
            "Epoch 4 - Training loss: 0.0618994060441105\n",
            "Epoch 4 - Training loss: 0.062002636880468906\n",
            "Epoch 4 - Training loss: 0.062034652146425395\n",
            "Epoch 4 - Training loss: 0.06214591130109897\n",
            "Epoch 4 - Training loss: 0.06215277871440675\n",
            "Epoch 4 - Training loss: 0.06222897590543114\n",
            "Epoch 4 - Training loss: 0.06242829291616628\n",
            "Epoch 4 - Training loss: 0.06251585151978345\n",
            "Epoch 4 - Training loss: 0.06252700220624696\n",
            "Epoch 4 - Training loss: 0.06265882339064421\n",
            "Epoch 4 - Training loss: 0.0626700497654352\n",
            "Epoch 4 - Training loss: 0.06271508308018306\n",
            "Epoch 4 - Training loss: 0.06278736887525505\n",
            "Epoch 4 - Training loss: 0.062959332103982\n",
            "Epoch 4 - Training loss: 0.06301150204234524\n",
            "Epoch 4 - Training loss: 0.06306445521703645\n",
            "Epoch 4 - Training loss: 0.06309039596029356\n",
            "Epoch 4 - Training loss: 0.0631262626462201\n",
            "Epoch 4 - Training loss: 0.06322129894006275\n",
            "Epoch 4 - Training loss: 0.06339529450925221\n",
            "Epoch 4 - Training loss: 0.06351945033298928\n",
            "Epoch 4 - Training loss: 0.06354344961221522\n",
            "Epoch 4 - Training loss: 0.06357488871714882\n",
            "Epoch 4 - Training loss: 0.06363523229192108\n",
            "Epoch 4 - Training loss: 0.06377023840094728\n",
            "Epoch 4 - Training loss: 0.06380990902527865\n",
            "Epoch 4 - Training loss: 0.0638422881007822\n",
            "Epoch 4 - Training loss: 0.0639284215385972\n",
            "Epoch 4 - Training loss: 0.06397115023308265\n",
            "Epoch 4 - Training loss: 0.0641054787999714\n",
            "Epoch 4 - Training loss: 0.06423102608715087\n",
            "Epoch 4 - Training loss: 0.0642781639287967\n",
            "Epoch 4 - Training loss: 0.06434671431412495\n",
            "Epoch 4 - Training loss: 0.06437583809553671\n",
            "Epoch 4 - Training loss: 0.06443018607769424\n",
            "Epoch 4 - Training loss: 0.06449300660910622\n",
            "Epoch 4 - Training loss: 0.06455377545872969\n",
            "Epoch 4 - Training loss: 0.06461409666190253\n",
            "Epoch 4 - Training loss: 0.06472278735997565\n",
            "Epoch 4 - Training loss: 0.06476914093269706\n",
            "Epoch 4 - Training loss: 0.06488737886670286\n",
            "Epoch 4 - Training loss: 0.06506402909841094\n",
            "Epoch 4 - Training loss: 0.06547275387487415\n",
            "Epoch 4 - Training loss: 0.06558180185132173\n",
            "Epoch 4 - Training loss: 0.06567820734423853\n",
            "Epoch 4 - Training loss: 0.06582112412707114\n",
            "Epoch 4 - Training loss: 0.0658502693348594\n",
            "Epoch 4 - Training loss: 0.06599104702264182\n",
            "Epoch 4 - Training loss: 0.06609954297302295\n",
            "Epoch 4 - Training loss: 0.06628989529903176\n",
            "Epoch 4 - Training loss: 0.06639888842077826\n",
            "Epoch 4 - Training loss: 0.06658852242503259\n",
            "Epoch 4 - Training loss: 0.06664336896461766\n",
            "Epoch 4 - Training loss: 0.06667789515368006\n",
            "Epoch 4 - Training loss: 0.06677428676214602\n",
            "Epoch 4 - Training loss: 0.06682431864599858\n",
            "Epoch 4 - Training loss: 0.06693570435728266\n",
            "Epoch 4 - Training loss: 0.06715856849112666\n",
            "Epoch 4 - Training loss: 0.06718150278412377\n",
            "Epoch 4 - Training loss: 0.06724842825977961\n",
            "Epoch 4 - Training loss: 0.0674070541231609\n",
            "Epoch 4 - Training loss: 0.06756973265757614\n",
            "Epoch 4 - Training loss: 0.06773076663121406\n",
            "Epoch 4 - Training loss: 0.06781276622797047\n",
            "Epoch 4 - Training loss: 0.0678470805163251\n",
            "Epoch 4 - Training loss: 0.06786639439706593\n",
            "Epoch 4 - Training loss: 0.06800143510601651\n",
            "Epoch 4 - Training loss: 0.06805486753652457\n",
            "Epoch 4 - Training loss: 0.0680901923277445\n",
            "Epoch 4 - Training loss: 0.06814700311599875\n",
            "Epoch 4 - Training loss: 0.06820361099507748\n",
            "Epoch 4 - Training loss: 0.06841947178967964\n",
            "Epoch 4 - Training loss: 0.06854362079033703\n",
            "Epoch 4 - Training loss: 0.0686978593420015\n",
            "Epoch 4 - Training loss: 0.06889302761845989\n",
            "Epoch 4 - Training loss: 0.06912329991701553\n",
            "Epoch 4 - Training loss: 0.06923614845210031\n",
            "Epoch 4 - Training loss: 0.0692877354381332\n",
            "Epoch 4 - Training loss: 0.06937119839881767\n",
            "Epoch 4 - Training loss: 0.06942570743822196\n",
            "Epoch 4 - Training loss: 0.06943238622495043\n",
            "Epoch 4 - Training loss: 0.06955333582079931\n",
            "Epoch 4 - Training loss: 0.0695623401006354\n",
            "Epoch 4 - Training loss: 0.06966553245291812\n",
            "Epoch 4 - Training loss: 0.06979673287209703\n",
            "Epoch 4 - Training loss: 0.06981911448769763\n",
            "Epoch 4 - Training loss: 0.06994955685398759\n",
            "Epoch 4 - Training loss: 0.07004735609000204\n",
            "Epoch 4 - Training loss: 0.07010659454032969\n",
            "Epoch 4 - Training loss: 0.07018638624728266\n",
            "Epoch 4 - Training loss: 0.07030645320350841\n",
            "Epoch 4 - Training loss: 0.07042448822436716\n",
            "Epoch 4 - Training loss: 0.07055516249593546\n",
            "Epoch 4 - Training loss: 0.07065515404543293\n",
            "Epoch 4 - Training loss: 0.07097649485416917\n",
            "Epoch 4 - Training loss: 0.0709995961736149\n",
            "Epoch 4 - Training loss: 0.07104825434276957\n",
            "Epoch 4 - Training loss: 0.0710891085768194\n",
            "Epoch 4 - Training loss: 0.07120615634014771\n",
            "Epoch 4 - Training loss: 0.07158520048174427\n",
            "Epoch 4 - Training loss: 0.07168324790628892\n",
            "Epoch 4 - Training loss: 0.0717316562387306\n",
            "Epoch 4 - Training loss: 0.07182942771378642\n",
            "Epoch 4 - Training loss: 0.07188760411879941\n",
            "Epoch 4 - Training loss: 0.07190803495317158\n",
            "Epoch 4 - Training loss: 0.07214745939242592\n",
            "Epoch 4 - Training loss: 0.07222693070852712\n",
            "Epoch 4 - Training loss: 0.07228962728864889\n",
            "Epoch 4 - Training loss: 0.07243096426244515\n",
            "Epoch 4 - Training loss: 0.07261813323654885\n",
            "Epoch 4 - Training loss: 0.072701780182093\n",
            "Epoch 4 - Training loss: 0.0728079759364904\n",
            "Epoch 4 - Training loss: 0.07285069488088237\n",
            "Epoch 4 - Training loss: 0.07294522025394065\n",
            "Epoch 4 - Training loss: 0.07297536297373648\n",
            "Epoch 4 - Training loss: 0.07319105156302166\n",
            "Epoch 4 - Training loss: 0.07345584513389193\n",
            "Epoch 4 - Training loss: 0.07348144758663484\n",
            "Epoch 4 - Training loss: 0.07356900073337688\n",
            "Epoch 4 - Training loss: 0.0736024451624753\n",
            "Epoch 4 - Training loss: 0.07369037134660238\n",
            "Epoch 4 - Training loss: 0.07374396419494168\n",
            "Epoch 4 - Training loss: 0.0738543511312733\n",
            "Epoch 4 - Training loss: 0.07393071782697778\n",
            "Epoch 4 - Training loss: 0.07406842096674957\n",
            "Epoch 4 - Training loss: 0.07409307226709434\n",
            "Epoch 4 - Training loss: 0.07415876242774985\n",
            "Epoch 4 - Training loss: 0.0742024202048699\n",
            "Epoch 4 - Training loss: 0.07426329678607616\n",
            "Epoch 4 - Training loss: 0.07429927560006346\n",
            "Epoch 4 - Training loss: 0.0743889161802606\n",
            "Epoch 4 - Training loss: 0.07448964803787406\n",
            "Epoch 4 - Training loss: 0.07455506023150613\n",
            "Epoch 4 - Training loss: 0.0746172661615063\n",
            "Epoch 4 - Training loss: 0.07467609060817022\n",
            "Epoch 4 - Training loss: 0.074720289342717\n",
            "Epoch 4 - Training loss: 0.07478795993203388\n",
            "Epoch 4 - Training loss: 0.07488126151942845\n",
            "Epoch 4 - Training loss: 0.07495159930539633\n",
            "Epoch 4 - Training loss: 0.07497605210694391\n",
            "Epoch 4 - Training loss: 0.07509285807589701\n",
            "Epoch 4 - Training loss: 0.07522232608416918\n",
            "Epoch 4 - Training loss: 0.07523184396060449\n",
            "Epoch 4 - Training loss: 0.07534489849868264\n",
            "Epoch 4 - Training loss: 0.07544359195528667\n",
            "Epoch 4 - Training loss: 0.07555305049208595\n",
            "Epoch 4 - Training loss: 0.07564574797083733\n",
            "Epoch 4 - Training loss: 0.07581839968722417\n",
            "Epoch 4 - Training loss: 0.07602797932466114\n",
            "Epoch 4 - Training loss: 0.07615194780362854\n",
            "Epoch 4 - Training loss: 0.07627531585158474\n",
            "Epoch 4 - Training loss: 0.07638529391149118\n",
            "Epoch 4 - Training loss: 0.07654657510460726\n",
            "Epoch 4 - Training loss: 0.07658524957916407\n",
            "Epoch 4 - Training loss: 0.07662360573600509\n",
            "Epoch 4 - Training loss: 0.07664132025390705\n",
            "Epoch 4 - Training loss: 0.07678674408562307\n",
            "Epoch 4 - Training loss: 0.07689451271477443\n",
            "Epoch 4 - Training loss: 0.07697367967704116\n",
            "Epoch 4 - Training loss: 0.07705294552233333\n",
            "Epoch 4 - Training loss: 0.07714796188706433\n",
            "Epoch 4 - Training loss: 0.07720373099201175\n",
            "Epoch 4 - Training loss: 0.0772581910664863\n",
            "Epoch 4 - Training loss: 0.07729194135859863\n",
            "Epoch 4 - Training loss: 0.07733936491571287\n",
            "Epoch 4 - Training loss: 0.07745048459043413\n",
            "Epoch 4 - Training loss: 0.07760976535826127\n",
            "Epoch 4 - Training loss: 0.07765195831787516\n",
            "Epoch 4 - Training loss: 0.07767170577196837\n",
            "Epoch 4 - Training loss: 0.07775371374069914\n",
            "Epoch 4 - Training loss: 0.07782618247214824\n",
            "Epoch 4 - Training loss: 0.07783420826010565\n",
            "Epoch 4 - Training loss: 0.07786505431510699\n",
            "Epoch 4 - Training loss: 0.07788764102025025\n",
            "Epoch 4 - Training loss: 0.07798631271289816\n",
            "Epoch 4 - Training loss: 0.0781172702256749\n",
            "Epoch 4 - Training loss: 0.07819776018229581\n",
            "Epoch 4 - Training loss: 0.0783817164310967\n",
            "Epoch 4 - Training loss: 0.07842396542495454\n",
            "Epoch 4 - Training loss: 0.07846617779738184\n",
            "Epoch 4 - Training loss: 0.07853139915924544\n",
            "Epoch 4 - Training loss: 0.07874389998028988\n",
            "Epoch 4 - Training loss: 0.0788193638371959\n",
            "Epoch 4 - Training loss: 0.07889699713954491\n",
            "Epoch 4 - Training loss: 0.0789912410982962\n",
            "Epoch 4 - Training loss: 0.07908139552231998\n",
            "Epoch 4 - Training loss: 0.07909457982112683\n",
            "Epoch 4 - Training loss: 0.07931178899606996\n",
            "Epoch 4 - Training loss: 0.0793938329071005\n",
            "Epoch 4 - Training loss: 0.07945043821263113\n",
            "Epoch 4 - Training loss: 0.07960844370794655\n",
            "Epoch 4 - Training loss: 0.07967209122997564\n",
            "Epoch 4 - Training loss: 0.07972602706714305\n",
            "Epoch 4 - Training loss: 0.07981181020852425\n",
            "Epoch 4 - Training loss: 0.07989537440337685\n",
            "Epoch 4 - Training loss: 0.0799266776126593\n",
            "Epoch 4 - Training loss: 0.0799892141284155\n",
            "Epoch 4 - Training loss: 0.08001620415250646\n",
            "Epoch 4 - Training loss: 0.08002992295699794\n",
            "Epoch 4 - Training loss: 0.08012073306388645\n",
            "Epoch 4 - Training loss: 0.08020416630167904\n",
            "Epoch 4 - Training loss: 0.0803313740913365\n",
            "Epoch 4 - Training loss: 0.08038813078742243\n",
            "Epoch 4 - Training loss: 0.0804972272403022\n",
            "Epoch 4 - Training loss: 0.08058266705130415\n",
            "Epoch 4 - Training loss: 0.08059305849739277\n",
            "Epoch 4 - Training loss: 0.08068944429948942\n",
            "Epoch 4 - Training loss: 0.08081674901818209\n",
            "Epoch 4 - Training loss: 0.08088326961673828\n",
            "Epoch 4 - Training loss: 0.08104764588270535\n",
            "Epoch 4 - Training loss: 0.08117584258204759\n",
            "Epoch 4 - Training loss: 0.08129984043441268\n",
            "Epoch 4 - Training loss: 0.08144868482926935\n",
            "Epoch 4 - Training loss: 0.08148688649776568\n",
            "Epoch 4 - Training loss: 0.08152860567432398\n",
            "Epoch 4 - Training loss: 0.08153762779524253\n",
            "Epoch 4 - Training loss: 0.08160961040410834\n",
            "Epoch 4 - Training loss: 0.08169946092705149\n",
            "Epoch 4 - Training loss: 0.08174122178873448\n",
            "Epoch 4 - Training loss: 0.08183679718530175\n",
            "Epoch 4 - Training loss: 0.08196041294520519\n",
            "Epoch 4 - Training loss: 0.08204094879676713\n",
            "Epoch 4 - Training loss: 0.08207642099510298\n",
            "Epoch 4 - Training loss: 0.08219360092283487\n",
            "Epoch 4 - Training loss: 0.08238410022703489\n",
            "Epoch 4 - Training loss: 0.0825175694353791\n",
            "Epoch 4 - Training loss: 0.0825485731202013\n",
            "Epoch 4 - Training loss: 0.08260322960643673\n",
            "Epoch 4 - Training loss: 0.08267389725931465\n",
            "Epoch 4 - Training loss: 0.08268833712988428\n",
            "Epoch 4 - Training loss: 0.08280162124592723\n",
            "Epoch 4 - Training loss: 0.08284410984187858\n",
            "Epoch 4 - Training loss: 0.0829236665139916\n",
            "Epoch 4 - Training loss: 0.08310477408334248\n",
            "Epoch 4 - Training loss: 0.0832211205071565\n",
            "Epoch 4 - Training loss: 0.08340082976615616\n",
            "Epoch 4 - Training loss: 0.08346141670405595\n",
            "Epoch 4 - Training loss: 0.0835129667985946\n",
            "Epoch 4 - Training loss: 0.08353835291840803\n",
            "Epoch 4 - Training loss: 0.08355961042964684\n",
            "Epoch 4 - Training loss: 0.08363741763712548\n",
            "Epoch 4 - Training loss: 0.08376659443731041\n",
            "Epoch 4 - Training loss: 0.08386247685657683\n",
            "Epoch 4 - Training loss: 0.08387864768248933\n",
            "Epoch 4 - Training loss: 0.0841289423282014\n",
            "Epoch 4 - Training loss: 0.0842067516564743\n",
            "Epoch 4 - Training loss: 0.084219557218758\n",
            "Epoch 4 - Training loss: 0.08424908484864448\n",
            "Epoch 4 - Training loss: 0.08427259569187392\n",
            "Epoch 4 - Training loss: 0.08433613770240263\n",
            "Epoch 4 - Training loss: 0.08438887657758508\n",
            "Epoch 4 - Training loss: 0.0845091247386031\n",
            "Epoch 4 - Training loss: 0.08458446170529053\n",
            "Epoch 4 - Training loss: 0.08467773584944846\n",
            "Epoch 4 - Training loss: 0.08480007589997243\n",
            "Epoch 4 - Training loss: 0.0848667683344938\n",
            "Epoch 4 - Training loss: 0.08487792382054507\n",
            "Epoch 4 - Training loss: 0.08498263057085004\n",
            "Epoch 4 - Training loss: 0.08507955383940308\n",
            "Epoch 4 - Training loss: 0.08528791226472443\n",
            "Epoch 4 - Training loss: 0.08537276403587271\n",
            "Epoch 4 - Training loss: 0.08545546633834794\n",
            "Epoch 4 - Training loss: 0.08554505155548882\n",
            "Epoch 4 - Training loss: 0.08558652663021597\n",
            "Epoch 4 - Training loss: 0.08572731972751475\n",
            "Epoch 4 - Training loss: 0.08580440326756648\n",
            "Epoch 4 - Training loss: 0.08594739085474391\n",
            "Epoch 4 - Training loss: 0.08603034588917376\n",
            "Epoch 4 - Training loss: 0.08606418320136681\n",
            "Epoch 4 - Training loss: 0.08612925906641794\n",
            "Epoch 4 - Training loss: 0.08620488578761454\n",
            "Epoch 4 - Training loss: 0.08640362008033928\n",
            "Epoch 4 - Training loss: 0.08642923339321686\n",
            "Epoch 4 - Training loss: 0.08649793746067025\n",
            "Epoch 4 - Training loss: 0.08661114954119568\n",
            "Epoch 4 - Training loss: 0.08664908987144306\n",
            "Epoch 4 - Training loss: 0.08671815638025321\n",
            "Epoch 4 - Training loss: 0.08684939386964495\n",
            "Epoch 4 - Training loss: 0.08701568974831354\n",
            "Epoch 4 - Training loss: 0.08711272370326383\n",
            "Epoch 4 - Training loss: 0.08721304276058557\n",
            "Epoch 4 - Training loss: 0.0874149366396108\n",
            "Epoch 4 - Training loss: 0.08750735784320832\n",
            "Epoch 4 - Training loss: 0.08758762574393246\n",
            "Epoch 4 - Training loss: 0.08785724249342754\n",
            "Epoch 4 - Training loss: 0.08787872923115121\n",
            "Epoch 4 - Training loss: 0.08801162949473317\n",
            "Epoch 4 - Training loss: 0.08803705518483433\n",
            "Epoch 4 - Training loss: 0.08816335229590329\n",
            "Epoch 4 - Training loss: 0.08820846858374806\n",
            "Epoch 4 - Training loss: 0.08829816017421022\n",
            "Epoch 4 - Training loss: 0.08838797446096074\n",
            "Epoch 4 - Training loss: 0.08849044087410433\n",
            "Epoch 4 - Training loss: 0.08868776791260394\n",
            "Epoch 4 - Training loss: 0.08881301935295513\n",
            "Epoch 4 - Training loss: 0.08891879207641283\n",
            "Epoch 4 - Training loss: 0.08893752623963029\n",
            "Predicted Digit = 2\n",
            "Probabilities for each digit class: [7.811318e-09, 4.7720878e-08, 0.999876, 0.00011681709, 3.320331e-06, 5.330695e-10, 1.5975319e-08, 1.1371065e-07, 3.6665192e-06, 2.6678244e-09]\n",
            "Number Of Images Tested = 10000\n",
            "\n",
            "Model Accuracy = 0.97\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to select a random image and test it\n",
        "def test_random_image():\n",
        "    # Select a random image from the validation set\n",
        "    random_value = random.randint(0, len(validation_loader.dataset) - 1)#run it through the whole dataset\n",
        "    img, label = validation_loader.dataset[random_value]  # Get the image and its label\n",
        "\n",
        "    # Display the image\n",
        "    plt.imshow(img.numpy().squeeze(), cmap=\"gray_r\")\n",
        "    plt.title(f\"True Label: {label}\")\n",
        "    plt.show()\n",
        "\n",
        "    # Flatten the image and pass it to the model\n",
        "    img = img.view(1, 784)  # Flatten the image to match the input size\n",
        "    with torch.no_grad():\n",
        "        output = model(img)\n",
        "\n",
        "    # Convert log probabilities to actual probabilities\n",
        "    probabilities = torch.exp(output)\n",
        "    probabilities_list = probabilities.numpy()[0]\n",
        "\n",
        "    # Get the index of the maximum probability\n",
        "    predicted_label = np.argmax(probabilities_list)\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Predicted Label: {predicted_label}\")\n",
        "    print(f\"Probabilities: {probabilities_list}\")\n",
        "    print(f\"True Label: {label}\")\n",
        "\n",
        "# Call the function to test a random image\n",
        "test_random_image()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "xB_ExHa7haod",
        "outputId": "2aba2aa1-4f87-4360-85f0-8e441882420b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Label: 5\n",
            "Probabilities: [0.0012 0.0007 0.0098 0.015  0.0039 0.5909 0.2769 0.     0.0971 0.0046]\n",
            "True Label: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the dataset\n",
        "x_train = (x_train / 255.0 - 0.5) / 0.5\n",
        "x_test = (x_test / 255.0 - 0.5) / 0.5\n",
        "\n",
        "# Convert NumPy arrays to PyTorch tensors\n",
        "x_train_tensor = torch.tensor(x_train, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "x_test_tensor = torch.tensor(x_test, dtype=torch.float32).unsqueeze(1)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create datasets\n",
        "training_set = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "validation_set = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "\n",
        "# DataLoader\n",
        "training_loader = DataLoader(training_set, batch_size=64, shuffle=True)\n",
        "validation_loader = DataLoader(validation_set, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define the model with CNN and Fully Connected layers\n",
        "class CNN_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_Model, self).__init__()\n",
        "\n",
        "        # Define the convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "\n",
        "        # Define max pooling layer\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(128 * 3 * 3, 512)  # Adjusted for output size\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply convolutional layers with ReLU activations\n",
        "        x = self.pool(nn.ReLU()(self.conv1(x)))\n",
        "        x = self.pool(nn.ReLU()(self.conv2(x)))\n",
        "        x = self.pool(nn.ReLU()(self.conv3(x)))\n",
        "\n",
        "        # Flatten the tensor for the fully connected layers\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output\n",
        "        x = nn.ReLU()(self.fc1(x))  # First fully connected layer\n",
        "        x = self.fc2(x)  # Second fully connected layer (output layer)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = CNN_Model()\n",
        "logloss = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in training_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = logloss(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(training_loader)}\")\n",
        "\n",
        "# Evaluation on validation set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation for faster inference\n",
        "    for images, labels in validation_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)  # Get the index of the max output\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Validation Accuracy: {100 * correct / total}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPvqCxePmGfn",
        "outputId": "2d53467a-2c23-4016-e34d-92186eba5427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.3502662780631119\n",
            "Epoch 2, Loss: 0.052825092793261606\n",
            "Epoch 3, Loss: 0.03536710995198565\n",
            "Epoch 4, Loss: 0.02646122193140194\n",
            "Epoch 5, Loss: 0.020855277899742328\n",
            "Validation Accuracy: 99.23%\n"
          ]
        }
      ]
    }
  ]
}